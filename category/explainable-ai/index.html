<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/evaluation-methods/">Evaluation Methods</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/e-commerce/">E-commerce</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reinforcement-learning/">Reinforcement Learning</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Evaluating the Pros and Cons of Recommender Systems Explanations (2024)</h3>
    <p><strong>Authors:</strong> Kathrin Wardatzky</p>
    <p>Despite the growing interest in explainable AI in the RecSys community, the evaluation of explanations is still an open research topic. Typically, explanations are evaluated using offline metrics, with a case study, or through a user study. In my research, I will have a closer look at the evaluation of the effects of explanations on users. I investigate two possible factors that can impact the effects reported in recent publications, namely the explanation design and content as well as the users themselves. I further address the problem of determining promising explanations for an application scenario from a seemingly endless pool of options. Lastly, I propose a user study to close some of the research gaps established in the surveys and investigate how recommender systems explanations impact the understanding of users with different backgrounds.</p>
    <p><strong>Categories:</strong> Explainable AI, Recommender Systems (RecSys), Evaluation Methods, User Study, Explanation Design, User-Centered Design, Offline Metrics, Case Study (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1140/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Designing and evaluating explainable AI for non-AI experts: challenges and opportunities (2022)</h3>
    <p><strong>Authors:</strong> Katrien Verbert, Vero Vanden Abeele, Maxwell Szymanski</p>
    <p>Artificial intelligence (AI) has seen a steady increase in use in the health and medical field, where it is used by lay users and health experts alike. However, these AI systems often lack transparency regarding the inputs and decision making process (often called black boxes), which in turn can be detrimental to the user’s satisfaction and trust towards these systems. Explainable AI (XAI) aims to overcome this problem by opening up certain aspects of the black box, and has proven to be a successful means of increasing trust, transparency and even system effectiveness. However, for certain groups (i.e. lay users in health), explanation methods and evaluation metrics still remain underexplored. In this paper, we will outline our research regarding designing and evaluating explanations for health recommendations for lay users and domain experts, as well as list a few takeaways we were already able to find in our initial studies.</p>
    <p><strong>Categories:</strong> Explainable AI, Transparency, Trust in AI, Healthcare, Medicine, User-Centered Design, Evaluation Metrics, Health Recommendations, Explanation Methods, User Trust, Challenges and Opportunities, User Satisfaction (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/811/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Explainable Recommendation for Repeat Consumption (2020)</h3>
    <p><strong>Authors:</strong> Masataka Goto, Kosetsu Tsukuda</p>
    <p>Displaying appropriate explanations for recommended items is of vital importance for improving the persuasiveness and user satisfaction of recommender systems. Although a user often consumes the same item repeatedly in some domains such as music and restaurants, existing studies have focused on generating explanations for recommending novel items. In this paper, we describe the concept of explainable recommendation for repeatedly consumed items. Because of the high proportion of repeat consumption in music listening, we suggest nine kinds of explanations for song recommendations according to three factors: personal, social, and item factors. From the results of an online survey involving 622 participants, we evaluate the usefulness of these explanations.</p>
    <p><strong>Categories:</strong> Explainable AI, Recommendation Systems, User Experience (UX), Repeat Consumption, Explanation Factors (Personal/Social/Item), Evaluation Metrics, Online Surveys/User Studies, Music Domain, Restaurant Domain, User Satisfaction, Persuasion in Recommendations, Transparency in AI (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/567/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>PicTouRe – A Picture-Based Tourism Recommender (2020)</h3>
    <p><strong>Authors:</strong> Hannes Werthner, Mete Sertkan, Julia Neidhardt</p>
    <p>We present PicTouRe – a picture-based tourism recommender. PicTouRe aims to mitigate people’s difficulties in explicitly expressing their touristic preferences, which is even more challenging in the initial phase of travel decision making. Addressing this issue, with PicTouRe we follow the idiom “a picture is worth a thousand words” and use pictures as a tool to implicitly elicit peoples’ touristic preferences. We describe the core concept of PicTouRe - the Generic Profiler, which in essence determines an explainable vector representation, i.e., touristic profile, given any picture collection as input. We showcase a user’s journey through PicTouRe and describe the steps behind. Finally, we present results of a first user study supporting our approach. PicTouRe is available under https://pictoprof.ec.tuwien.ac.at and a demo video under https://youtu.be/xZnXLPcenEs.</p>
    <p><strong>Categories:</strong> Tourism, Recommender Systems, Visual Recommendations, Picture-Based, Implicit Feedback, User-Centered Design, Explainable AI, Profiling, Evaluation Methods, Real-World Application (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/595/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Content-based Feature Exploration for Transparent Music Recommendation using Self-attentive Genre Classification (2018)</h3>
    <p><strong>Authors:</strong> Juheon Lee, Kyogu Lee, Seungjin Lee</p>
    <p>Interpretation of retrieved results is an important issue in music recommender systems, particularly from a user perspective. In this study, we investigate the methods for providing interpretability of content features using self-attention. We extract lyric features with the self-attentive genre classification model trained on 140,000 tracks of lyrics. Likewise, we extract acoustic features using the acoustic model with self-attention trained on 120,000 tracks of acoustic signals. The experimental results show that the proposed methods provide the characteristics that are interpretable in terms of both lyrical and musical contents. We demonstrate this by visualizing the attention weights, and by presenting the most similar songs found using lyric or audio features.</p>
    <p><strong>Categories:</strong> Music Recommendations, Content-Based Recommendations, Deep Learning, Explainable AI, Interpretability, Scalability, User-Centric Design, Model Interpretation, Feature Extraction, Self-attention, Genre Classification, Beyond Accuracy, Transparency, Lyric Analysis, Acoustic Features (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/415/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Using Explainability for Constrained Matrix Factorization (2017)</h3>
    <p><strong>Authors:</strong> Olfa Nasraoui, Behnoush Abdollahi</p>
    <p>Accurate model-based Collaborative Filtering (CF) approaches tend to be black-box machine learning models, such as Matrix Factorization (MF), that lack interpretability and do not provide a straightforward explanation for their outputs. Yet explanations can improve the transparency of a recommender system by justifying recommendations, and this in turn can enhance the user’s trust in the recommendations. Hence, one main challenge in designing a recommender system is mitigating the trade-off between an explainable technique with moderate prediction accuracy and a more accurate technique with no explainable recommendations. In this paper, we focus on MF and further assume the absence of any additional data source, such as item content or user attributes. We propose an explainability constrained MF technique that computes the top-n recommendation list from items that are explainable. Experimental results show that our method is effective in generating accurate and explainable recommendations.</p>
    <p><strong>Categories:</strong> Matrix Factorization, Collaborative Filtering, Explainable AI, Transparency, Trust, User Trust, Recommendation Systems, Algorithmic Transparency, Interpretability, Evaluation Metrics, Performance Analysis (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/309/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>An Explanatory Matrix Factorization with User Comments Data (2017)</h3>
    <p><strong>Authors:</strong> Donghyun Kim, Hayong Shin</p>
    <p>Matrix factorization is one of the crucial algorithms of the Recommendation system. It implies that the relationship between user and contents can be explained by hidden latent variables. However, it is not intuitive to understand the meaning of these hidden latent variables. Therefore, this study suggests a way to learn the meaning from supplementary data such as comments and use in matrix factorization. The data used in this study is user comment data from Naver which is the largest web platform and also the largest Webtoons (Web comics) platform in South Korea. We show that the suggest method which uses the supervised latent variable also fits well with users with the distinct tendency compare to conventional matrix factorization.</p>
    <p><strong>Categories:</strong> Matrix Factorization, Recommendation Systems, User Feedback, Webtoon, Explainable AI, Data Augmentation, Platform-Specific (Naver), Evaluation Methods, Media Recommendations, User Behavior (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/325/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>