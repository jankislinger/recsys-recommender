<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/explainability/">Explainability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/evaluation-methods/">Evaluation Methods</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/music-domain/">Music Domain</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reinforcement-learning/">Reinforcement Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/multi-armed-bandits/">Multi-Armed Bandits</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>A Unified Graph Transformer for Overcoming Isolations in Multi-modal Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Zixuan Yi, Iadh Ounis</p>
    <p>With the rapid development of online multimedia services, especially in e-commerce platforms, there is a pressing need for personalised recommendation systems that can effectively encode the diverse multi-modal content associated with each item. However, we argue that existing multi-modal recommender systems typically use isolated processes for both feature extraction and modality modelling. Such isolated processes can harm the recommendation performance. Firstly, an isolated extraction process underestimates the importance of effective feature extraction in multi-modal recommendations, potentially incorporating non-relevant information, which is harmful to item representations. Second, an isolated modality modelling process produces disjointed embeddings for item modalities due to the individual processing of each modality, which leads to a suboptimal fusion of user/item representations for effective user preferences prediction. We hypothesise that the use of a unified model for addressing both aforementioned isolated processes will enable the consistent extraction and cohesive fusion of joint multi-modal features, thereby enhancing the effectiveness of multi-modal recommender systems. In this paper, we propose a novel model, called Unified Multi-modal Graph Transformer (UGT), which firstly leverages a multi-way transformer to extract aligned multi-modal features from raw data for top-k recommendation. Subsequently, we build a unified graph neural network in our UGT model to jointly fuse the user/item representations with their corresponding multi-modal features. Using the graph transformer architecture of our UGT model, we show that the UGT model can achieve significant effectiveness gains, especially when jointly optimised with the commonly-used multi-modal recommendation losses. Our extensive experiments conducted on three benchmark datasets demonstrate the superiority of our proposed UGT model over seven existing state-of-the-art recommendation approaches.</p>
    <p><strong>Categories:</strong> Graph Neural Networks, Transformer Models, Multi-Modal Recommendations, E-Commerce, Feature Extraction, Modality Modeling, Multi-Modal Fusion, Online Services, Multimedia, Benchmark Testing, Recommendation Systems, Model Architecture, Comparison Studies (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1023/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>KGGLM: A Generative Language Model for Generalizable Knowledge Graph Representation Learning in Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Alessandro Soccol, Giacomo Balloccu, Gianni Fenu, Mirko Marras, Ludovico Boratto</p>
    <p>Current recommendation methods based on knowledge graphs rely on entity and relation representations for several steps along the pipeline, with knowledge completion and path reasoning being the most influential. Despite their similarities, the most effective representation methods for these steps differ, leading to inefficiencies, limited representativeness, and reduced interpretability. In this paper, we introduce KGGLM, a decoder-only Transformer model designed for generalizable knowledge representation learning to support recommendation. The model is trained on generic paths sampled from the knowledge graph to capture foundational patterns, and then fine-tuned on paths specific of the downstream step (knowledge completion and path reasoning in our case). Experiments on ML1M and LFM1M show that KGGLM beats twenty-two baselines in effectiveness under both knowledge completion and recommendation. Source code and pre-processed data sets are available at https://github.com/mirkomarras/kgglm.</p>
    <p><strong>Categories:</strong> Recommendation Algorithms, Knowledge Graphs, Generative Models, Transformer Models, Representation Learning, Knowledge Completion, Path Reasoning, Evaluation Methods, Effectiveness in Recommendations, Scalability (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1195/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Track Mix Generation on Music Streaming Services using Transformers (2023)</h3>
    <p><strong>Authors:</strong> Thomas Bouabça, Walid Bendada, Benjamin Chapus, Thibault Cador, Théo Bontempelli, Mathieu Morlon, Guillaume Salha-Galvan</p>
    <p>This paper introduces Track Mix, a personalized playlist generation system released in 2022 on the music streaming service Deezer. Track Mix automatically generates “mix” playlists inspired by initial music tracks, allowing users to discover music similar to their favorite content. To generate these mixes, we consider a Transformer model trained on millions of track sequences from user playlists. In light of the growing popularity of Transformers in recent years, we analyze the advantages, drawbacks, and technical challenges of using such a model for mix generation on the service, compared to a more traditional collaborative filtering approach. Since its release, Track Mix has been generating playlists for millions of users daily, enhancing their music discovery experience on Deezer.</p>
    <p><strong>Categories:</strong> Transformer Models, Music Streaming, Recommendation Systems, Deep Learning, Personalization, Language Modeling, Collaborative Filtering, Real-World Applications, User Engagement, Playlist Generation, Neural Networks, Music Domain, Scalability, Model Evaluation (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1016/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>STRec: Sparse Transformer for Sequential Recommendations (2023)</h3>
    <p><strong>Authors:</strong> Lixin Zou, Yiqi Wang, Qidong Liu, Wanyu Wang, Wenqi Fan, Yejing Wang, Chengxi Li, Qing Li, Xiangyu Zhao</p>
    <p>With the rapid evolution of transformer architectures, an increasing number of researchers are exploring their application in sequential recommender systems (SRSs). Compared with the former SRS models, the transformer-based models get promising performance on SRS tasks. Existing transformer-based SRS frameworks, however, retain the vanilla attention mechanism, which calculates the attention scores between all item-item pairs in each layer, i.e., item interactions. Consequently, redundant item interactions may downgrade the inference speed and cause high memory costs for the model. In this paper, we first identify the sparse information phenomenon in transformer-based SRS scenarios and propose an efficient model, i.e., Sparse Transformer sequential Recommendation model (STRec). First, we devise a cross-attention-based sparse transformer for efficient sequential recommendation. Then, a novel sampling strategy is derived to  preserve the necessary interactions. Extensive experimental results validate the effectiveness of our framework, which could outperform the state-of-the-art accuracy while reducing 54% inference time and 70% memory cost. Besides, we provide massive extended experiments to further investigate the property of our framework. Our code is available to ease reproducibility.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Sequential Recommendations, Transformer Models, Sparse Attention Mechanism, Efficiency Optimization, Inference Speed, Memory Management, Experimental Results, Model Design, Attention Mechanisms, Evaluation Metrics, Real-World Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/879/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Masked and Swapped Sequence Modeling for Next Novel Basket Recommendation in Grocery Shopping (2023)</h3>
    <p><strong>Authors:</strong> Andrew Yates, Mozhdeh Ariannezhad, Maarten de Rijke, Ming Li</p>
    <p>Next basket recommendation (NBR) is the task of predicting the next set of items based on a sequence of already purchased baskets. It is a recommendation task that has been widely studied, especially in the context of grocery shopping. In NBR, it is useful to distinguish between repeat items, i.e., items that a user has consumed before, and explore items, i.e., items that a user has not consumed before. Most NBR work either ignores this distinction or focuses on repeat items. We formulate the next novel basket recommendation (NNBR) task, i.e., the task of recommending a basket that only consists of novel items, which is valuable for both real-world application and NBR evaluation. We evaluate how existing NBR methods perform on the NNBR task and find that, so far, limited progress has been made w.r.t. the NNBR task. To address the NNBR task, we propose a simple bi-directional transformer basket recommendation model (BTBR), which is focused on directly modeling item-to-item correlations within and across baskets instead of learning complex basket representations. To properly train BTBR, we propose and investigate several masking strategies and training objectives: (i) item-level random masking, (ii) item-level select masking, (iii) basket-level all masking, (iv) item basket-level explore masking, and (v) joint masking. In addition, an item-basket swapping strategy is proposed to enrich the item interactions within the same baskets. We conduct extensive experiments on three open datasets with various characteristics. The results demonstrate the effectiveness of BTBR and our masking and swapping strategies for the NNBR task. BTBR with a properly selected masking and swapping strategy can substantially improve the NNBR performance.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Grocery Shopping, Basket Recommendation, Novelty, Transformer Models, Masking Strategies, Item-Item Correlations, Real-World Applications, Evaluation Methods, Sequence Modeling, Implicit Feedback (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/876/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>SSE-PT: Sequential Recommendation Via Personalized Transformer (2020)</h3>
    <p><strong>Authors:</strong> Shuqing Li, James Sharpnack, Liwei Wu, Cho-Jui Hsieh</p>
    <p>Temporal information is crucial for recommendation problems because user preferences are naturally dynamic in the real world. Recent advances in deep learning, especially the discovery of various attention mechanisms and newer architectures in addition to widely used RNN and CNN in natural language processing, have allowed for better use of the temporal ordering of items that each user has engaged with. In particular, the SASRec model, inspired by the popular Transformer model in natural languages processing, has achieved state-of-the-art results. However, SASRec, just like the original Transformer model, is inherently an un-personalized model and does not include personalized user embeddings. To overcome this limitation, we propose a Personalized Transformer (SSE-PT) model, outperforming SASRec by almost 5% in terms of NDCG@10 on 5 real-world datasets. Furthermore, after examining some random users’ engagement history, we find our model not only more interpretable but also able to focus on recent engagement patterns for each user. Moreover, our SSE-PT model with a slight modification, which we call SSE-PT++, can handle extremely long sequences and outperform SASRec in ranking results with comparable training speed, striking a balance between performance and speed requirements. Our novel application of the Stochastic Shared Embeddings (SSE) regularization is essential to the success of personalization. Code and data are open-sourced at https://github.com/wuliwei9278/SSE-PT.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Sequential Recommendations, Personalization, Transformer Models, Attention Mechanisms, Deep Learning, User Engagement History, Scalability, Model Interpretability, Evaluation Metrics, Real-World Applications, Regularization Techniques (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/554/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>What does BERT know about books, movies and music? Probing BERT for Conversational Recommendation (2020)</h3>
    <p><strong>Authors:</strong> Claudia Hauff, Gustavo Penha</p>
    <p>Heavily pre-trained transformer models such as BERT have recently shown to be remarkably powerful at language modelling, achieving impressive results on numerous downstream tasks. It has also been shown that they implicitly store factual knowledge in their parameters after pre-training. Understanding what the pre-training procedure of LMs actually learns is a crucial step for using and improving them for Conversational Recommender Systems (CRS). We first study how much off-the-shelf pre-trained BERT “knows” about recommendation items such as books, movies and music. In order to analyze the knowledge stored in BERT’s parameters, we use different probes (i.e., tasks to examine a trained model regarding certain properties) that require different types of knowledge to solve, namely content-based and collaborative-based. Content-based knowledge is knowledge that requires the model to match the titles of items with their content information, such as textual descriptions and genres. In contrast, collaborative-based knowledge requires the model to match items with similar ones, according to community interactions such as ratings. We resort to BERT’s Masked Language Modelling (MLM) head to probe its knowledge about the genre of items, with cloze style prompts. In addition, we employ BERT’s Next Sentence Prediction (NSP) head and representations’ similarity (SIM) to compare relevant and non-relevant search and recommendation query-document inputs to explore whether BERT can, without any fine-tuning, rank relevant items first. Finally, we study how BERT performs in a conversational recommendation downstream task. To this end, we fine-tune BERT to act as a retrieval-based CRS. Overall, our experiments show that: (i) BERT has knowledge stored in its parameters about the content of books, movies and music; (ii) it has more content-based knowledge than collaborative-based knowledge; and (iii) fails on conversational recommendation when faced with adversarial data.</p>
    <p><strong>Categories:</strong> BERT, Transformer Models, Conversational Recommender Systems, Books, Movies, Music, Probing Methods, Content-Based Knowledge, Collaborative-Based Knowledge, Masked Language Modeling (MLM), Next Sentence Prediction (NSP), Similarity (SIM) (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/564/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>