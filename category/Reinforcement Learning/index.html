<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/Large%20Language%20Models%20(LLMs)/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/Transfer%20Learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/Explainability/">Explainability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/Sequential%20Recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/Bias%20Mitigation/">Bias Mitigation</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/AB%20Testing/">AB Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/AB%20Test/">AB Test</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/Fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/Reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/Data%20Sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">Reinforcement Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/Evaluation%20Metrics/">Evaluation Metrics</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Multi-Preview Recommendation via Reinforcement Learning (2024)</h3>
    <p><strong>Authors:</strong> Kuan-Ting Lai, Pengcheng Xiong, Zhong Wu, Yang Xu</p>
    <p>Preview recommendations serve as a crucial shortcut for attracting users’ attention on various systems, platforms, and webpages, significantly boosting user engagement. However, the variability of preview types and the flexibility of preview duration make it challenging to use an integrated framework for multi-preview recommendations under resource constraints. In this paper, we present an approach that incorporates constrained Q-learning into a notification recommendation system, effectively handling both multi-preview ranking and duration orchestration by targeting long-term user retention. Our method bridges the gap between combinatorial reinforcement learning, which often remains too theoretical for practical use, and segmented modules in production, where model performance is typically compromised due to over-simplification. We demonstrate the superiority of our approach through off-policy evaluation and online A/B testing using Microsoft data.</p>
    <p><strong>Categories:</strong> Reinforcement Learning, Recommendation Systems, Multi-Preview, Notifications, User Engagement, Resource Constraints, Q-Learning, Combinatorial Optimization, Online Evaluation, A/B Testing, Microsoft Data, Production Systems (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1207/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Learning Personalized Health Recommendations via Offline Reinforcement Learning (2024)</h3>
    <p><strong>Authors:</strong> Larry Preuett</p>
    <p>The healthcare industry is strained and would benefit from personalized treatment plans for treating various health conditions (e.g., HIV and diabetes). Reinforcement Learning is a promising approach to learning such sequential recommendation systems. However, applying reinforcement learning in the medical domain is challenging due to the lack of adequate evaluation metrics, partial observability, and the inability to explore due to safety concerns. In this line of work, we identify three research directions to improve the applicability of treatment plans learned using offline reinforcement learning.</p>
    <p><strong>Categories:</strong> Reinforcement Learning, Offline Reinforcement Learning, Personalized Recommendations, Healthcare, Medical Domain, Evaluation Metrics, Safety Concerns, Health Conditions, Improving Methods, Sequential Recommendation (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1141/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>The Fault in Our Recommendations: On the Perils of Optimizing the Measurable (2024)</h3>
    <p><strong>Authors:</strong> Akshit Kumar, Yash Kanoria, Omar Besbes</p>
    <p>Recommendation systems play a pivotal role on digital platforms by curating content, products and services for users. These systems are widespread, and through customized recommendations, promise to match users with options they will like. To that end,  data on engagement is collected and used, with, e.g., measurements of clicks, but also  purchases or consumption times.   Most recommendation systems are ranking-based, where they rank and recommend items based on their predicted engagement.  However, the engagement signals  are often only a crude proxy for user utility, as data on the latter is rarely collected or available. This paper explores the following critical research question: By optimizing for measurable proxies, are recommendation systems at risk of significantly under-delivering on user utility? If that is indeed the case, how can one improve utility which is seldom measured? To study these questions, we introduce a model of repeated user consumption in which, at each interaction, users select between an outside option and the best option from a recommendation set. Our model accounts for user heterogeneity, with the majority preferring “popular” content, and a minority favoring “niche” content. The system initially lacks knowledge of individual user preferences but can learn these preferences through observations of users’ choices over time. Our theoretical and numerical analysis demonstrate that optimizing for engagement signals can lead to significant utility losses. Instead, we propose a utility-aware policy that initially recommends a mix of popular and niche content. We show that such a policy substantially improves  utility despite not measuring it. In fact, in the limit of a forward-looking platform with discount factor $\delta \to 1$, our utility-aware policy achieves the best of both worlds: near-optimal user utility and near-optimal engagement simultaneously. Our study elucidates  an important feature of recommendation systems; given the ability to suggest multiple items, one can perform significant exploration without incurring significant reductions in short term engagement. By recommending high-risk, high-reward items alongside popular items, systems can enhance discovery of high utility items without significantly affecting engagement.</p>
    <p><strong>Categories:</strong> Optimization, Evaluation Metrics, Recommendation Systems, User Behavior, Engagement, Machine Learning, Reinforcement Learning, Bandit Algorithms, Ranking Algorithms, Digital Platforms, Personalization, Cold Start, Exploration vs Exploitation, Long-term User Satisfaction, Scalability, Theoretical Modeling, Numerical Analysis, User Heterogeneity, Ethical Considerations in AI, Beyond Accuracy, High-Risk High-Reward Recommendations (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1062/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Optimal Baseline Corrections for Off-Policy Contextual Bandits (2024)</h3>
    <p><strong>Authors:</strong> Shashank Gupta, Olivier Jeunen, Maarten de Rijke, Harrie Oosterhuis</p>
    <p>The off-policy learning paradigm allows for recommender systems and general ranking applications to be framed as decision-making problems, where we aim to learn decision policies that optimize an unbiased offline estimate of an online reward metric. With unbiasedness comes potentially high variance, and prevalent methods exist to reduce estimation variance. These methods typically make use of control variates, either additive (i.e., baseline corrections or doubly robust methods) or multiplicative (i.e., self-normalisation). Our work unifies these approaches by proposing a single framework built on their equivalence in learning scenarios. The foundation of our framework is the derivation of an equivalent baseline correction for all of the existing control variates. Consequently, our framework enables us to characterize the variance-optimal unbiased estimator and provide a closed-form solution for it. This optimal estimator brings significantly improved performance in both evaluation and learning, and minimizes data requirements. Empirical observations corroborate our theoretical findings.</p>
    <p><strong>Categories:</strong> Contextual Bandits, Off-Policy Learning, Control Variates, Baseline Corrections, Doubly Robust Methods, Variance Reduction, Optimal Estimation, Recommender Systems, Ranking, Reinforcement Learning (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1052/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>RPAF: A Reinforcement Prediction-Allocation Framework for Cache Allocation in Large-Scale Recommender Systems (2024)</h3>
    <p><strong>Authors:</strong> Kaiqiao Zhan, Kun Gai, Xiaoshuang Chen, Yao Wang, Ziqiang Zhang, Ben Wang, Yulin Wu, Shuo Su</p>
    <p>Modern recommender systems are built upon computation-intensive infrastructure, and it is challenging to perform real-time computation for each request, especially in peak periods, due to the limited computational resources. Recommending by user-wise result caches is widely used when the system cannot afford a real-time recommendation. However, it is challenging to allocate real-time and cached recommendations to maximize the users’ overall engagement. This paper shows two key challenges to cache allocation, i.e., the temporal dependency and the streaming allocation. Then, we propose a reinforcement prediction-allocation framework (RPAF) to address these issues. RPAF is a reinforcement-learning-based two-stage framework containing prediction and allocation stages. The prediction stage estimates the values of the cache choices considering the strategy and value dependencies, while the allocation stage determines the cache choices for each request. We show that the challenge of training RPAF includes globality and the strictness of budget constraints, and a relaxed local allocator (RLA) is proposed to address this issue. Moreover, a PoolRank algorithm is used in the allocation stage to deal with the streaming allocation problem. Experiments show that RPAF significantly improves users’ engagement under computational budget constraints.</p>
    <p><strong>Categories:</strong> Reinforcement Learning, Cache Management, Recommender Systems, Real-time Recommendations, Scalability, Streaming Allocation, Optimization, Computational Constraints, Algorithm Design, Evaluation Metrics, Temporal Dependency, Recommendation Strategies (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1058/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Off-Policy Selection for Optimizing Ad Display Timing in Mobile Games (Samsung Instant Plays) (2024)</h3>
    <p><strong>Authors:</strong> Michał Romaniuk, Katarzyna Siudek-Tkaczuk, Sławomir Kapka, Jędrzej Alchimowicz, Bartłomiej Swoboda</p>
    <p>Off-Policy Selection (OPS) aims to select the best policy form a set of policies trained using offline Reinforcement Learning. In this work, we describe our custom OPS method and its successful application in Samsung Instant Plays for optimizing ad delivery timings. The motivation behind proposing our custom OPS method is the fact that traditional Off-Policy Evaluation (OPE) methods often exhibit enormous variance leading to unreliable results. We applied our OPS method to initialize policies for ours custom pseudo-online training pipeline. The final policy resulted in a substantial 49% lift in the number of watched ads while maintaining similar retention rate.</p>
    <p><strong>Categories:</strong> Off-Policy Selection, Reinforcement Learning, Offline Evaluation, Mobile Games, Digital Advertising, Ad Display Optimization, Policy Selection, Variance Reduction, User Retention, Custom Pipelines (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1172/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Incentivizing Exploration in Linear Contextual Bandits under Information Gap (2023)</h3>
    <p><strong>Authors:</strong> Zhiyuan Liu, Hongning Wang, Huazheng Wang, Chuanhao Li, Haifeng Xu</p>
    <p>Contextual bandit algorithms have been popularly used to address interactive recommendation, where the users are assumed to be cooperative to explore all recommendations from a system. In this paper, we relax this strong assumption and study the problem of incentivized exploration with myopic users, where the users are only interested in recommendations with their currently highest estimated reward. As a result, in order to obtain long-term optimality, the system needs to offer compensation to incentivize the users to take the exploratory recommendations. We consider a new and practically motivated setting where the context features employed by the user are more <i>informative</i> than those used by the system: for example, features based on users’ private information are not accessible by the system. We develop an effective solution for incentivized exploration under such an information gap, and prove that the method achieves a sublinear rate in both regret and compensation. We theoretically and empirically analyze the added compensation due to the information gap, compared with the case where the system has access to the same context features as the user does, i.e., without information gap. Moreover, we also provide a compensation lower bound of this problem.</p>
    <p><strong>Categories:</strong> Contextual Bandits, Reinforcement Learning, Exploration vs Exploitation, Incentivizing Exploration, User Behavior Modeling, Interactive Recommendations, Information Gap, Regret Analysis, Compensation Mechanisms, Theoretical Analysis, Empirical Evaluation, Multi-Armed Bandits (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/874/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Generative Learning Plan Recommendation for Employees: A Performance-aware Reinforcement Learning Approach (2023)</h3>
    <p><strong>Authors:</strong> Ying Sun, Xin Song, Hengshu Zhu, Hui Xiong, Zhi Zheng</p>
    <p>With the rapid development of enterprise Learning Management Systems (LMS), more and more companies are trying to build enterprise training and course learning platforms for promoting the career development of employees. Indeed, through course learning, many employees have the opportunity to improve their knowledge and skills. For these systems, a major issue is how to recommend learning plans, i.e., a set of courses arranged in the order they should be learned, that can help employees improve their work performance. Existing studies mainly focus on recommending courses that users are most likely to click on by capturing their learning preferences. However, the learning preference of employees may not be the right fit for their career development, and thus it may not necessarily mean their work performance can be improved accordingly. Furthermore, how to capture the mutual correlation and sequential effects between courses, and ensure the rationality of the generated results, is also a major challenge. To this end, in this paper, we propose the Generative Learning plAn recommenDation (GLAD) framework, which can generate personalized learning plans for employees to help them improve their work performance. Specifically, we first design a performance predictor and a rationality discriminator, which have the same transformer-based model architecture, but with totally different parameters and functionalities. In particular, the performance predictor is trained for predicting the work performance of employees based on their work profiles and historical learning records, while the rationality discriminator aims to evaluate the rationality of the generated results. Then, we design a learning plan generator based on the gated transformer and the cross-attention mechanism for learning plan generation. We calculate the weighted sum of the output from the performance predictor and the rationality discriminator as the reward, and we use Self-Critical Sequence Training (SCST) based policy gradient methods to train the generator following the Generative Adversarial Network (GAN) paradigm. Finally, extensive experiments on real-world data clearly validate the effectiveness of our GLAD framework compared with state-of-the-art baseline methods and reveal some interesting findings for talent management</p>
    <p><strong>Categories:</strong> Employee Training, Career Development, Learning Plan Generation, Reinforcement Learning, Sequence Modeling, Performance Prediction, Rationality Evaluation, Dual-Model Framework, Algorithm Design, Policy Gradient Methods, GAN-based Models, Practical Applications, Performance Comparison (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/868/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Towards Companion Recommenders Assisting Users’ Long-Term Journeys (2023)</h3>
    <p><strong>Authors:</strong> Minmin Chen, Konstantina Christakopoulou</p>
    <p>Nowadays, with the abundance of the internet content, users expect the recommendation platforms to not only help them with one-off decisions and short-term tasks, but to also support their persistent and overarching interest journeys, including their real-life goals that last days, months or even years. In order for recommender systems to truly assist users through their real-life journeys, they need to first be able to understand and reason about interests, needs, and goals users want to pursue; and then plan taking those into account. However, the task presents several challenges. In this talk, we will present the key steps and elements needed to tackle the problem — particularly (1) user research for interest journeys; (2) personalized and interpretable user profiles; (3) adapting large language models, and other foundational models, for better user understanding; (4) better planning at a macro-level through reinforcement learning and reason-and-act conversational agents; (5) novel journey-powered front end user experiences, allowing for more user control. We hope that the talk will help inspire other researchers, and will pave the way towards companion recommenders that can truly assist the users throughout their interest journeys.</p>
    <p><strong>Categories:</strong> Companion Recommenders, Long-term Goals, User Journeys, Personalized Recommendations, Large Language Models, Foundational Models, Reinforcement Learning, Conversational Agents, Reasoning Mechanisms, Interest Journey Modeling, User Control, Human-Centered Design, Sustained Engagement (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1019/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>InTune: Reinforcement Learning-based Data Pipeline Optimization for Deep Recommendation Models (2023)</h3>
    <p><strong>Authors:</strong> Pablo Delgado, Kabir Nagrecha, Lingyi Liu, Prasanna Padmanabhan</p>
    <p>Deep learning-based recommendation models (DLRMs) have become an essential component of many modern recommender systems. Several companies are now building large compute clusters reserved only for DLRM training, driving new interest in cost- & time- saving optimizations. The systems challenges faced in this setting are unique; while typical deep learning (DL) training jobs are dominated by model execution times, the most important factor in DLRM training performance is often online data ingestion. In this paper, we explore the unique characteristics of this data ingestion problem and provide insights into the specific bottlenecks and challenges of the DLRM training pipeline at scale. We study real-world DLRM data processing pipelines taken from our compute cluster to both observe the performance impacts of online ingestion and to identify shortfalls in existing data pipeline optimizers. We find that current tooling either yields sub-optimal performance, frequent crashes, or else requires impractical cluster re-organization to adopt. Our studies lead us to design and build a new solution for data pipeline optimization, InTune. InTune employs a reinforcement learning (RL) agent to learn how to distribute CPU resources across a DLRM data pipeline to more effectively parallelize data-loading and improve throughput. Our experiments show that InTune can build an optimized data pipeline configuration within only a few minutes, and can easily be integrated into existing training workflows. By exploiting the responsiveness and adaptability of RL, InTune achieves significantly higher online data ingestion rates than existing optimizers, thus reducing idle times in model execution and increasing efficiency. We apply InTune to our real-world cluster, and find that it increases data ingestion throughput by as much as 2.29X versus current state-of-the-art data pipeline optimizers while also improving both CPU & GPU utilization.</p>
    <p><strong>Categories:</strong> Reinforcement Learning, Deep Learning Recommender Models, Data Pipeline Optimization, Resource Allocation, Performance Optimization, Real-World Applications, Large-Scale Systems, Compute Clusters, Resource Utilization, Deep Learning, Scalability (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/873/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Scalable Deep Q-Learning for Session-Based Slate Recommendation (2023)</h3>
    <p><strong>Authors:</strong> Aonghus Lawlor, Edoardo D’Amico, Aayush Singha Roy, Neil Hurley, Elias Tragos</p>
    <p>Reinforcement learning (RL) has demonstrated great potential to improve slate-based recommender systems by optimizing recommendations for long-term user engagement. To handle the combinatorial action space in slate recommendation, recent works decompose the Q-value of a slate into item-wise Q-values, using an item-wise value-based policy. However, the common case where the value function is a parameterized function taking state and action as input results in a linearly increasing number of evaluations required to select an action, proportional to the number of candidate items. While slow training may be acceptable, this becomes intractable when considering the costly evaluation of the parameterized function, such as with deep neural networks, during model serving time. To address this issue, we propose an actor-based policy that reduces the evaluation of the Q-function to a subset of items, significantly reducing inference time and enabling practical deployment in real-world industrial settings. In our empirical evaluation, we demonstrate that our proposed approach achieves equivalent user session engagement to a value-based policy, while significantly reducing the slate serving time by at least 4 times.</p>
    <p><strong>Categories:</strong> Reinforcement Learning, Deep Q-Learning, Recommendation Systems, Slate Recommendation, Session-Based, Combinatorial Action Space, Deep Neural Networks, Long-Term User Engagement, Scalable Deep Q-Learning, Practical Deployment, Actor-Based Policy, Multi-Item Recommendation (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/924/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Integrating Offline Reinforcement Learning with Transformers for Sequential Recommendation (2023)</h3>
    <p><strong>Authors:</strong> Xumei Xi, Yang Wu, Liwen Ouyang, Yuke Zhao, Quan Liu</p>
    <p>We consider the problem of sequential recommendation, where the current recommendation is made based on past interactions. This recommendation task requires efficient processing of the sequential data and aims to provide recommendations that maximize the long-term reward. To this end, we train a farsighted recommender by using an offline RL algorithm with the policy network in our model architecture that has been initialized from a pre-trained transformer model. The pre-trained model leverages the superb ability of the transformer to process sequential information. Compared to prior works that rely on online interaction via simulation, we focus on implementing a fully offline RL framework that is able to converge in a fast and stable way. Through extensive experiments on public datasets, we show that our method is robust across various recommendation regimes, including e-commerce and movie suggestions. Compared to state-of-the-art supervised learning algorithms, our algorithm yields recommendations of higher quality, demonstrating the clear advantage of combining RL and transformers.</p>
    <p><strong>Categories:</strong> Reinforcement Learning, Transformers, Sequential Recommendation, Offline Reinforcement Learning, Sequential Data Processing, Transfer Learning, E-commerce, Movies, Recommendation System, Long-term Reward Maximization, Algorithm Comparison, Efficiency (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/955/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>User-Centric Conversational Recommendation: Adapting the Need of User with Large Language Models (2023)</h3>
    <p><strong>Authors:</strong> Gangyi Zhang</p>
    <p>Conversational recommender systems (CRS) promise to provide a more natural user experience for exploring and discovering items of interest through ongoing conversation. However, effectively modeling user preferences during conversations and generating personalized recommendations in real time remain challenging problems. Users often express their needs in a vague and evolving manner, and CRS must adapt to capture the dynamics and uncertainty in user preferences to have productive interactions. This research develops user-centric methods for building conversational recommendation system that can understand complex and changing user needs. We propose a graph-based conversational recommendation framework that represents multi-turn conversations as reasoning over a user-item-attribute graph. Enhanced conversational path reasoning incorporates graph neural networks to improve representation learning in this framework. To address uncertainty and dynamics in user preferences, we present the vague preference multi-round conversational recommendation scenario and an adaptive vague preference policy learning solution that employs reinforcement learning to determine recommendation and preference elicitation strategies tailored to the user. Looking to the future, large language models offer promising opportunities to enhance various aspects of CRS, including user modeling, policy learning, response generation.  Overall, this research takes a user-centered perspective in designing conversational agents that can adapt to the inherent ambiguity involved in natural language dialogues with people.</p>
    <p><strong>Categories:</strong> Conversational Recommender Systems, Large Language Models, Graph Neural Networks, Reinforcement Learning, User Dynamics, Conversational Agents, Personalization, Real-Time Recommendations, Multi-Round Dialogue, AI/ML in Recommendations, User-Centered Design (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/983/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Optimizing Long-term Value for Auction-Based Recommender Systems via On-Policy Reinforcement Learning (2023)</h3>
    <p><strong>Authors:</strong> Fan Liu, Dmytro Korenkevych, Yuchen He, Jalaj Bhandari, Alex Nikulkov, Zheqing Zhu, Ruiyang Xu</p>
    <p>Auction-based recommender systems are prevalent in online advertising platforms, but they are typically optimized to allocate recommendation slots based on immediate expected return metrics, neglecting the downstream effects of recommendations on user behavior. In this study, we employ reinforcement learning to optimize for long-term return metrics in an auction-based recommender system. Utilizing temporal difference learning, a fundamental reinforcement learning algorithm, we implement a <i>one-step policy improvement approach</i> that biases the system towards recommendations with higher long-term user engagement metrics. This optimizes value over long horizons while maintaining compatibility with the auction framework. Our approach is based on dynamic programming ideas which show that our method provably improves upon the existing auction-based base policy. Through an online A/B test conducted on an auction-based recommender system, which handles billions of impressions and users daily, we empirically establish that our proposed method outperforms the current production system in terms of long-term user engagement metrics.</p>
    <p><strong>Categories:</strong> Auction-Based Recommender Systems, Reinforcement Learning, Temporal Difference Learning, Policy Iteration, Online Advertising, Long-Term Value Optimization, Beyond Accuracy, Real-World Applications, On-Policy Reinforcement Learning, User Engagement Metrics, Dynamic Programming, Theoretical Approach (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/897/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Reproducibility of Multi-Objective Reinforcement Learning Recommendation: Interplay between Effectiveness and Beyond-Accuracy Perspectives (2023)</h3>
    <p><strong>Authors:</strong> Vincenzo Paparella, Ludovico Boratto, Vito Walter Anelli, Tommaso Di Noia</p>
    <p>Providing effective suggestions is of predominant importance for successful Recommender Systems (RSs). Nonetheless, the need of accounting for additional multiple objectives has become prominent, from both the final users’ and the item providers’ points of view. This need has led to a new class of RSs, called Multi-Objective Recommender Systems (MORSs). These systems are designed to provide suggestions by considering multiple (conflicting) objectives simultaneously, such as diverse, novel, and fairness-aware recommendations. In this work, we reproduce a state-of-the-art study on MORSs that exploits a reinforcement learning agent to satisfy three objectives, i.e., accuracy, diversity, and novelty of recommendations. The selected study is one of the few MORSs where the source code and datasets are released to ensure the reproducibility of the proposed approach. Interestingly, we find that some challenges arise when replicating the results of the original work, due to the nature of multiple-objective problems. We also extend the evaluation of the approach to analyze the impact of improving user-centred objectives of recommendations (i.e., diversity and novelty) in terms of algorithmic bias. To this end, we take into consideration both popularity and category of the items. We discover some interesting trends in the recommendation performance according to different evaluation metrics. In addition, we see that the multi-objective reinforcement learning approach is responsible for increasing the bias disparity in the output of the recommendation algorithm for those items belonging to positively/negatively biased categories. We publicly release datasets and codes in the following GitHub repository: https://anonymous.4open.science/r/MORS_reproducibility-BD60</p>
    <p><strong>Categories:</strong> Recommender Systems, Multi-Objective Recommender Systems, Reinforcement Learning, Reproducibility, Algorithmic Bias, Diversity of Recommendations, Novelty, Fairness-aware Recommendations, Beyond Accuracy, Evaluation Metrics, Multi-Objective Optimization, Open Source (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/946/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Automate Page Template Optimization: An Offline Deep Q-Learning Approach (2022)</h3>
    <p><strong>Authors:</strong> Wenyang Liu, Zhou Qin</p>
    <p>The modern e-commerce web pages have brought better customer experience and more profitable services by whole page optimization at different granularity, e.g., page layout optimization, item ranking optimization, etc. Generating the proper page layout per customer’s request is one of the vital tasks during the web page rendering process, which can directly impact customers’ shopping experience and their decision-making. In this paper, we formulate the request-rendering interactions as a Markov decision process (MDP) and solve it by deep reinforcement learning (RL). Specifically, we present the design and implementation of applying offline Deep Q-Learning (DQN) to the contextual page layout optimization problem. Through the offline evaluation method, we demonstrate the effectiveness of the proposed framework, i.e., the RL agent has the potential to perform better than the baseline ranker by learning from the offline data set, e.g., the RL agent can improve the average cumulative rewards up to 36.69% comparing to the baseline ranker.</p>
    <p><strong>Categories:</strong> Reinforcement Learning, Deep Learning, E-commerce Optimization, Web Page Layout Optimization, Markov Decision Process (MDP), Offline Deep Q-Learning, Customer Experience, Performance Improvement, Profitability Optimization, Algorithm Design, Offline Evaluation Methods, Machine Learning Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/832/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>