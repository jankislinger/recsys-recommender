<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">Hyperparameter Optimization</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/beyond-accuracy/">Beyond Accuracy</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/explainability/">Explainability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reinforcement-learning/">Reinforcement Learning</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Recommender Systems Algorithm Selection for Ranking Prediction on Implicit Feedback Datasets (2024)</h3>
    <p><strong>Authors:</strong> Joeran Beel, Lukas Wegmeth, Tobias Vente</p>
    <p>The recommender systems algorithm selection problem for ranking prediction on implicit feedback datasets is under-explored. Traditional approaches in recommender systems algorithm selection focus predominantly on rating prediction on explicit feedback datasets, leaving a research gap for ranking prediction on implicit feedback datasets. Algorithm selection is a critical challenge for nearly every practitioner in recommender systems. In this work, we take the first steps toward addressing this research gap.<br> We evaluate the NDCG@10 of 24 recommender systems algorithms, each with two hyperparameter configurations, on 72 recommender systems datasets. We train four optimized machine-learning meta-models and one automated machine-learning meta-model with three different settings on the resulting meta-dataset.<br> Our results show that the predictions of all tested meta-models exhibit a median Spearman correlation ranging from 0.857 to 0.918 with the ground truth. We show that the median Spearman correlation between meta-model predictions and the ground truth increases by an average of 0.124 when the meta-model is optimized to predict the ranking of algorithms instead of their performance. Furthermore, in terms of predicting the best algorithm for an unknown dataset, we demonstrate that the best optimized traditional meta-model, e.g., XGBoost, achieves a recall of 48.6%, outperforming the best tested automated machine learning meta-model, e.g., AutoGluon, which achieves a recall of 47.2%.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Algorithm Selection, Ranking Prediction, Implicit Feedback Datasets, Meta-Model Selection, Hyperparameter Optimization, Beyond Accuracy, Evaluation Metrics (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1199/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>AutoOpt: Automatic Hyperparameter Scheduling and Optimization for Deep Click-through Rate Prediction (2023)</h3>
    <p><strong>Authors:</strong> Yimin Huang, Bo Chen, Yujun Li, Zhenguo Li, Xing Tang, Ruiming Tang</p>
    <p>Click-through Rate (CTR) prediction is essential for commercial recommender systems. Recently, to improve the prediction accuracy, plenty of deep learning-based CTR models have been proposed, which are sensitive to hyperparameters and difficult to optimize well. General hyperparameter optimization methods fix these hyperparameters across the entire model training and repeat them multiple times. This trial-and-error process not only leads to suboptimal performance but also requires non-trivial computation efforts. In this paper, we propose an automatic hyperparameters scheduling and optimization method for deep CTR models, <i>AutoOpt</i>, making the optimization process more stable and efficient. Specifically, the whole training regime is firstly divided into several consecutive stages, where a data-efficient model is learned to model the relation between model states and prediction performance. To optimize the stage-wise hyperparameters, AutoOpt uses the <i>global</i> and <i>local</i> scheduling modules to propose proper hyperparameters for the next stage based on the training in the current stage. Extensive experiments on three public benchmarks are conducted to validate the effectiveness of AutoOpt. Moreover, AutoOpt has been deployed onto an advertising platform and a music platform, where online A/B tests also demonstrate superior improvement.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Deep Learning, Hyperparameter Optimization, AutoML, Real-World Applications, Industry Applications, Beyond Accuracy, Web Systems, Deep Learning Challenges, User Behavior (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/856/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Introducing LensKit-Auto, an Experimental Automated Recommender System (AutoRecSys) Toolkit (2023)</h3>
    <p><strong>Authors:</strong> Tobias Vente, Michael Ekstrand, Joeran Beel</p>
    <p>LensKit is one of the first and most popular Recommender System Libraries. While LensKit offers a wide variety of features, it does not include any optimization strategies or guidelines on how to select and tune LensKit algorithms. LensKit developers have to manually include third-party libraries into their experimental setup or implement optimization strategies by hand to optimize hyperparameters. We found that 65.5% (19 out of 29) of papers using LensKit algorithms for their experiments did not select algorithms or tune hyperparameters. Non-optimized models represent poor baselines and produce less meaningful research results. This demo introduces LensKit-Auto. LensKit-Auto automates the entire Recommender System pipeline and enables LensKit developers to automatically select, optimize, and ensemble LensKit algorithms.</p>
    <p><strong>Categories:</strong> Automated Recommendations, Recommender Systems, Algorithm Selection, Hyperparameter Optimization, Pipeline Automation, Research Tools, Algorithm Ensembles, Performance Evaluation (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/973/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Dynamic Surrogate Switching: Sample-Efficient Search for Factorization Machine Configurations in Online Recommendations (2022)</h3>
    <p><strong>Authors:</strong> Jure Ferlež, Adi Schwartz, Naama Ziporin, Blaz Skrlj</p>
    <p>Hyperparameter optimization is the process of identifying the appropriate hyperparameter configuration of a given machine learning model with regard to a given learning task. For smaller data sets, an exhaustive search is possible; However, when the data size and model complexity increase, the number of configuration evaluations becomes the main computational bottleneck. A promising paradigm for tackling this type of problem is surrogate-based optimization. The main idea underlying this paradigm considers an incrementally updated model of the relation between the hyperparameter space and the output (target) space; the data for this model are obtained by evaluating the main learning engine, which is, for example, a factorization machine-based model. By learning to approximate the hyperparameter-target relation, the surrogate (machine learning) model can be used to score large amounts of hyperparameter configurations, exploring parts of the configuration space beyond the reach of direct machine learning engine evaluation. Commonly, a surrogate is selected prior to optimization initialization and remains the same during the search. We investigated whether dynamic switching of surrogates during the optimization itself is a sensible idea of practical relevance for selecting the most appropriate factorization machine-based models for large-scale online recommendation. We conducted benchmarks on data sets containing hundreds of millions of instances against established baselines such as Random Forest- and Gaussian process-based surrogates. The results indicate that surrogate switching can offer good performance while considering fewer learning engine evaluations.</p>
    <p><strong>Categories:</strong> Dynamic Surrogate Switching, Hyperparameter Optimization, Factorization Machines, Online Recommendations, Large-Scale Recommendations, Surrogate Models, Random Forest, Gaussian Process, Evaluation Metrics, Beyond Accuracy, Sample Efficiency, Search Strategies, Benchmarking, Real-World Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/831/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>V-Elliot: Design, Evaluate and Tune Visual Recommender Systems (2021)</h3>
    <p><strong>Authors:</strong> Claudio Pomo, Felice Antonio Merra, Alejandro Bellogin, Vito Walter Anelli, Daniele Malitesta, Antonio Ferrara, Francesco M Donini, Tommaso Di Noia</p>
    <p>The paper introduces Visual-Elliot (V-Elliot), a reproducibility framework for Visual Recommendation systems (VRSs) based on Elliot. framework provides the widest set of VRSs compared to other recommendation frameworks in the literature (i.e., 6 state-of-the-art models which have been commonly employed as baselines in recent works). The framework pipeline spans from the dataset preprocessing and item visual features loading to easily train and test complex combinations of visual models and evaluation settings. V-Elliot provides an extended set of features to ease the design, testing, and integration of novel VRSs into V-Elliot. The framework exploits of dataset filtering/splitting functions, 40 evaluation metrics, five hyper-parameter optimization methods, more than 50 recommendation algorithms, and two statistical hypothesis tests. The files of this demonstration are available at: github.com/sisinflab/elliot.</p>
    <p><strong>Categories:</strong> Visual Recommender Systems, Recommender System Frameworks, Reproducibility, Baseline Models, Evaluation Metrics, Hyperparameter Optimization, Dataset Preprocessing, Statistical Tests, Open Source Tools, System Design, Framework Evaluation, Tools and Systems (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/710/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Tuning Word2vec for Large Scale Recommendation Systems (2020)</h3>
    <p><strong>Authors:</strong> Suvash Sedhain, Ben Chamberlain, Dan Shiebler, Emanuele Rossi, Michael Bronstein</p>
    <p>Word2vec is a powerful machine learning tool that emerged from Natural Language Processing (NLP) and is now applied in multiple domains, including recommender systems, forecasting, and network analysis. As Word2vec is often used off the shelf, we address the question of whether the default hyperparameters are suitable for recommender systems. The answer is emphatically no. In this paper, we first elucidate the importance of hyperparameter optimization and show that unconstrained optimization yields an average 221% improvement in hit rate over the default parameters. However, unconstrained optimization leads to hyperparameter settings that are very expensive and not feasible for large scale recommendation tasks. To this end, we demonstrate 138% average improvement in hit rate with a runtime budget-constrained hyperparameter optimization. Furthermore, to make hyperparameter optimization applicable for large scale recommendation problems where the target dataset is too large to search over, we investigate generalizing hyperparameters settings from samples. We show that applying constrained hyperparameter optimization using only a 10% sample of the data still yields a 91% average improvement in hit rate over the default parameters when applied to the full datasets. Finally, we apply hyperparameters learned using our method of constrained optimization on a sample to the Who To Follow recommendation service at Twitter and are able to increase follow rates by 15%.</p>
    <p><strong>Categories:</strong> Recommender Systems, Word2vec, Hyperparameter Optimization, Large Scale Systems, Evaluation Metrics, Real-World Applications, NLP, Social Networks. (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/615/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>BETA-Rec: Build, Evaluate and Tune Automated Recommender Systems (2020)</h3>
    <p><strong>Authors:</strong> Iadh Ounis, Yaxiong Wu, Shangsong Liang, Zaiqiao Meng, Craig Macdonald, Guangtao Zeng, Richard McCreadie, Siwei Liu, Yucheng Liang, Qiang Zhang, Junhua Liang</p>
    <p>The field of recommender systems has rapidly evolved over the last few years, with significant advances made due to the in-flux of deep learning techniques. However, as a result of this rapid progress, escalating barriers-to-entry for new researchers is emerging. In particular, state-of-the-art approaches have fragmented into a large number of code-bases, often requiring different input formats, pre-processing stages and evaluating with different metric packages. Hence, it is time-consuming for new researchers to reach the point of having both an effective baseline set and a sound comparative environment. As a step towards elevating this problem, we have developed BETA-Rec, an open source project for Building, Evaluating and Tuning Automated Recommender Systems. BETA-Rec aims to provide a practical data toolkit for building end-to-end recommendation systems in a standardized way. It provides means for dataset preparation and splitting using common strategies, a generalized model engine for implementing recommender models using Pytorch with 9 models available out-of-the-box, as well as a unified training, validation, tuning and testing pipeline. Furthermore, BETA-Rec is designed to be both modular and extensible, enabling new models to be quickly added to the framework. It is deployable in a wide range of environments via pre-built docker containers and supports distributed parameter tuning using Ray. In this demo, we will illustrate the deployment and use of BETA-Rec for researchers and practitioners on a number of standard recommendation datasets. The source code of the project is available at github: https://github.com/beta-team/beta-recsys.</p>
    <p><strong>Categories:</strong> Recommender Systems, Machine Learning Methods, Open Source Tools, Pre-trained Models, Hyperparameter Optimization, Deployment, Practical Applications, Modular Frameworks, Distributed Computing, Data Preprocessing, Benchmarking (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/594/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Word2vec applied to Recommendation: Hyperparameters Matter (2018)</h3>
    <p><strong>Authors:</strong> Jimena Royo-Letelier, Hugo Caselles-Dupré, Florian Lesaint</p>
    <p>Skip-gram with negative sampling, a popular variant of Word2vec originally designed and tuned to create word embeddings for Natural Language Processing, has been used to create item embeddings with successful applications in recommendation. While these fields do not share the same type of data, neither evaluate on the same tasks, recommendation applications tend to use the same already tuned hyperparameters values, even if optimal hyperparameters values are often known to be data and task dependent. We thus investigate the marginal importance of each hyperparameter in a recommendation setting through large hyperparameter grid searches on various datasets. Results reveal that optimizing neglected hyperparameters, namely negative sampling distribution, number of epochs, subsampling parameter and window-size, significantly improves performance on a recommendation task, and can increase it by an order of magnitude. Importantly, we find that optimal hyperparameters configurations for Natural Language Processing tasks and Recommendation tasks are noticeably different.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Word Embeddings, Collaborative Filtering, Algorithm Performance, Hyperparameter Optimization, Item Embeddings, Cross-Domain Applications, Beyond Accuracy, Explicit Feedback, Implicit Feedback, Domain Adaptation (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/407/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>