<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reinforcement-learning/">Reinforcement Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/evaluation-methods/">Evaluation Methods</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/multi-armed-bandits/">Multi-Armed Bandits</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Positive-Sum Impact of Multistakeholder Recommender Systems for Urban Tourism Promotion and User Utility (2024)</h3>
    <p><strong>Authors:</strong> Francesco Ricci, Pavel Merinov</p>
    <p>We explore how multistakeholder recommender systems (MRSs) can impact urban tourism promotion. Usually, two conflicting goals are of interest: (i) to cut down the number of visitors at popular sites and (ii) to satisfy individual tourist preferences, often biased towards popular sites. By leveraging users’ limited knowledge of the sites catalogue, we model and simulate interactions between tourists and an MRS that jointly optimises user utility and promotes less popular sites. Experiments based on data logs collected in tourist cities reveal that an MRS can lift user utility and at the same time reduce the number of visitors at popular sites, manifesting the so-called positive-sum impact. However, a delicate balance is crucial; under- or over-promotion of unpopular sites in recommendation lists can be adverse to both destination and user utilities.</p>
    <p><strong>Categories:</strong> Multistakeholder Recommender Systems, Urban Tourism, Recommender Systems, Tourist Behavior, Positive-Sum Impact, Balanced Optimization, User Satisfaction, Destination Management, Recommendation Strategies, Crowdsourcing, Tourism Promotion, Behavioral Modeling (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1108/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Soliciting User Preferences in Conversational Recommender Systems via Usage-related Questions (2021)</h3>
    <p><strong>Authors:</strong> Ivica Kostric, Filip Radlinski, Krisztian Balog</p>
    <p>A key distinguishing feature of conversational recommender systems over traditional recommender systems is their ability to elicit user preferences using natural language. Currently, the predominant approach to preference elicitation is to ask questions directly about items or item attributes. These strategies do not perform well in cases where the user does not have sufficient knowledge of the target domain to answer such questions. Conversely, in a shopping setting, talking about the planned use of items does not present any difficulties, even for those that are new to a domain. In this paper, we propose a novel approach to preference elicitation by asking implicit questions based on item usage. Our approach consists of two main steps. First, we identify the sentences from a large review corpus that contain information about item usage. Then, we generate implicit preference elicitation questions from those sentences using a neural text-to-text model. The main contributions of this work also include a multi-stage data annotation protocol using crowdsourcing for collecting high-quality labeled training data for the neural model. We show that out approach is effective in selecting review sentences and transforming them to elicitation questions, even with limited training data.</p>
    <p><strong>Categories:</strong> Conversational Recommender Systems, Natural Language Processing, Preference Elicitation, Implicit Feedback, Neural Models, Text Generation, Data Annotation, Crowdsourcing, Recommendation Systems, Review Mining, User Interaction, Usage-Based Recommendations (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/703/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>A Crowdsourcing Triage Algorithm for Geopolitical Forecasting (2018)</h3>
    <p><strong>Authors:</strong> David Huber, Tsai-Ching Lu, Mohammad Rostami</p>
    <p>Predicting the outcome of geopolitical events is of huge importance to many organizations, as these forecasts provide actionable intelligence that may be used to make consequential decisions. Prediction polling is a common method used in crowdsourcing platforms for geopolitical forecasting, where a group of non-expert participants are asked to predict the outcome of a geopolitical event and the collected responses are aggregated to generate a forecast. It has been demonstrated that forecasts by such a crowd can be more accurate than the forecasts of experts. However, geopolitical prediction polling is challenging because participants are highly heterogeneous and diverse in terms of their skills and background knowledge and human resources are often limited. As a result, it is crucial to refer each question to the subset of participants that possess suitable skills to answer it, such that individual efforts are not wasted. In this paper, we propose an algorithm based on multitask learning to learn the skills of participants of a forecasting platform by using their performance history. The learned model then can be used to recommend suitable questions to forecasters. Our experimental results demonstrate that the prediction accuracy can be increased based on the proposed algorithm as opposed to when questions have been randomly assigned.</p>
    <p><strong>Categories:</strong> Crowdsourcing, Geopolitical Forecasting, Multitask Learning, Recommendation Systems, Crowd Intelligence, Decision-Making, User Modeling, Algorithm Evaluation, Machine Learning Algorithms, Prediction Accuracy (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/364/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Understanding Latent Factors Using a GWAP (2018)</h3>
    <p><strong>Authors:</strong> Jürgen Ziegler, Johannes Kunkel, Benedikt Loepp</p>
    <p>Recommender systems relying on latent factor models often appear as black boxes to their users. Semantic descriptions for the factors might help to mitigate this problem. Achieving this automatically is, however, a non-straightforward task due to the models’ statistical nature. We present an output-agreement game that represents factors by means of sample items and motivates players to create such descriptions. A user study shows that the collected output actually reflects real-world characteristics of the factors.</p>
    <p><strong>Categories:</strong> Algorithm Family (Latent Factor Models), User Study, Games with a Purpose (GWAP), Real-World Applications, Transparency/Interpretability in Recommendations, Explainability, Human-Readable Descriptions, Recommendation Systems, Social Aspects, Crowdsourcing, Methodology (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/420/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Crowd-Based Personalized Natural Language Explanations for Recommendations (2016)</h3>
    <p><strong>Authors:</strong> F. Maxwell Harper, Shuo Chang, Loren Gilbert Terveen</p>
    <p>Explanations are important for users to make decisions on whether to take recommendations. However, algorithm generated explanations can be overly simplistic and unconvincing. We believe that humans can overcome these limitations. Inspired by how people explain word-of-mouth recommendations, we designed a process, combining crowdsourcing and computation, that generates personalized natural language explanations. We modeled key topical aspects of movies, asked crowdworkers to write explanations based on quotes from online movie reviews, and personalized the explanations presented to users based on their rating history. We evaluated the explanations by surveying 220 MovieLens users, finding that compared to personalized tag-based explanations, natural language explanations: 1) contain a more appropriate amount of information, 2) earn more trust from users, and 3) make users more satisfied. This paper contributes to the research literature by describing a scalable process for generating high quality and personalized natural language explanations, improving on state-of-the-art content-based explanations, and showing the feasibility and advantages of approaches that combine human wisdom with algorithmic processes.</p>
    <p><strong>Categories:</strong> Personalized Recommendations, Natural Language Explanations, Crowdsourcing, Trust in Recommendations, Human-Computer Collaboration, Content-Based Recommendations, Movie Recommendations, User Satisfaction, Scalability, Recommendation Explanations, Crowdworker Contributions (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/169/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Item Familiarity Effects in User-Centric Evaluations of Recommender Systems (2015)</h3>
    <p><strong>Authors:</strong> Michael Jugovac, Dietmar Jannach, Lukas Lerche</p>
    <p>Laboratory studies are a common way of comparing recommendation approaches with respect to different quality dimensions that might be relevant for real users. One typical experimental setup is to first present the participants with recommendation lists that were created with different algorithms and then ask the participants to assess these recommendations individually or to compare two item lists. The cognitive effort required by the participants for the evaluation of item recommendations in such settings depends on whether or not they already know the (features of the) recommended items. Furthermore, lists containing popular and broadly known items are correspondingly easier to evaluate. In this paper we report the results of a user study in which participants recruited on a crowdsourcing platform assessed system-provided recommendations in a between-subjects experimental design. The results surprisingly showed that . An analysis revealed a measurable correlation between item familiarity and user acceptance. Overall, the observations indicate that item familiarity can be a potential confounding factor in such studies and should be considered in experimental designs.</p>
    <p><strong>Categories:</strong> Recommender Systems, User-Centric Evaluation, Evaluation Methodology, Crowdsourcing, Human Factors in Recommendation, Item Familiarity, Experimental Design, User Studies, Behavioral Analysis, Confounding Factors (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/151/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>The Recommendation Game: Using a Game-with-a-Purpose to Generate Recommendation Data (2015)</h3>
    <p><strong>Authors:</strong> Sam Banks, Rachael Rafter, Barry Smyth</p>
    <p>This paper describes a casual Facebook game to capture recommendation data as a side-effect of gameplay. We show how this data can be used to make successful recommendations as part of a live-user trial.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Social Media, Crowdsourcing, Game Design, Game-with-a-Purpose, Real-World Applications, A/B Test, Implicit Feedback, Evaluation of Recommendations, Beyond Accuracy, Data Collection Methods (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/139/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>