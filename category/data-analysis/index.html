<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">Data Analysis</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/bias-mitigation/">Bias Mitigation</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/real-world-applications/">Real-world Applications</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/offline-evaluation/">Offline Evaluation</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Uncovering User Interest from Biased and Noised Watch Time in Video Recommendation (2023)</h3>
    <p><strong>Authors:</strong> Jun Xu, Guohao Cai, Zhenhua Dong, Haiyuan Zhao, Lei Zhang, Ji-Rong Wen</p>
    <p>In micro-video recommendation scenarios, watch time is commonly adopted as an indicator of users’ interest. However, watch time is not only determined by the matching of users’ interests but is affected by other factors. These factors mainly lie in two folds: on the one hand, users tend to spend more time on those charming videos with the growth of the duration (i.e., video length), named as duration bias; on the other hand, it costs people a period of time to judge whether they like the video, named as noisy watching. Consequently, the existence of duration bias and noisy watching make watch time an inadequate label for training a reliable recommendation model. Moreover, current methods focus only on the duration bias and ignore the duration noise, so they do not really uncover the user interest from watch time. In this study, we first analyze the generation mechanism of users’ watch time in a unified causal viewpoint. Unlike current methods, which only notice the duration bias in watch time, we considered the watch time as a mixture of the user’s actual interest, the duration biased watch time, and the noisy watch time. To mitigate both the duration bias and noisy watching, we propose Debiased and Denoised watch time Correction (D$^2$Co), which can be divided into two steps: First, we employ a duration-wise Gaussian Mixture Model plus frequency-weighted moving average for estimating the bias and noise terms; Then we utilize a sensitivity-controlled correction function to separate the user interest from the watch time, which is robust to the estimation error of bias and noise terms. The experiments on two public video recommendation datasets indicate the effectiveness of the proposed method.</p>
    <p><strong>Categories:</strong> Video Recommendations, Duration Bias, Noisy Watching, Algorithm Development, Data Cleaning/Preprocessing, User Interest Modeling, Training Challenges, Data Analysis, Signal Processing, Recommendation Accuracy (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/886/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Estimating Long-term Effects from Experimental Data (2022)</h3>
    <p><strong>Authors:</strong> Lihong Li, Ziyang Tang, Stephanie Zhang, Yiheng Duan, Steven Zhu</p>
    <p>A/B testing is a powerful tool for a company to make informed decisions about their services and products. A limitation of A/B tests is that they do not easily extend to measure post-experiment (long-term) differences. In this talk, we study a different approach inspired by recent advances in off-policy evaluation in reinforcement learning (RL). The basic RL approach assumes customer behavior follows a stationary Markovian process, and estimates the average engagement metric when the process reaches the steady state. However, in realistic scenarios, the stationary assumption is often violated due to weekly variations and seasonality effects. To tackle this challenge, we propose a variation by relaxing the stationary assumption. We empirically tested both stationary and nonstationary approaches in a synthetic dataset and an online store dataset.</p>
    <p><strong>Categories:</strong> A/B Testing, Long-term Effects, Reinforcement Learning, Off-Policy Evaluation, Stationary Assumption, Nonstationary Assumption, Empirical Testing, Experimental Design, Real-world Applications, Data Analysis, Evaluation Beyond Accuracy (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/829/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Offline Evaluation Standards for Recommender Systems (2021)</h3>
    <p><strong>Authors:</strong> Fernando Mourão</p>
    <p>Offline evaluation has nowadays become a major step in developing Recommendation Systems in both academia and industry [4, 5]. While academia anchors on offline evaluation due to the lack of proper environments for conducting online tests with real users, the industry uses offline evaluation to filter the most promising solutions for further online testing, aiming at reducing costs and potential damage to customers. Despite the blunt advances observed on this topic recently, consolidating a reliable, replicable, flexible and efficient offline evaluation process capable of satisfactorily predicting online test results remains an open challenge [2]. The community still lacks an integrated and updated view on this topic, useful for practitioners to inspect and refine their current offline evaluation stack.<br>The main Recommendation Systems venues have plenty of studies with relevant findings, presenting new challenges, pitfalls and divergent guidelines for better offline evaluation procedures [3, 5]. However, inspecting all those studies and keeping an updated perspective about where they agree is impractical, especially for the industry, given the need for fast iterations and deliveries. Thus, it is not rare to observe professionals struggle to obtain solid answers to practical and high-impact questions, such as: What are the main existing pitfalls we should be aware of when setting up an offline evaluation in a given domain? What is the desired evaluation framework for a given recommendation task? How reliable is a given offline evaluation stack, and how far is it from an ideal setting?<br>In this work, we bring an updated snapshot of offline evaluation standards for Recommendation Systems. For this, we reviewed dozens of studies published in the main Recommendation Systems venues in the last five years, dealing with recurring questions related to offline evaluation design and compiling the main findings in the literature. Then, we contrasted this curated body of knowledge against practical issues we face internally at SEEK, aiming to identify the most valuable guidelines. As a result of this process, we propose an integrated evaluation framework for offline stacks, a reliability score to monitor signs of progress on our stack over time, and a list of best practices to bear in mind when starting a new evaluation. Hence, we have organised the work into three parts:<br>Part I - Integrated Evaluation Framework. We present an offline evaluation framework that compiles the primary directives, pitfalls, and knowledge raised in the last five years by representative studies in the Recommendation Systems literature. This framework aims to compile the main steps, flaws and decisions to be aware of when designing offline tests. Also, it aims to present the leading solutions suggested in the literature for each known issue. The proposed framework can be seen as an extension of Cañamares’ work [1], in which we expand the factors, steps and decisions related to the design of offline experiments for recommenders. Figure 1 depicts the main steps of the framework along with some of the main pitfalls recurrently related to each step. It is noteworthy that this framework should not be deemed as a rigid and thorough set of steps and rules that all professionals must consider in every scenario. It is rather an organized collection of concerns raised in different situations, in which the strength and potential impact of each of them should be carefully inspected through the lens of each evaluation scenario.<br>Part II - Reliability Score. We also propose a Reliability Score to quantify how close a given offline evaluation setting is from the idealised framework instantiated to a given domain and task. This score is derived from a question-driven process that estimates the current state, effort, and impact that each known issue has for each team or company. These questions represent a non-closed set of concerns related to distinct steps of the evaluation process that should be addressed by a reliable evaluation framework. The final score ranges from 0 to 1 and the higher its value, the more reliable a given offline evaluation setting is, considering the specific needs and perspectives of a team or company. Further, this score allows teams of professionals to monitor progress in their offline evaluation settings over time. The proposed score empowers companies to compare the maturity of different teams w.r.t. offline assessments using a unified view. In order to illustrate the practical utility of the Reliability Score, we also present a few internal use cases that demonstrate how the proposed score helped us at SEEK to identify the main flaws in our offline settings and outline strategies for refining our current evaluation stack.<br>Part III - Best Practices & Limitations. Finally, we compiled a list of best practices derived from academic works, experience reports from other companies, and our own experience at SEEK. We expect the proposed list to serve as a starting point for practitioners to qualitatively review their decisions when designing offline assessments, as well as that these professionals would contribute to refining and growing it over time.</p>
    <p><strong>Categories:</strong> Evaluation Methods, Recommender Systems, Offline Evaluation, Best Practices, Research Methodology, Industry Applications, Algorithm Design, Data Analysis, Practical Guidelines, Reproducibility (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/729/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Baby Shark to Barracuda: Analyzing Children’s Music Listening Behavior (2021)</h3>
    <p><strong>Authors:</strong> Amifa Raj, Lawrence Spear, Michael D Ekstrand, Michael Green, Maria Soledad Pera, Garrett Allen, Ashlee Milton</p>
    <p>Music is an important part of childhood development, with online music listening platforms being a significant channel by which children consume music. Children’s offline music listening behavior has been heavily researched, yet relatively few studies explore how their behavior manifests online. In this paper, we use data from LastFM 1 Billion and the Spotify API to explore online music listening behavior of children, ages 6–17, using education levels as lenses for our analysis. Understanding the music listening behavior of children can be used to inform the future design of recommender systems.</p>
    <p><strong>Categories:</strong> Music, Children, Real-World Applications, Data Analysis, User Behavior, Recommender Systems, Child Development, Music Platforms, Music Genres, Insights (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/683/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Learning a voice-based conversational recommender using offline policy optimization (2021)</h3>
    <p><strong>Authors:</strong> Francois Mairesse</p>
    <p>Voice-based conversational recommenders offer a natural way to improve recommendation quality by asking the user for missing information. This talk details how we use offline policy optimization to learn a dialog manager that determines what items to present and what clarifying questions to ask, in order to maximize the success of the conversation. Counter-factual learning allows us to compare various modeling techniques using only logged conversational data. Our approach is applied to Amazon Music’s first voice browsing experience (Alexa, help me find music), which interleaves disambiguation questions and music sample suggestions. Offline policy evaluation results show that an XGBoost reward regressor outperforms linear and neural policies on held out data. A first user-facing A/B test confirms our offline results, by increasing our task completion rate by 8% relative compared to our production rule-based conversational recommender, while reducing the number of turns to complete the task by 20%. A second A/B test shows that extending the set of candidate items to present and adding an embedding-based user-item affinity action feature improves task success rate further by 4% relative, while reducing the number of turns further by 13%. These results suggest that offline policy optimization from conversation logs is a viable way to foster conversational recommender research, while minimizing the number of user-facing experiments needed to determine the optimal dialog policy.</p>
    <p><strong>Categories:</strong> Conversational Recommender, Offline Policy Optimization, Counter-factual Learning, Music, Voice Interfaces, Voice Assistants, User Interaction, A/B Testing, Data Analysis, Evaluation Methods, Machine Learning Algorithms, User-Item Affinity, Task Success Rate, Beyond Accuracy, Diversity of Recommendations, Real-World Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/726/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Offline Contextual Multi-armed Bandits for Mobile Health Interventions: A Case Study on Emotion Regulation (2020)</h3>
    <p><strong>Authors:</strong> Bethany Teachman, Laura E Barnes, Miranda L. Beltzer, Lihua Cai, Mehdi Boukhechba, Mawulolo Koku Ameko</p>
    <p>Delivering treatment recommendations via pervasive electronic devices such as mobile phones has the potential to be a viable and scalable treatment medium for long-term health behavior management. But active experimentation of treatment options can be time-consuming, expensive and altogether unethical in some cases. There is a growing interest in methodological approaches that allow an experimenter to learn and evaluate the usefulness of a new treatment strategy before deployment. We present the first development of a treatment recommender system for emotion regulation using real-world historical mobile digital data from n = 114 high socially anxious participants to test the usefulness of new emotion regulation strategies. We explore a number of offline contextual bandits estimators for learning and propose a general framework for learning algorithms. Our experimentation shows that the proposed doubly robust offline learning algorithms performed significantly better than baseline approaches, suggesting that this type of recommender algorithm could improve emotion regulation. Given that emotion regulation is impaired across many mental illnesses and such a recommender algorithm could be scaled up easily, this approach holds potential to increase access to treatment for many people. We also share some insights that allow us to translate contextual bandit models to this complex real-world data, including which contextual features appear to be most important for predicting emotion regulation strategy effectiveness.</p>
    <p><strong>Categories:</strong> Multi-Armed Bandits, Healthcare, Mobile Systems, Real World Applications, Mental Health, Treatment Recommendation, Offline Contextual Bandits, Mental Illness, Evaluation of Recommendations, Recommender Systems, Data Analysis, Scalability (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/544/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>A College Major Recommendation System (2020)</h3>
    <p><strong>Authors:</strong> Gary Weiss, Yiwen Chen, Daniel Leeds, Samuel Stein</p>
    <p>College students are required to select a major but are often provided with only a modest amount of support in making this important decision. A poor decision is detrimental to the student, since it may result in the student later switching to a different major with a delay in graduation—or even result in the student leaving the university. This also impacts the university since time to graduation and retention rate are used to evaluate the quality of a university. There is a general lack of research on recommender systems for college majors, with the most relevant systems focusing on course-level recommendations. This study describes and evaluates a recommender system for selecting an undergraduate major, utilizing nine years of historical student data from a large university. The system bases its recommendations on the courses that the student takes in the first few years of college, and how well they performed in these courses. The system is designed to recommend majors that the student is likely to be interested in and will perform well in. Recommendations are evaluated based on the likelihood that the student's actual major was in the top five recommended majors, and whether the student performed above average in that major. The recommendation system dramatically outperforms the baseline strategy of randomly selecting a major, and when the recommendation is followed the student is 12% more likely to perform above average in the major.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Education, Educational Technology, Data Analysis, Academic Records, Student Performance, Major Recommendation, Higher Education, University Impact, Graduation Rate, Retention, Student Success (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/589/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Pace My Race: Recommendations for Marathon Running (2019)</h3>
    <p><strong>Authors:</strong> Barry Smyth, Aonghus Lawlor, Jakim Berndsen</p>
    <p>We propose marathon running as a novel domain for recommender systems and machine learning. Using high-resolution marathon performance data from multiple marathon races (n=7931), we build in-race recommendations for runners. We show that we can outperform the existing techniques which are currently employed for in-race finish-time prediction, and we demonstrate how such predictions may be used to make real time recommendations to runners. The recommendations are made at critical points in the race to provide personalised guidance so the runner can adjust their race strategy. Through the association of model features and the expert domain knowledge of marathon runners we generate explainable, adaptable pacing recommendations which can guide runners to their best possible finish time and help them avoid the potentially catastrophic effects of hitting the wall.</p>
    <p><strong>Categories:</strong> Recommender Systems, Marathon Running, Sports and Athletics, Personalization, Machine Learning, Real-time Recommendations, Data Analysis, Technique Comparison, Health and Wellness, Real-World Applications, Predictive Analytics, Adaptability (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/481/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Predicting Musical Sophistication from Music Listening Behaviors: A Preliminary Study (2018)</h3>
    <p><strong>Authors:</strong> Mark Graus, Bruce Ferwerda</p>
    <p>Psychological models are increasingly being used to explain online behavioral traces. Aside from the commonly used personality traits as a general user model, more domain dependent models are gaining attention. The use of domain dependent psychological models allows for more fine-grained identification of behaviors and provide a deeper understanding behind the occurrence of those behaviors. Understanding behaviors based on psychological models can provide an advantage over data-driven approaches. For example, relying on psychological models allow for ways to personalize when data is scarce. In this preliminary work we look at the relation between users’ musical sophistication and their online music listening behaviors and to what extent we can successfully predict musical sophistication. An analysis of data from a study with 61 participants shows that listening behaviors can successfully be used to infer users’ musical sophistication.</p>
    <p><strong>Categories:</strong> Psychological Modeling, User Behavior Analysis, Music Domain, Prediction Methods, Domain-Specific Models, Online Behavior, Personalization, Model Evaluation, Participant Behavior, Data Analysis (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/419/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Towards a Recommender System for Undergraduate Research (2017)</h3>
    <p><strong>Authors:</strong> Felipe Del Rio, Denis Parra, Erick Svec, Jovan Kuzmicic</p>
    <p>Several studies indicate that attracting students to research careers requires to engage them from early undergraduate years. Following this, the Engineering School at PUC Chile has developed an undergraduate research program that allows students to enroll in research in exchange for course credits. Moreover, we developed a web portal to inform students about the program, but participation remains lower than expected. In order to promote student engagement, we attempt to build a personalized recommender system of research opportunities to undergraduates. With this goal in mind we investigate two tasks. First, identifying students that are more willing to participate on this kind of program. A second task is generating a personalized list of recommendations of research opportunities for each student. To evaluate our approach, we perform a simulated prediction experiment with data from our school, which has more than 4,000 active undergraduate students nowadays. Results indicate that there is a big potential to create a personalized recommender system for this purpose. Our research can be used as a baseline for colleges seeking strategies to encourage research activities within undergraduate students.</p>
    <p><strong>Categories:</strong> Recommendation System, Education, Higher Education, Student Engagement, Research Opportunities, Personalized Recommendations, User Personalization, Evaluation Methods, Undergraduate Students, Engineering Education, User Engagement, Student Interests and Preferences, Recommendation Algorithms, Data Analysis (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/319/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Visual Analysis of Recommendation Performance (2017)</h3>
    <p><strong>Authors:</strong> Markus Zanker, Ludovik Çoba, Panagiotis Symeonidis</p>
    <p>rrecsys is a novel library in R for developing and assessing recommendation algorithms. In this demo, we extend rrecsys with functions for visual analytics of recommendation performance, that is one of the strong capabilities of the R environment. In particular, we show how the library can be used to depict dataset characteristics, train and test recommendation algorithms and to visually assess, for instance, their capability to exploit long-tail items for making correct predictions.</p>
    <p><strong>Categories:</strong> Visualization, Recommendation Systems, Algorithm Evaluation, R (programming language), Tools/Frameworks, Statistical Computing, Data Analysis, Machine Learning, Performance Analysis (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/304/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>How to Survive Dynamic Pricing Competition in E-commerce (2016)</h3>
    <p><strong>Authors:</strong> Martin Boissier, Andre Schober, Rainer Schlosser, Matthias Uflacker</p>
    <p>Pricing on e-commerce platforms is highly challenging. Sellers typically i) rival against dozens of competitors, ii) decide on prices for thousands of products, and iii) face steadily changing market situations. With respect to pricing, the challenge is to circumvent the curse of dimensionality to dynamically price products for a given market situation in a timely manner. In this project, we create a stochastic pricing model by analyzing recorded market data. This pricing model can be applied ad-hoc in less than a millisecond per item, allowing us to react immediately to new market situations. Our pricing approach is currently being applied in practice by a large German book seller on Amazon and outperforms the previous rule-based strategy by over 20% with respect to cash-in per book.</p>
    <p><strong>Categories:</strong> Dynamic Pricing, E-commerce, Stochastic Models, Market Analysis, Competition Strategy, Real-World Application, Performance Evaluation, Data Analysis, A/B Test, Metrics Evaluation (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/229/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Exploring the Value of Personality in Predicting Rating Behaviors: A Study of Category Preferences on MovieLens (2016)</h3>
    <p><strong>Authors:</strong> Joseph A. Konstan, Raghav Pavan Karumur, Tien T. Nguyen</p>
    <p>Prior work relevant to incorporating personality into recommender systems falls into two categories: social science studies and algorithmic ones. Social science studies of preference have found only small relationships between personality and category preferences, whereas, algorithmic approaches found a little improvement when incorporating personality into recommendations. As a result, despite good reasons to believe personality assessments should be useful in recommenders, we are left with no substantial demonstrated impact. In this work, we start with user data from a live recommender system, but study category-by-category variations in preference (both rating levels and distribution) across different personality types. By doing this, we hope to isolate specific areas where personality is most likely to provide value in recommender systems, while also modeling an analytic process that can be used in other domains. After controlling for the family-wise error rate, we find that High Agreeableness users rate at least 0.5 stars higher on a 5-star scale compared to low Agreeableness users. We also find differences in consumption in four different personality types between people who manifested high and low levels of that personality.</p>
    <p><strong>Categories:</strong> Personality, Rating Behavior, Recommender Systems, MovieLens, Social Science, Algorithmic Approaches, Category Preferences, User Behavior Analysis, Movies, Empirical Study, Cross-Domain Analysis, Data Analysis (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/208/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Adaptation and Evaluation of Recommendations for Short-term Shopping Goals (2015)</h3>
    <p><strong>Authors:</strong> Michael Jugovac, Dietmar Jannach, Lukas Lerche</p>
    <p>An essential characteristic in many of e-commerce settings is that website visitors can have very specific short-term shopping goals when they browse the site. Relying solely on long-term user models that are pre-trained on historical data can therefore be insufficient for recommendation. Simple “real-time” recommendation approaches based, e.g., on unpersonalized co-occurrence patterns, on the other hand do not fully exploit the available information about the user’s long-term preference profile. In this work, we aim to explore and quantify the effectiveness of using and combining long-term models and short-term adaptation strategies. We conducted an empirical evaluation based on a novel evaluation design and two real-world datasets. The results indicate that maintaining short-term and profiles of the visitors can lead to significant accuracy increases. At the same time, the experiments show that the choice of the algorithm for learning the long term preferences is particularly important at the beginning of new shopping sessions.</p>
    <p><strong>Categories:</strong> Recommendation Systems, E-Commerce, Short-Term Adaptation, Long-Term Models, Empirical Evaluation, Data Analysis, Accuracy Improvement, Hybrid Approaches, User Behavior, Applied Research, Algorithm Selection (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/85/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>