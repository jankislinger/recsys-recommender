<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">User-Centric Evaluation</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/scalability/">Scalability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/real-world-applications/">Real-world Applications</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Putting Popularity Bias Mitigation to the Test: A User-Centric Evaluation in Music Recommenders (2024)</h3>
    <p><strong>Authors:</strong> Robin Ungruh, Karlijn Dinnissen, Maria Soledad Pera, Hanna Hauptmann, Anja Volk</p>
    <p>Popularity bias is a prominent phenomenon in recommender systems (RS), especially in the music domain. Although popularity bias mitigation techniques are known to enhance the fairness of RS while maintaining their high performance, there is a lack of understanding regarding users’ actual perception of the suggested music. To address this gap, we conducted a user study (n=40) exploring user satisfaction and perception of personalized music recommendations generated by algorithms that explicitly mitigate popularity bias. Specifically, we investigate item-centered and user-centered bias mitigation techniques, aiming to ensure fairness for artists or users, respectively. Results show that neither mitigation technique harms the users’ satisfaction with the recommendation lists despite promoting underrepresented items. However, the item-centered mitigation technique impacts user perception; by promoting less popular items, it reduces users’ familiarity with the items. Lower familiarity evokes discovery—the feeling that the recommendations enrich the user’s taste. We demonstrate that this  can ultimately lead to higher satisfaction, highlighting the potential of less-popular recommendations to improve the user experience.</p>
    <p><strong>Categories:</strong> Recommendation Algorithms, Music Recommendations, Popularity Bias, User-Centric Evaluation, Bias Mitigation, Recommender Systems Evaluation, Personalization, User Perception, Fairness in Recommendations, Discovery (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1056/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>EasyStudy: Framework for Easy Deployment of User Studies on Recommender Systems (2023)</h3>
    <p><strong>Authors:</strong> Ladislav Peska, Patrik Dokoupil</p>
    <p>Improvements in the recommender systems (RS) domain are not possible without a thorough way to evaluate and compare newly proposed approaches. User studies represent a viable alternative to online and offline evaluation schemes, but despite their numerous benefits, they are only rarely used. One of the main reasons behind this fact is that preparing a user study from scratch involves a lot of extra work on top of a simple algorithm proposal.  To simplify this task, we propose \textsc{EasyStudy}, a modular framework built on the credo “<i>Make simple things fast and hard things possible</i>”. It features ready-to-use datasets, preference elicitation methods, incrementally tuned baseline algorithms, study flow plugins, and evaluation metrics. As a result, a simple study comparing several RS can be deployed with just a few clicks, while more complex study designs can still benefit from a range of reusable components, such as preference elicitation. Overall, \textsc{EasyStudy} dramatically decreases the gap between the laboriousness of offline evaluation vs. user studies and, therefore, may contribute towards the more reliable and insightful user-centric evaluation of next-generation RS.</p>
    <p><strong>Categories:</strong> Recommender Systems, User-Centric Evaluation, Evaluation Methods, User Studies, Preference Elicitation, Framework Development, Baseline Algorithms, Beyond Accuracy Evaluation, Scalability, Modular Frameworks (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/962/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Generation-based vs. Retrieval-based Conversational Recommendation: A User-Centric Comparison (2021)</h3>
    <p><strong>Authors:</strong> Dietmar Jannach, Ahtsham Manzoor</p>
    <p>In the past few years we observed a renewed interest in conversational recommender systems (CRS) that interact with users in natural language. Most recent research efforts use neural models trained on recorded recommendation dialogs between humans, supporting an end-to-end learning process. Given the user’s utterances in a dialog, these systems aim to generate appropriate responses in natural language based on the learned models. An alternative to such language generation approaches is to retrieve and possibly adapt suitable sentences from the recorded dialogs. Approaches of this latter type are explored only to a lesser extent in the current literature.<br>In this work, we revisit the potential value of retrieval-based approaches to conversational recommendation. To that purpose, we compare two recent deep learning models for response generation with a retrieval-based method that determines a set of response candidates using a nearest-neighbor technique and heuristically reranks them. We adopt a user-centric evaluation approach, where study participants (N=60) rated the responses of the three compared systems. We could reproduce the claimed improvement of one of the deep learning methods over the other. However, the retrieval-based system outperformed both language generation based approaches in terms of the perceived quality of the system responses. Overall, our study suggests that retrieval-based approaches should be considered as an alternative or complement to modern language generation-based approaches.</p>
    <p><strong>Categories:</strong> Conversational Recommender Systems (CRS), Generation-based Approaches, Retrieval-based Approaches, Deep Learning Models, Neural Models, User-Centric Evaluation, Response Generation, End-to-End Learning, Nearest-Neighbor Technique, Perceived Quality of Responses (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/676/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>User-Centric Evaluation of Session-Based Recommendations for an Automated Radio Station (2019)</h3>
    <p><strong>Authors:</strong> Dietmar Jannach, Malte Ludewig</p>
    <p>The creation of an automated and virtually endless playlist given a start item is a common feature of modern media streaming services. When no past information about the user’s preferences is available, the creation of such playlists can be done using session-based recommendation techniques. In this case, the recommendations only depend on the start item and the user’s interactions in the current listening session, such as ‘liking’ or skipping an item. In recent years, various novel session-based techniques were proposed, often based on deep learning. The evaluation of such approaches is in most cases solely based on offline experimentation and abstract accuracy measures. However, such evaluations cannot inform us about the quality as perceived by users. To close this research gap, we have conducted a user study (N=250), where the participants interacted with an automated online radio station. Each treatment group received recommendations that were generated by one of five different algorithms. Our results show that comparably simple techniques led to quality perceptions that are similar or even better than when a complex deep learning mechanism or Spotify’s recommendations are used. The simple mechanisms, however, often tend to recommend comparably popular tracks, which can lead to lower discovery effects. i>Presentation: Wednesday Poster Lunch</i</p>
    <p><strong>Categories:</strong> Session-Based Recommendations, User-Centric Evaluation, Recommendation Systems, Media Streaming, Automated Radio, Deep Learning, Algorithm Comparison, A/B Testing, User Study, Real-World Applications, Perceived Quality, Discovery Effects, Beyond Accuracy (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/499/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Judging Similarity: A User-Centric Study of Related Item Recommendations (2018)</h3>
    <p><strong>Authors:</strong> Yuan Yao, F. Maxwell Harper</p>
    <p>Related item recommenders operate in the context of a particular item. For instance, a music system’s page about the artist Radiohead might recommend other similar artists such as The Flaming Lips. Often central to these recommendations is the computation of similarity between pairs of items. Prior work has explored many algorithms and features that allow for the computation of similarity scores, but little work has evaluated these approaches from a user-centric perspective. In this work, we build and evaluate six similarity scoring algorithms that span a range of activity- and content-based approaches. We evaluate the performance of these algorithms using both offline metrics and a new set of more than 22,000 user-contributed evaluations. We integrate these results with a survey of more than 700 participants concerning their expectations about item similarity and related item recommendations. We find that content-based algorithms outperform ratings- and clickstream-based algorithms in terms of how well they match user expectations for similarity and recommendation quality. Our results yield a number of implications to guide the construction of related item recommendation algorithms.</p>
    <p><strong>Categories:</strong> Content-Based Filtering, Activity-Based Approaches, Similarity Computation, Related Item Recommendations, User-Centric Evaluation, Offline Metrics, Real-World Application (User Survey), Large-Scale Study, Recommendation Algorithms, User Expectations, Algorithm Design (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/347/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Item Familiarity Effects in User-Centric Evaluations of Recommender Systems (2015)</h3>
    <p><strong>Authors:</strong> Michael Jugovac, Dietmar Jannach, Lukas Lerche</p>
    <p>Laboratory studies are a common way of comparing recommendation approaches with respect to different quality dimensions that might be relevant for real users. One typical experimental setup is to first present the participants with recommendation lists that were created with different algorithms and then ask the participants to assess these recommendations individually or to compare two item lists. The cognitive effort required by the participants for the evaluation of item recommendations in such settings depends on whether or not they already know the (features of the) recommended items. Furthermore, lists containing popular and broadly known items are correspondingly easier to evaluate. In this paper we report the results of a user study in which participants recruited on a crowdsourcing platform assessed system-provided recommendations in a between-subjects experimental design. The results surprisingly showed that . An analysis revealed a measurable correlation between item familiarity and user acceptance. Overall, the observations indicate that item familiarity can be a potential confounding factor in such studies and should be considered in experimental designs.</p>
    <p><strong>Categories:</strong> Recommender Systems, User-Centric Evaluation, Evaluation Methodology, Crowdsourcing, Human Factors in Recommendation, Item Familiarity, Experimental Design, User Studies, Behavioral Analysis, Confounding Factors (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/151/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>