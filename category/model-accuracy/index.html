<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">Model Accuracy</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/bias-mitigation/">Bias Mitigation</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/real-world-applications/">Real-world Applications</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reinforcement-learning/">Reinforcement Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Countering Popularity Bias by Regularizing Score Differences (2022)</h3>
    <p><strong>Authors:</strong> Sung Min Cho, Wondo Rhee, Bongwon Suh</p>
    <p>Recommendation system often suffers from popularity bias. Often the training data inherently exhibits long-tail distribution in item popularity (data bias). Moreover, the recommendation systems could give unfairly higher recommendation scores to popular items even among items a user equally liked, resulting in over-recommendation of popular items (model bias). In this study we propose a novel method to reduce the model bias while maintaining accuracy by directly regularizing the recommendation scores to be equal across items a user preferred. Akin to contrastive learning, we extend the widely used pairwise loss (BPR loss) which maximizes the score differences between preferred and unpreferred items, with a regularization term that minimizes the score differences within preferred and unpreferred items, respectively, thereby achieving both high debias and high accuracy performance with no additional training. To test the effectiveness of the proposed method, we design an experiment using a synthetic dataset which induces model bias with baseline training; we showed applying the proposed method resulted in drastic reduction of model bias while maintaining accuracy. Comprehensive comparison with earlier debias methods showed the proposed method had advantages in terms of computational validity and efficiency. Further empirical experiments utilizing four benchmark datasets and four recommendation models indicated the proposed method showed general improvements over performances of earlier debias methods. We hope that our method could help users enjoy diverse recommendations promoting serendipitous findings. Code available at https://github.com/stillpsy/popbias.</p>
    <p><strong>Categories:</strong> Popularity Bias, Recommendation Systems, Regularization, Pairwise Loss, Loss Functions, Evaluation Metrics, Fairness, Diversity of Recommendations, Model Accuracy, Bias Mitigation, Benchmark Datasets (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/753/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>FR-FMSS: Federated Recommendation via Fake Marks and Secret Sharing (2021)</h3>
    <p><strong>Authors:</strong> Zhaohao Lin, Weike Pan, Zhong Ming</p>
    <p>With the implementation of privacy protection laws such as GDPR, it is increasingly difficult for organizations to legally collect user data. However, a typical recommendation algorithm based on machine learning requires user data to learn user preferences. In order to protect user privacy, a lot of recent works turn to develop federated learning-based recommendation algorithms. However, some of these works can only protect the users’ rating values, some can only protect the users’ rating behavior (i.e., the engaged items), and only a few works can protect the both types of privacy at the same time. Moreover, most of them can only be applied to a specific algorithm or a class of similar algorithms. In this paper, we propose a generic cross-user federated recommendation framework called FR-FMSS. Our FR-FMSS can not only protect the two types of user privacy, but can also be applied to most recommendation algorithms for rating prediction, item ranking, and sequential recommendation. Specifically, we use fake marks and secret sharing to modify the data uploaded by the clients to the server, which protects user privacy without loss of model accuracy. We take three representative recommendation algorithms, i.e., MF-MPC, eALS, and Fossil, as examples to show how to apply our FR-FMSS to a specific algorithm.</p>
    <p><strong>Categories:</strong> Federated Learning, Matrix Factorization, Recommendation Systems, Privacy Preservation, Cross-User Federated Recommendations, Fake Marks, Secret Sharing, User Privacy Protection, Model Accuracy, A/B Testing, Data Privacy, Cold Start, User-Centric (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/688/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>