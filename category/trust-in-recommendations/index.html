<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">Trust in Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/bias-mitigation/">Bias Mitigation</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/benchmarking/">Benchmarking</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reinforcement-learning/">Reinforcement Learning</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Developing a Human-Centered Framework for Transparency in Fairness-Aware Recommender Systems (2022)</h3>
    <p><strong>Authors:</strong> Jessie J. Smith</p>
    <p>Though recommender systems fundamentally rely on human input and feedback, human-centered research in the RecSys discipline is lacking. When recommender systems aim to treat users more fairly, misinterpreting user objectives could lead to unintentional harm, whether or not fairness is part of the aim. When users seek to understand recommender systems better, a lack of transparency could act as an obstacle for their trust and adoption of the platform. Human-centered machine learning seeks to design systems that understand their users, while simultaneously designing systems that the users can understand. In this work, I propose to explore the intersection of transparency and user-system understanding through three phases of research that will result in a Human-Centered Framework for Transparency in Fairness-Aware Recommender Systems.</p>
    <p><strong>Categories:</strong> Recommender Systems, Fairness-Aware Recommendation, Transparency in Recommendations, Human-Centered Design, User-System Interaction, Trust in Recommendations, Ethical AI, Explainable AI (XAI), User Trust (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/815/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>ExpLOD: A Framework for Explaining Recommendations based on the Linked Open Data Cloud (2016)</h3>
    <p><strong>Authors:</strong> Fedelucio Narducci, Pasquale Lops, Marco De Gemmis, Giovanni Semeraro, Cataldo Musto</p>
    <p>In this paper we present ExpLOD, a framework which exploits the information available in the Linked Open Data (LOD) cloud to generate a natural language explanation of the suggestions produced by a recommendation algorithm. The methodology is based on building a graph in which the items liked by a user are connected to the items recommended through the properties available in the LOD cloud. Next, given this graph, we implemented some techniques to rank those properties and we used the most relevant ones to feed a module for generating explanations in natural language. In the experimental evaluation we performed a user study with 308 subjects aiming to investigate to what extent our explanation framework can lead to more transparent, trustful and engaging recommendations. The preliminary results provided us with encouraging findings, since our algorithm performed better than both a non-personalized explanation baseline and a popularity-based one.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Explanation Generation, Linked Open Data (LOD), Natural Language Processing (NLP), User Study, Trust in Recommendations, Explainable AI (XAI), Transparency in Recommendations, Graph-Based Methods, Evaluation Techniques, Framework Design (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/204/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Crowd-Based Personalized Natural Language Explanations for Recommendations (2016)</h3>
    <p><strong>Authors:</strong> F. Maxwell Harper, Shuo Chang, Loren Gilbert Terveen</p>
    <p>Explanations are important for users to make decisions on whether to take recommendations. However, algorithm generated explanations can be overly simplistic and unconvincing. We believe that humans can overcome these limitations. Inspired by how people explain word-of-mouth recommendations, we designed a process, combining crowdsourcing and computation, that generates personalized natural language explanations. We modeled key topical aspects of movies, asked crowdworkers to write explanations based on quotes from online movie reviews, and personalized the explanations presented to users based on their rating history. We evaluated the explanations by surveying 220 MovieLens users, finding that compared to personalized tag-based explanations, natural language explanations: 1) contain a more appropriate amount of information, 2) earn more trust from users, and 3) make users more satisfied. This paper contributes to the research literature by describing a scalable process for generating high quality and personalized natural language explanations, improving on state-of-the-art content-based explanations, and showing the feasibility and advantages of approaches that combine human wisdom with algorithmic processes.</p>
    <p><strong>Categories:</strong> Personalized Recommendations, Natural Language Explanations, Crowdsourcing, Trust in Recommendations, Human-Computer Collaboration, Content-Based Recommendations, Movie Recommendations, User Satisfaction, Scalability, Recommendation Explanations, Crowdworker Contributions (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/169/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>