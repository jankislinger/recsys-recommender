<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">Performance Improvement</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/bias-mitigation/">Bias Mitigation</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reinforcement-learning/">Reinforcement Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/user-centric-design/">User-Centric Design</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/evaluation-methods/">Evaluation Methods</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>A Multimodal Single-Branch Embedding Network for Recommendation in Cold-Start and Missing Modality Scenarios (2024)</h3>
    <p><strong>Authors:</strong> Anna Hausberger, Markus Schedl, Christian Ganhör, Shah Nawaz, Marta Moscati</p>
    <p>Most recommender systems adopt collaborative filtering (CF) and provide recommendations based on past collective interactions. Therefore, the performance of CF algorithms degrades when few or no interactions are available, a scenario referred to as cold-start. To address this issue, previous work relies on models leveraging both collaborative data and side information on the users or items. Similar to multimodal learning, these models aim at combining collaborative and content representations in a shared embedding space. In this work we propose a novel technique for multimodal recommendation, relying on a multimodal Single-Branch embedding network for Recommendation (SiBraR). Leveraging weight-sharing, SiBraR encodes interaction data as well as multimodal side information using the same single-branch embedding network on different modalities. This makes SiBraR effective in scenarios of missing modality, including cold start. Our extensive experiments on large-scale recommendation datasets from three different recommendation domains (music, movie, and e-commerce) and providing multimodal content information (audio, text, image, labels, and interactions) show that SiBraR significantly outperforms CF as well as state-of-the-art content-based RSs in cold-start scenarios, and is competitive in warm scenarios. We show that SiBraR’s recommendations are accurate in missing modality scenarios, and that the model is able to map different modalities to the same region of the shared embedding space, hence reducing the modality gap.</p>
    <p><strong>Categories:</strong> Recommender Systems, Multimodal Learning, Cold Start, Missing Modality, Embedding Networks, Collaborative Filtering, Content-Based Recommendations, Music, Movies, E-commerce, Large-Scale Datasets, Performance Improvement (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1018/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Bridging the Gap: Unpacking the Hidden Challenges in Knowledge Distillation for Online Ranking Systems (2024)</h3>
    <p><strong>Authors:</strong> Shuo Yang, Yang Liu, Nikhil Khani, Li Wei, Pendo Abbo, Aniruddh Nath, Shawn Andrews</p>
    <p>Knowledge Distillation (KD) is a powerful approach for compressing large models into smaller, more efficient models, particularly beneficial for latency-sensitive applications like recommender systems. However, current KD research predominantly focuses on Computer Vision (CV) and NLP tasks, overlooking unique data characteristics and challenges inherent to recommender systems.  This paper addresses these overlooked challenges, specifically: (1) mitigating data distribution shifts between teacher and student models, (2) efficiently identifying optimal teacher configurations within time and budgetary constraints, and (3) enabling computationally efficient and rapid sharing of teacher labels to support multiple students. We present a robust KD system developed and rigorously evaluated on multiple large-scale personalized video recommendation systems within Google. Our live experiment results demonstrate significant improvements in student model performance while ensuring consistent and reliable generation of high-quality teacher labels from continuous data streams.</p>
    <p><strong>Categories:</strong> Knowledge Distillation, Recommender Systems, Online Ranking, Data Distribution Shifts, Teacher-Student Models, Model Compression, Optimization Techniques, Efficient Label Sharing, Multi-Teacher Settings, Video Recommendations, Large-Scale Systems, Performance Improvement, Real-World Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1158/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>CALRec: Contrastive Alignment of Generative LLMs For Sequential Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Mohamed Hammad, Ivan Vulić, Xiang Zhai, Yaoyiran Li, Anna Korhonen, Keyi Yu, Moustafa Alzantot</p>
    <p>Traditional recommender systems such as matrix factorization methods rely on learning a shared dense embedding space to represent both items and user preferences. Sequence models such as RNN, GRUs, and, recently, Transformers have also excelled in the task of sequential recommendation. The sequential recommendation task requires understanding the sequential structure present in users’ historical interactions to predict the next item they may like. Building upon the success of Large Language Models (LLMs) in a variety of tasks, researchers have recently explored using LLMs that are pretrained on giant corpora of text for sequential recommendation. To use LLMs in sequential recommendations, both the history of user interactions and the model’s prediction of the next item are expressed in text form. We propose CALRec, a two-stage LLM finetuning framework that finetunes a pretrained LLM in a two-tower fashion using a mixture of two contrastive losses and a language modeling loss: the LLM is first finetuned on a data mixture from multiple domains followed by another round of target domain finetuning. Our model significantly outperforms many state-of-the-art baselines (+37% in Recall@1 and +24% in NDCG@10) and systematic ablation studies reveal that (i) both stages of finetuning are crucial, and, when combined, we achieve improved performance, and (ii) contrastive alignment is effective among the target domains explored in our experiments.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Sequential Recommendation, Large Language Models (LLMs), Contrastive Learning, Transformers, Fine-tuning, Performance Improvement, Text Representation (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1031/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Knowledge-Enhanced Multi-Behaviour Contrastive Learning for Effective Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Iadh Ounis, Zeyuan Meng, Zixuan Yi</p>
    <p>Real-world recommendation scenarios usually need to handle diverse user-item interaction behaviours, including page views, adding items into carts, and purchasing activities. The interactions that precede the actual target behaviour (e.g. purchasing an item) allow to better capture the user’s preferences from different angles, and are used  as auxiliary information (e.g. page views) to enrich the system’s knowledge about the users’ preferences, thereby helping to enhance recommendation for the target behaviour. Despite efforts in modelling the users’ multi-behaviour interaction information, the existing multi-behaviour recommenders  still face two challenges: (1) Data sparsity across multiple user behaviours is a common issue that limits the recommendation performance, particularly for the target behaviour, which typically exhibits fewer interactions compared to other auxiliary behaviours. (2) Noisy auxiliary interactive behaviour where the information in the auxiliary information  might be non-relevant to recommendation.  In this case, a direct  adoption of  contrastive learning between the target behaviour and the auxiliary behaviours will amplify the noise in the auxiliary behaviours, thereby negatively impacting the real semantics that can be derived from the target behaviour. To address these two challenges, we propose a new model called Knowledge-Enhanced Multi-behaviour Contrastive Learning for Recommendation (KEMCL). In particular, to address the problem of sparse user multi-behaviour interaction information, we leverage a tailored knowledge graph (KG) to enrich the semantic representations of items, and generate supervision signals through self-supervised learning so as to enhance  recommendation. In addition, we develop two contrastive learning (CL) methods, inter CL and intra CL, to alleviate the problem of noisy auxiliary interactions. Extensive experiments on three public recommendation datasets show that our proposed KEMCL model significantly outperforms the existing state-of-the-art (SOTA) methods. In particular, our KEMCL model outperforms the best baseline performance, namely KMCLR,  by 5.42% on the large Tmall dataset.</p>
    <p><strong>Categories:</strong> Recommendation Systems, User Interaction, Auxiliary Information, Data Sparsity, Noisy Data, Contrastive Learning, Knowledge Graphs, Self-Supervised Learning, Real-World Applications, Performance Improvement, E-Commerce (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1095/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Distillation Matters: Empowering Sequential  Recommenders to Match the Performance of Large Language Models (2024)</h3>
    <p><strong>Authors:</strong> Jiawei Chen, Yi Wan, Heng Tang, Bohao Wang, Feng Liu, Pengbo Wang, Jun Wang, Yu Cui</p>
    <p>Owing to their powerful semantic reasoning capabilities, Large Language Models (LLMs) have been effectively utilized as recommenders, achieving impressive performance. However, the high inference latency of LLMs significantly restricts their practical deployment. To address this issue, this work investigates knowledge distillation from cumbersome LLM-based recommendation models to lightweight conventional sequential models. It encounters three challenges: 1) the teacher’s knowledge may not always be reliable; 2) the capacity gap between the teacher and student makes it difficult for the student to assimilate the teacher’s knowledge; 3) divergence in semantic space poses a challenge to distill the knowledge from embeddings. To tackle these challenges, this work proposes a novel distillation strategy, DLLM2Rec, specifically tailored for knowledge distillation from LLM-based recommendation models to conventional sequential models. DLLM2Rec comprises: 1) Importance-aware ranking distillation, which filters reliable and student-friendly knowledge by weighting instances according to teacher confidence and student-teacher consistency; 2)  Collaborative embedding distillation integrates knowledge from teacher embeddings with collaborative signals mined from the data. Extensive experiments demonstrate the effectiveness of the proposed DLLM2Rec, boosting three typical sequential models with an average improvement of 47.97%, even enabling them to surpass LLM-based recommenders in some cases.</p>
    <p><strong>Categories:</strong> Knowledge Distillation, Sequential Recommenders, Large Language Models (LLMs), Recommendation Systems, Performance Improvement, Efficiency Optimization, Cold Start, Algorithmic Innovation, Collaborative Filtering, Scalability and Efficiency (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1028/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Leveraging Large Language Models for Sequential Recommendation (2023)</h3>
    <p><strong>Authors:</strong> Panos Louridas, Dietmar Jannach, Marios Fragkoulis, Wouter Zorgdrager, Jesse Harte, Asterios Katsifodimos</p>
    <p>Sequential recommendation problems have received increasing attention in research during the past few years, leading to the inception of a large variety of algorithmic approaches. In this work, we explore how large language models (LLMs), which are nowadays introducing disruptive effects in many AI-based applications, can be used to build or improve sequential recommendation approaches. Specifically, we devise and evaluate three approaches to leverage the power of LLMs in different ways. Our results from experiments on two datasets show that initializing the state-of-the-art sequential recommendation model BERT4Rec with embeddings obtained from an LLM improves NDCG by 15-20% compared to the vanilla BERT4Rec model. Furthermore, we find that a simple approach that leverages LLM embeddings for producing recommendations, can provide competitive performance by highlighting semantically related items. We publicly share the code and data of our experiments to ensure reproducibility.</p>
    <p><strong>Categories:</strong> Large Language Models, Sequential Recommendation, Algorithmic Approaches, Recommendation Systems, Evaluation Methods, Natural Language Processing, Performance Improvement, Embeddings, Reproducibility, Datasets, Research Methods (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/956/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Advancing Automation of Design Decisions in Recommender System Pipelines (2023)</h3>
    <p><strong>Authors:</strong> Tobias Vente</p>
    <p>Recommender systems have become essential in domains like streaming services, social media platforms, and e-commerce websites.  However, the development of a recommender system involves a complex pipeline with preprocessing, data splitting, algorithm and model selection, and postprocessing stages, requiring critical design decisions.  Every stage of the recommender systems pipeline requires design decisions that influence the performance of the recommender system. To ease design decisions, automated machine learning (AutoML) techniques have been adapted to the field of recommender systems, resulting in various AutoRecSys libraries.  Nevertheless, these libraries lack library independence and limit flexibility in integrating automation techniques from different sources. In response, our research aims to enhance the usability of AutoML techniques for design decisions in recommender system pipelines.  We focus on developing flexible and library-independent automation techniques for algorithm selection, model selection, and postprocessing steps.  By enabling developers to make informed choices and ease the recommender system development process, we decrease the developer’s effort while improving the performance of the recommender systems.  Moreover, we want to analyze the cost-to-benefit ratio of automation techniques in recommender systems, evaluating the computational overhead and the resulting improvements in predictive performance.  Our objective is to leverage AutoML concepts to automate design decisions in recommender system pipelines, reduce manual effort, and enhance the overall performance and usability of recommender systems.</p>
    <p><strong>Categories:</strong> Recommender Systems, AutoML, Automation Techniques, Library Independence, Recommender System Pipelines, Cost-Benefit Analysis, Algorithm Selection, Usability, Performance Improvement (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/978/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Heterogeneous Knowledge Fusion: A Novel Approach for Personalized Recommendation via LLM (2023)</h3>
    <p><strong>Authors:</strong> Zhichao Feng, Wei Lin, Zixiang Ding, Xiang Li, Bin Yin, Yu Qin, Junjie Xie</p>
    <p>The analysis and mining of user heterogeneous behavior are of paramount importance in recommendation systems. However, the conventional approach of incorporating various types of heterogeneous behavior into recommendation models leads to feature sparsity and knowledge fragmentation issues. To address this challenge, we propose a novel approach for personalized recommendation via Large Language Model (LLM), by extracting and fusing heterogeneous knowledge from user heterogeneous behavior information. In addition, by combining heterogeneous knowledge and recommendation tasks, instruction tuning is performed on LLM for personalized recommendations. The experimental results demonstrate that our method can effectively integrate user heterogeneous behavior and significantly improve recommendation performance.</p>
    <p><strong>Categories:</strong> Heterogeneous Data, Large Language Models (LLMs), Personalized Recommendations, User Behavior Analysis, Knowledge Fusion, Recommendation Systems, Performance Improvement, Natural Language Processing (NLP), Heterogeneous Knowledge, Data Handling Challenges, Instruction Tuning, Advanced Techniques in Recommendation (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/997/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Deep Situation-Aware Interaction Network for Click-Through Rate Prediction (2023)</h3>
    <p><strong>Authors:</strong> Yongkang Wang, Dong Wang, Yisong Yu, Beihong Jin, Yapeng Zhang, Jian Dong, Xingxing Wang, Shuli Wang, Yimin Lv</p>
    <p>User behavior sequence modeling plays a significant role in Click-Through Rate (CTR) prediction on e-commerce platforms. Except for the interacted items, user behaviors contain rich interaction information, such as the behavior type, time, location, etc. However, so far, the information related to user behaviors has not yet been fully exploited. In the paper, we propose the concept of a situation and situational features for distinguishing interaction behaviors and then design a CTR model named Deep Situation-Aware Interaction Network (DSAIN). DSAIN first adopts the reparameterization trick to reduce noise in the original user behavior sequences. Then it learns the embeddings of situational features by feature embedding parameterization and tri-directional correlation fusion. Finally, it obtains the embedding of behavior sequence via heterogeneous situation aggregation. We conduct extensive offline experiments on three real-world datasets. Experimental results demonstrate the superiority of the proposed DSAIN model. More importantly, DSAIN has increased the CTR by 2.70\%, the CPM by 2.62\%, and the GMV by 2.16\% in the online A/B test. Now, DSAIN has been deployed on the Meituan food delivery platform and serves the main traffic of the Meituan takeout app. Our source code is available at https://github.com/W-void/DSAIN</p>
    <p><strong>Categories:</strong> Deep Learning, Matrix Factorization, Interaction Network, E-commerce, Food Delivery, Click-Through Rate Prediction, Situational Awareness, User Behavior Modeling, Contextual Features, Sequence Modeling, A/B Test, Performance Improvement, User Interaction Analysis (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/849/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>MCM: A Multi-task Pre-trained Customer Model for Personalization (2023)</h3>
    <p><strong>Authors:</strong> Tianxin Wang, Peng Wan, Rui Luo, Jingyuan Deng</p>
    <p>Personalization plays a critical role in helping customers discover the products and contents they prefer for e-commerce stores.Personalized recommendations differ in contents, target customers, and UI. However, they require a common core capability – the ability to deeply understand customers’ preferences and shopping intents. In this paper, we introduce the MLCM (Multi-task Large pre-trained Customer Model), a large pre-trained BERT-based multi-task customer model with 10 million trainable parameters for e-commerce stores. This model aims to empower all personalization projects by providing commonly used preference scores for recommendations, customer embeddings for transfer learning, and a pre-trained model for fine-tuning. In this work, we improve the SOTA BERT4Rec framework to handle heterogeneous customer signals and multi-task training as well as innovate new data augmentation method that is suitable for recommendation task. Experimental results show that MLCM outperforms the original BERT4Rec by 17% on preference prediction tasks. Additionally, we demonstrate that the model can be easily fine-tuned to assist a specific recommendation task. For instance, after fine-tuning MLCM for an incentive based recommendation project, performance improves by 60% on the conversion prediction task and 25% on the click-through prediction task compared to the production baseline model.</p>
    <p><strong>Categories:</strong> Personalization, Recommendation Systems, Multi-Task Learning, Deep Learning, E-Commerce, Customer Modeling, Natural Language Processing, Transfer Learning, Data Augmentation, Model Fine-Tuning, Performance Improvement, Incentive-Based Recommendations, Conversion Prediction, Click-Through Rate, Real-World Applications. (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1004/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>A Systematic Review and Replicability Study of BERT4Rec for Sequential Recommendation (2022)</h3>
    <p><strong>Authors:</strong> Aleksandr Petrov, Craig Macdonald</p>
    <p>BERT4Rec is an effective model for sequential recommendation based on the Transformer architecture. In the original publication, BERT4Rec claimed superiority over other available sequential recommendation approaches (e.g. SASRec), and it is now frequently being used as a state-of-the art baseline for sequential recommendation. However, not all subsequent publications confirmed its superiority and have proposed other models that were shown to outperform BERT4Rec in effectiveness. In this paper we systematically review all publications that compare BERT4Rec with another popular Transformer-based model, namely SASRec, and show that BERT4Rec results are not consistent within these publications. To understand the reasons behind this inconsistency, we analyse the available implementations of BERT4Rec and show that we fail to reproduce results of the original BERT4Rec publication when using their default configuration parameters. However, we are able to replicate the reported results with the original code if training for a much longer amount of time (up to 30x) compared to the default configuration. We also propose our own implementation of BERT4Rec based on the HuggingFace Transformers library, which we demonstrate replicates the originally reported results on 3 out 4 datasets, while requiring up to 95% less training time to converge. Overall, from our systematic review and detailed experiments, we conclude that BERT4Rec does indeed exhibit state-of-the-art effectiveness for sequential recommendation, but only when trained for a sufficient amount of time. Additionally, we show that our implementation can further benefit from adapting other Transformer architectures that are available in the HuggingFace Transformers library (e.g. using disentangled attention, as provided by DeBERTa, or larger hidden layer size cf. ALBERT). For example, on the MovieLens-1M dataset, we demonstrate that both these models can improve BERT4Rec performance by up to 9%. Moreover, we show that an ALBERT-based BERT4Rec model achieves better performance on that dataset than state-of-the-art results reported in the most recent publications.</p>
    <p><strong>Categories:</strong> BERT4Rec, SASRec, Transformer-based Models, Sequential Recommendation, Systematic Review, Replicability Study, HuggingFace Transformers Library, Training Time Optimization, Effectiveness, Performance Improvement (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/785/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Context and Attribute-Aware Sequential Recommendation via Cross-Attention (2022)</h3>
    <p><strong>Authors:</strong> Ahmed Rashed, Lars Schmidt-Thieme, Shereen Elsayed</p>
    <p>In sparse recommender settings, users’ context and item attributes play a crucial role in deciding which items to recommend next. Despite that, recent works in sequential and time-aware recommendations usually either ignore both aspects or only consider one of them, limiting their predictive performance. In this paper, we address these limitations by proposing a context and attribute-aware recommender model (CARCA) that can capture the dynamic nature of the user profiles in terms of contextual features and item attributes via dedicated multi-head self-attention blocks that extract profile-level features and predict item scores. Also, unlike many of the current state-of-the-art sequential item recommendation approaches that use a simple dot-product between the most recent item’s latent features and the target items embeddings for scoring, CARCA uses cross-attention between all profile items and the target items to predict their final scores. This cross-attention allows CARCA to harness the correlation between old and recent items in the user profile and their influence on deciding which item to recommend next. Experiments on four real-world recommender system datasets show that the proposed model significantly outperforms all state-of-the-art models in the task of item recommendation and achieving improvements of up to 53% in Normalized Discounted Cumulative Gain (NDCG) and Hit-Ratio. Results also show that CARCA outperformed several state-of-the-art dedicated image-based recommender systems by merely utilizing image attributes extracted from a pre-trained ResNet50 in a black-box fashion.</p>
    <p><strong>Categories:</strong> Sequential Recommendation, Context-Aware, Attribute-Based, Cross-Attention, Recommender Systems, Effectiveness Evaluation, Recommendation Algorithm, Visual Attributes, Transfer Learning, Dynamic Profiling, Performance Improvement (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/752/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Automate Page Template Optimization: An Offline Deep Q-Learning Approach (2022)</h3>
    <p><strong>Authors:</strong> Wenyang Liu, Zhou Qin</p>
    <p>The modern e-commerce web pages have brought better customer experience and more profitable services by whole page optimization at different granularity, e.g., page layout optimization, item ranking optimization, etc. Generating the proper page layout per customer’s request is one of the vital tasks during the web page rendering process, which can directly impact customers’ shopping experience and their decision-making. In this paper, we formulate the request-rendering interactions as a Markov decision process (MDP) and solve it by deep reinforcement learning (RL). Specifically, we present the design and implementation of applying offline Deep Q-Learning (DQN) to the contextual page layout optimization problem. Through the offline evaluation method, we demonstrate the effectiveness of the proposed framework, i.e., the RL agent has the potential to perform better than the baseline ranker by learning from the offline data set, e.g., the RL agent can improve the average cumulative rewards up to 36.69% comparing to the baseline ranker.</p>
    <p><strong>Categories:</strong> Reinforcement Learning, Deep Learning, E-commerce Optimization, Web Page Layout Optimization, Markov Decision Process (MDP), Offline Deep Q-Learning, Customer Experience, Performance Improvement, Profitability Optimization, Algorithm Design, Offline Evaluation Methods, Machine Learning Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/832/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Adversarial Tensor Factorization for Context-aware Recommendation (2019)</h3>
    <p><strong>Authors:</strong> Huiyuan Chen, Jing Li</p>
    <p>Contextual factors such as time, location, or tag, can affect user preferences for a particular item. Context-aware recommendations are thus critical to improve both quality and explainability of recommender systems, compared to traditional recommendations solely based on user-item interactions. Tensor factorization machines have achieved state-of-the-art performance  due to their generic integration of users, items, and contextual factors in one unify way. However,  few work has focused on the robustness of a context-aware recommender system. Improving the robustness of a tensor-based model is challenging due to the sparsity of the observed tensor and the multi-linear nature of tensor factorization. In this paper, we propose ATF, a model that combines tensor factorization and adversarial learning for context-aware recommendations. Doing so allows us to reap the benefits of tensor factorization, while enhancing the robustness of a recommender model, and thus improves its performance. Empirical studies on two real-world datasets show that the proposed method outperforms standard tensor-based  methods. i>Presentation: Monday Poster Lunch</i</p>
    <p><strong>Categories:</strong> Context-aware Recommendations, Tensor Factorization, Adversarial Learning, Recommender Systems, Robustness, Performance Improvement, Multi-Linear Models, Real World Applications, Recommendation Quality (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/464/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Explicit Elimination of Similarity Blocking for Session-based Recommendation (2016)</h3>
    <p><strong>Authors:</strong> Martha Larson, Mattia Brusamento, Paolo Cremonesi, Roberto Pagano</p>
    <p>A single `odd’ interaction can cause two user interaction sessions to diverge in similarity, and stand in the way of generalization. The sensitivity of session-based recommenders to session similarity motivates us to explicitly identify and remove such `similarity blockers’. Specifically, we leverage huge amounts of data, which allow us to identify blockers in the form of non-co-occurring items. Other blockers can be identified using content-based similarity. Our experiments reveal that explicitly eliminating relatively few blockers improves performance.</p>
    <p><strong>Categories:</strong> Session-Based Recommendation, Similarity Blocking, Recommendation Algorithms, Data Utilization, Content-Based Methods, Performance Improvement, User Interaction Analysis, Generalization in ML (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/221/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Analysis of User-generated Content for Improving YouTube Video Recommendation (2015)</h3>
    <p><strong>Authors:</strong> Fabio Gasparetti, Davide Feltoni Gurini, Alessandro Micarelli, Giuseppe Sansonetti, Michele Galli</p>
    <p>Everyday video-sharing websites such as YouTube collect large amounts of new multimedia resources. Comments left by viewers often provide valuable information to describe sentiments, opinions and tastes of users. For this reason, we propose a novel re-ranking approach that takes into consideration that information in order to provide better recommendations of related videos. Early experiments indicate an improvement in the recommendation performance.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Video Sharing Platforms, Sentiment Analysis, User-generated Content, Textual Data, Re-ranking Approaches, Performance Improvement, Machine Learning Techniques, User Feedback, Recommendation Evaluation (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/144/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>