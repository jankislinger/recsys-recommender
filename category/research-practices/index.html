<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">Research Practices</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/explainability/">Explainability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/offline-evaluation/">Offline Evaluation</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/real-world-applications/">Real-world Applications</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Does It Look Sequential? An Analysis of Datasets for Evaluation of Sequential Recommendations. (2024)</h3>
    <p><strong>Authors:</strong> Anna Volodkevich, Alexey Vasilev, Anton Klenitskiy, Anton Pembek</p>
    <p>Sequential recommender systems are an important and demanded area of research. Such systems aim to use the order of interactions in a user’s history to predict future interactions. The premise is that the order of interactions and sequential patterns play an important role. Therefore, it is crucial to use datasets that exhibit a sequential structure for a proper evaluation of sequential recommenders. We apply several methods based on the random shuffling of the user’s sequence of interactions to assess the strength of sequential structure across 15 datasets, frequently used for sequential recommender systems evaluation in recent research papers presented at top-tier conferences. As shuffling explicitly breaks sequential dependencies inherent in datasets, we estimate the strength of sequential patterns by comparing metrics for shuffled and original versions of the dataset. Our findings show that several popular datasets have a rather weak sequential structure.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Sequential Recommendations, Dataset Evaluation, Methodology Analysis, Strength of Sequential Patterns, Importance of Proper Evaluation, Research Practices, Challenges in Evaluation (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1087/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Everyone’s a Winner! On Hyperparameter Tuning of Recommendation Models (2023)</h3>
    <p><strong>Authors:</strong> Dietmar Jannach, Faisal Shehzad</p>
    <p>The performance of a recommender system algorithm in terms of common offline accuracy measures often strongly depends on the chosen hyperparameters. Therefore, when comparing algorithms in offline experiments, we can obtain reliable insights regarding the effectiveness of a newly proposed algorithm only if we compare it to a number of state-of-the-art baselines that are carefully tuned for each of the considered datasets. While this fundamental principle of any area of applied machine learning is undisputed, we find that the tuning process for the baselines in the current literature is barely documented in much of today’s published research. Ultimately, in case the baselines are actually not carefully tuned, progress may remain unclear. In this paper, we showcase how every method in such an unsound comparison can be reported to be outperforming the state-of-the-art. Finally, we iterate appropriate research practices to avoid unreliable algorithm comparisons in the future.</p>
    <p><strong>Categories:</strong> Algorithm Comparison, Hyperparameter Tuning, Reproducibility, Research Methodology, Model Evaluation, Experimental Design, Best Practices, Recommendation Systems, Research Practices (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/942/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>The effect of third party implementations on reproducibility (2023)</h3>
    <p><strong>Authors:</strong> Ádám Tibor Czapp, Balázs Hidasi</p>
    <p>Reproducibility of recommender systems research has come under scrutiny during recent years. Along with works focusing on repeating experiments with certain algorithms, the research community has also started discussing various aspects of evaluation and how these affect reproducibility. We add a novel angle to this discussion by examining how unofficial third-party implementations could benefit or hinder reproducibility. Besides giving a general overview, we thoroughly examine six third-party implementations of a popular recommender algorithm and compare them to the official version on five public datasets. In the light of our alarming findings we aim to draw the attention of the research community to this neglected aspect of reproducibility.</p>
    <p><strong>Categories:</strong> Reproducibility, Recommender Systems, Implementation Details, Evaluation Methods, Research Practices, Third-Party Software, Research Methodology, Benchmarking, Empirical Analysis, Neglected Aspects, Practical Implications, Software Tools (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/951/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Widespread flaws in offline evaluation of recommender systems (2023)</h3>
    <p><strong>Authors:</strong> Ádám Tibor Czapp, Balázs Hidasi</p>
    <p>Even though offline evaluation is just an imperfect proxy of online performance — due to the interactive nature of recommenders — it will probably remain the primary way of evaluation in recommender systems research for the foreseeable future, since the proprietary nature of production recommenders prevents independent validation of A/B test setups and verification of online results. Therefore, it is imperative that offline evaluation setups are as realistic and as flawless as they can be. Unfortunately, evaluation flaws are quite common in recommenders systems research nowadays, due to later works copying flawed evaluation setups from their predecessors without questioning their validity. In the hope of improving the quality of offline evaluation of recommender systems, we discuss four of these widespread flaws and why researchers should avoid them.</p>
    <p><strong>Categories:</strong> Evaluation Methodology, Offline Evaluation, Research Limitations, Research Flaws, Evaluation Challenges, Methodology Design, Underlying Assumptions, Real-World Applications, Research Practices, Evaluation Setup and Execution, Best Practices. (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/917/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Exploring Data Splitting Strategies for the Evaluation of Recommendation Models (2020)</h3>
    <p><strong>Authors:</strong> Craig Macdonald, Richard McCreadie, Zaiqiao Meng, Iadh Ounis</p>
    <p>Effective methodologies for evaluating recommender systems are critical, so that different systems can be compared in a sound manner. A commonly overlooked aspect of evaluating recommender systems is the selection of the data splitting strategy. In this paper, we both show that there is no standard splitting strategy and that the selection of splitting strategy can have a strong impact on the ranking of recommender systems during evaluation. In particular, we perform experiments comparing three common data splitting strategies, examining their impact over seven state-of-the-art recommendation models on two datasets. Our results demonstrate that the splitting strategy employed is an important confounding variable that can markedly alter the ranking of recommender systems, making much of the currently published literature non-comparable, even when the same datasets and metrics are used.</p>
    <p><strong>Categories:</strong> Evaluation Methods, Recommender Systems, Data Splitting, Model Comparison, Research Methodology, Impact of Evaluation Strategy, Reproducibility, Experimental Design, Cross-Dataset Analysis, Statistical Significance, Research Practices (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/608/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Performance Comparison of Neural and Non-Neural Approaches to Session-based Recommendation (2019)</h3>
    <p><strong>Authors:</strong> Malte Ludewig, Sara Latifi, Noemi Mauro, Dietmar Jannach</p>
    <p>The benefits of neural approaches are undisputed in many application areas. However, today’s research practice in applied machine learning‚ where researchers often use a variety of baselines, datasets, and evaluation procedures, can make it difficult to understand how much progress is actually achieved through novel technical approaches. In this work, we focus on the fast-developing area of session-based recommendation and aim to contribute to a better understanding of what represents the state-of-the-art. To that purpose, we have conducted an extensive set of experiments, using a variety of datasets, in which we benchmarked four neural approaches that were published in the last three years against each other and against a set of simpler baseline techniques, e.g., based on nearest neighbors. The evaluation of the algorithms under the exact same conditions revealed that the benefits of applying today’s neural approaches to session-based recommendations are still limited. In the majority of the cases, and in particular when precision and recall are used, it turned out that simple techniques in most cases outperform recent neural approaches. Our findings therefore point to certain major limitations of today’s research practice. By sharing our evaluation framework publicly, we hope that some of these limitations can be overcome in the future. i>Presentation: Tuesday Poster Lunch</i</p>
    <p><strong>Categories:</strong> Neural Networks, Recommendation Systems, Session-Based Recommendations, Algorithm Comparison, Performance Evaluation, Machine Learning, Evaluation Framework, Research Practices (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/490/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>