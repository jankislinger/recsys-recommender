<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/explainability/">Explainability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reinforcement-learning/">Reinforcement Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/e-commerce/">E-commerce</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Explainability in Music Recommender System (2024)</h3>
    <p><strong>Authors:</strong> Shahrzad Shashaani</p>
    <p>Recommendation systems play a crucial role in our daily lives, influencing many of our significant and minor decisions. These systems also have become integral to the music industry, guiding users to discover new content based on their tastes. However, the lack of transparency in these systems often leaves users questioning the rationale behind recommendations. To address this issue, adding transparency and explainability to recommender systems is a promising solution. Enhancing the explainability of these systems can significantly improve user trust and satisfaction. This research focuses on exploring transparency and explainability in the context of recommendation systems, focusing on the music domain. This research can help to understand the gaps in explainability in music recommender systems to create more engaging and trustworthy music recommendations.</p>
    <p><strong>Categories:</strong> Music Recommendations, Explainability, Transparency, User Trust, Recommendation Systems, Human-Computer Interaction, Algorithmic Transparency, Ethical Considerations, Case Study, Music Industry Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1132/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Fairness and Transparency in Music Recommender Systems: Improvements for Artists (2024)</h3>
    <p><strong>Authors:</strong> Karlijn Dinnissen</p>
    <p>Music streaming services have become one of the main sources of music consumption in the last decade, with recommender systems as an important component. As those systems partially decide the songs that music consumers listen to, the systems greatly impact the artists who created the songs. However, when evaluating performance and fairness of these music recommending systems (MRSs), the perspective of the item providers or other music industry professionals is often not considered. Additionally, artists indicate they would appreciate more transparency – both towards and users and the artists themselves – regarding why certain items are recommended and others are not. This research project takes a multi-stakeholder approach to bridge the gap between music systems and their item providers. We first establish artists’ and music industry professionals’ perspective on MRSs through interviews and questionnaires. Based on those insights, we then aim to increase matching between end users and lesser-known artists by generating rich item and user representations. Results will be evaluated both quantitatively and qualitatively. Lastly, we plan to effectively communicate MRS fairness by increasing transparency for both end users and artists.</p>
    <p><strong>Categories:</strong> Fairness, Transparency, Music Recommender Systems (MRS), Artist Perspective, Stakeholder Approach, Evaluation Methods, Recommendation Algorithms, Representation Learning, Multi-Stakeholder Systems, User-Centric Design, Algorithmic Transparency, Diversity in Recommendations (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1136/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Demystifying Recommender Systems: A Multi-faceted Examination of Explanation Generation, Impact, and Perception (2023)</h3>
    <p><strong>Authors:</strong> Giacomo Balloccu</p>
    <p>Recommender systems have become an integral component of the digital landscape, impacting a multitude of services and industries ranging from e-commerce to entertainment and beyond. By offering personalised suggestions, these systems challenge a fundamental problem in our modern information society named information overload. As users face a deluge of choices, recommender systems help sift through this immense sea of possibilities, delivering a personalised subset of options that align with user preferences and historical behaviour. However, despite their considerable utility, recommender systems often operate as “black boxes,” obscuring the rationale behind recommendations. This opacity can engender mistrust and undermine user engagement, thus attenuating the overall effectiveness of the system. Researchers have emphasized the importance of explanations in recommender systems, highlighting how explanations can enhance system transparency, foster user trust, and improve decision-making processes, thereby enriching user experiences and yielding potential business benefits. Yet, a significant gap persists in the current state of human-understandable explanations research. While recommender systems have grown increasingly complex, our capacity to generate clear, concise, and relevant explanations that reflect this complexity remains limited. Crafting explanations that are both understandable and reflective of sophisticated algorithmic decision-making processes poses a significant challenge, especially in a manner that meets the user’s cognitive and contextual needs.</p>
    <p><strong>Categories:</strong> Explainability, User Trust, Information Overload, Transparency, Artificial Intelligence, Human-Computer Interaction, Personalization, Relevance, Algorithmic Transparency, Cross-Domain Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/975/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>EXTRA: Explaining Team Recommendation in Networks (2018)</h3>
    <p><strong>Authors:</strong> Nan Cao, Qinghai Zhou, Hanghang Tong, Norbou Buchler, Liangyue Li</p>
    <p>State-of-the-art in network science of teams offers effective recommendation methods to answer questions like who is the best replacement, what is the best team expansion strategy, but lacks intuitive ways to explain why the optimization algorithm gives the specific recommendation for a given team optimization scenario. To tackle this problem, we develop an interactive prototype system, Extra, as the first step towards addressing such a sense-making challenge, through the lens of the underlying network where teams embed, to explain the team recommendation results. The main advantages are (1) Algorithm efficacy: we propose an effective and fast algorithm to explain random walk graph kernel, the central technique for networked team recommendation; (2) Intuitive visual explanation: we present intuitive visual analysis of the recommendation results, which can help users better understand the rationality of the underlying team recommendation algorithm.</p>
    <p><strong>Categories:</strong> Team Recommendation, Recommendation Explanation, Visualization, Algorithmic Transparency, Network Analysis, Optimization Algorithms, Sense-Making Tools, User Interaction, Human-Computer Interaction, Collaborative Systems (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/395/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Using Explainability for Constrained Matrix Factorization (2017)</h3>
    <p><strong>Authors:</strong> Olfa Nasraoui, Behnoush Abdollahi</p>
    <p>Accurate model-based Collaborative Filtering (CF) approaches tend to be black-box machine learning models, such as Matrix Factorization (MF), that lack interpretability and do not provide a straightforward explanation for their outputs. Yet explanations can improve the transparency of a recommender system by justifying recommendations, and this in turn can enhance the user’s trust in the recommendations. Hence, one main challenge in designing a recommender system is mitigating the trade-off between an explainable technique with moderate prediction accuracy and a more accurate technique with no explainable recommendations. In this paper, we focus on MF and further assume the absence of any additional data source, such as item content or user attributes. We propose an explainability constrained MF technique that computes the top-n recommendation list from items that are explainable. Experimental results show that our method is effective in generating accurate and explainable recommendations.</p>
    <p><strong>Categories:</strong> Matrix Factorization, Collaborative Filtering, Explainable AI, Transparency, Trust, User Trust, Recommendation Systems, Algorithmic Transparency, Interpretability, Evaluation Metrics, Performance Analysis (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/309/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>