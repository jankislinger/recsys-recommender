<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/explainability/">Explainability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/e-commerce/">E-commerce</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reinforcement-learning/">Reinforcement Learning</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Bias in Book Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Savvina Daniil</p>
    <p>Recommender systems are prevalent in many applications, but hide risks; issues like bias propagation have been on the focus of related studies in recent years. My own research revolves around tracking bias in the book recommendation domain. Specifically, I am interested in whether the incorporation of recommender systems in a library’s loaning system serves their social responsibility and purpose, with bias being the main point of concern. To this end, I engage with the topic in three ways; by mapping the area of ethics in book recommendation, by investigating and reflecting on challenges with studying bias in recommender systems in general, and by showcasing a set of social implication of statistical bias in the book recommendation domain in particular. In this doctoral symposium paper, I further elaborate on the problem at hand, the outline of my thesis, the progress I have made so far, as well as my plans for future work along with specific questions that have arisen from my research efforts.</p>
    <p><strong>Categories:</strong> Book Recommendation, Recommender Systems, Ethics, Bias, Statistical Bias, Social Implications, Library Systems, Social Responsibility, Algorithmic Fairness (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1127/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Imbalanced Data Sparsity as Source of Unfair Bias in Collaborative Filtering (2022)</h3>
    <p><strong>Authors:</strong> Chin Lin Wong, Fernando Mourão, Farhad Zafari, Sabir Ribas, Saumya Pandey, Aditya Joshi, Diego Marinho de Oliveira</p>
    <p>Collaborative Filtering (CF) is a class of methods widely used to support high-quality Recommender Systems (RSs) across several industries [6]. Studies have uncovered distinct advantages and limitations of CF in many real-world applications [5, 9]. Besides the inability to address the cold-start problem, sensitivity to data sparsity is among the main limitations recurrently associated with this class of RSs. Past work has extensively demonstrated that data sparsity critically impacts CF accuracy [2, 3, 4]. The proposed talk revisits the relation between data sparsity and CF from a new perspective, evincing that the former also impacts the fairness of recommendations. In particular, data sparsity might lead to unfair bias in domains where the volume of activity strongly correlates with personal characteristics that are protected by law (i.e., protected attributes). This concern is critical for RSs deployed in domains such as the recruitment domain, where RSs have been reported to automate or facilitate discriminatory behaviour [7]. Our work at SEEK deals with recommender algorithms that recommend jobs to candidates via SEEK’s multiple channels. While this talk focuses on our perspective of the problem in the job recommendation domain, the discussion is relevant to many other domains where recommenders potentially have a social or economic impact on the lives of individuals and groups.</p>
    <p><strong>Categories:</strong> Collaborative Filtering, Unfair Bias, Data Sparsity, Recommender Systems, Recruitment Domain, Social Implications, Ethics, Fairness, Evaluation Metrics, User Bias, Algorithm Improvements, Domains with High Impact (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/825/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Measuring and Mitigating Bias and Harm in Personalized Advertising (2021)</h3>
    <p><strong>Authors:</strong> Muhammad Ali</p>
    <p>Online personalized advertising is often very effective in identifying relevant audiences for each piece of content, which has led to its widespread adoption. In today’s internet, however, these advertising systems are used not only to market products, but also consequential life opportunities such as employment or housing, as well as socially important political messaging. This has led to increasing concerns about the presence of algorithmic bias and possible discrimination in these important domains — with results showing problematic biases along gender, race, and political affiliation, even when the advertiser might have targeted broadly.<br>A growing body of work focuses on measuring and characterizing these biases, as well as finding ways to mitigate these effects and building responsible systems. However, these results often emerge from different scientific communities and are often disconnected in the literature. In this paper, I attempt at bridging the gap between isolated efforts to either measure these biases, or to mitigate them. I discuss how the need to measure bias in advertising, and the efforts to mitigate it, despite being distant in the literature, are complementary problems that need to center their methodolgy around user studies.<br>This paper presents a research agenda that focuses on the need for user-centric measurements of bias, by collecting real ads from users, and using surveys to understand user perceptions for these ads. My approach also calls for incorporating user sentiments into the mitigation efforts, by constraining optimization on user values that emerge from surveys. Finally, I also emphasize the need for involving users in the evaluation of responsible advertising systems; efforts to mitigate bias eventually need to be contextualized in terms of benefits to users instead of simple performance tradeoffs. My focus on the users is motivated by the fact that they are stakeholders in personalized advertising, vulnerable at the hand of algorithmic bias and harm, and therefore crucial in both efforts to measure and mitigate these effects.</p>
    <p><strong>Categories:</strong> Bias in Algorithms, Discrimination, Personalized Advertising, User-Centric Design, Stakeholder Analysis, Measurement Tools, Mitigation Strategies, Responsible AI, Ethics, Harm Reduction, Real-World Applications, Fairness in Algorithms, Cross-Disciplinary Approaches, Algorithmic Justice, User Studies (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/719/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Debiasing Item-to-Item Recommendations With Small Annotated Datasets (2020)</h3>
    <p><strong>Authors:</strong> Tobias Schnabel, Paul N. Bennett</p>
    <p>Item-to-item recommendation (e.g., “People who like this also like...”) is a ubiquitous and important type of recommendation in real-world systems. Observational data from historical interaction logs abound in these settings. However, since virtually all observational data exhibit biases, such as time-in-inventory or interface biases, it is crucial that recommender algorithms account for these biases. In this paper, we develop a principled approach for item-to-item recommendation based on causal inference and present a practical and highly effective method for estimating the causal parameters from a small annotated dataset. Empirically, we find that our approach substantially improves upon existing methods while requiring only small amounts of annotated data.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Bias Mitigation, Fairness, Item-to-Item Recommendations, Causal Inference, Beyond Accuracy, Empirical Evaluation, Practical Applications, Ethics, Robustness, Small Datasets, Evaluation Methods (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/530/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>