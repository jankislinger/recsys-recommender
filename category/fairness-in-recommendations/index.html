<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">Fairness in Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/bias-mitigation/">Bias Mitigation</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reinforcement-learning/">Reinforcement Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Multi-Objective Recommendation via Multivariate Policy Learning (2024)</h3>
    <p><strong>Authors:</strong> Ivan Potapov, Sourabh Vaid, Wenzhe Shi, Nakul Agarwal, Olivier Jeunen, Jatin Mandav, Aleksei Ustimenko</p>
    <p>Real-world recommender systems often need to balance multiple objectives when deciding which recommendations to present to users. These include behavioural signals (e.g. clicks, shares, dwell time), as well as broader objectives (e.g. diversity, fairness). Scalarisation methods are commonly used to handle this balancing task, where a weighted average of per-objective reward signals determines the final score used for ranking. Naturally, <i>how</i> these weights are computed exactly, is key to success for any online platform. We frame this as a decision-making task, where the scalarisation weights are <i>actions</i> taken to maximise an overall North Star reward (e.g. long-term user retention or growth). We extend existing policy learning methods to the continuous multivariate action domain, proposing to maximise a pessimistic lower bound on the North Star reward that the learnt policy will yield. Typical lower bounds based on normal approximations suffer from insufficient coverage, and we propose an efficient and effective policy-dependent correction for this. We provide guidance to design stochastic data collection policies, as well as highly sensitive reward signals. Empirical observations from simulations, offline and online experiments highlight the efficacy of our deployed approach.</p>
    <p><strong>Categories:</strong> Multi-Objective Optimization, Fairness in Recommendations, Diversity of Recommendations, Policy Learning, Reinforcement Learning in Recommender Systems, Implicit Feedback, Real-World Applications, A/B Testing, Scalarization Methods, Evaluation Methods, Scalability (<i>deepseek-r1:70b</i>)</p>
    <p><a href="/recsys-recommender/response/1051/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>CAPRI-FAIR: Integration of Multi-sided Fairness in Contextual POI Recommendation Framework (2024)</h3>
    <p><strong>Authors:</strong> Yonchanok Khaokaew, Jeffrey Chan, Flora D. Salim, Francis Dela Cruz</p>
    <p>Point-of-interest (POI) recommendation, a form of context-aware recommendation, takes into account spatio-temporal constraints and contexts like distance, peak business hours, and previous user check-ins. Given the ability of these kinds of systems to influence not just the consumer’s travel experience, but also the POI’s business, it is important to consider fairness from multiple perspectives. Unfortunately, these systems tend to provide less accurate recommendations to inactive users, and less exposure to unpopular POIs. The goal of this paper is to develop a post-filter methodology that incorporates provider and consumer fairness factors into pre-existing recommendation models, to satisfy fairness metrics like item exposure, and performance metrics like precision and distance, making the system more sustainable to both consumers and providers. Experiments have shown that using a linear scoring model for provider fairness in re-scoring recommended items yields the best tradeoff between performance and long-tail exposure, in some cases without a significant decrease in precision. When attempting to address consumer fairness by recommending more popular POIs to inactive users, the result was an increase in precision for only some recommendation models and datasets. Finally, when considering the tradeoff between both parameters, the combinations that reached the Pareto front of consumer and provider fairness, unfortunately, achieved the lowest precision values. We find that the nature of this tradeoff depends heavily on the model and the dataset.</p>
    <p><strong>Categories:</strong> Fairness in Recommendations, Context-Aware Recommendations, Point of Interest (POI) Recommendations, Post-Filter Methodology, Provider Fairness, Consumer Fairness, Spatio-Temporal Recommendations, Long-Tail Recommendations, Diversity of Recommendations, Beyond Accuracy (<i>deepseek-r1:70b</i>)</p>
    <p><a href="/recsys-recommender/response/1078/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Putting Popularity Bias Mitigation to the Test: A User-Centric Evaluation in Music Recommenders (2024)</h3>
    <p><strong>Authors:</strong> Robin Ungruh, Karlijn Dinnissen, Maria Soledad Pera, Hanna Hauptmann, Anja Volk</p>
    <p>Popularity bias is a prominent phenomenon in recommender systems (RS), especially in the music domain. Although popularity bias mitigation techniques are known to enhance the fairness of RS while maintaining their high performance, there is a lack of understanding regarding users’ actual perception of the suggested music. To address this gap, we conducted a user study (n=40) exploring user satisfaction and perception of personalized music recommendations generated by algorithms that explicitly mitigate popularity bias. Specifically, we investigate item-centered and user-centered bias mitigation techniques, aiming to ensure fairness for artists or users, respectively. Results show that neither mitigation technique harms the users’ satisfaction with the recommendation lists despite promoting underrepresented items. However, the item-centered mitigation technique impacts user perception; by promoting less popular items, it reduces users’ familiarity with the items. Lower familiarity evokes discovery—the feeling that the recommendations enrich the user’s taste. We demonstrate that this  can ultimately lead to higher satisfaction, highlighting the potential of less-popular recommendations to improve the user experience.</p>
    <p><strong>Categories:</strong> Recommendation Algorithms, Music Recommendations, Popularity Bias, User-Centric Evaluation, Bias Mitigation, Recommender Systems Evaluation, Personalization, User Perception, Fairness in Recommendations, Discovery (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1056/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Fair Reciprocal Recommendation in Matching Markets (2024)</h3>
    <p><strong>Authors:</strong> Yoji Tomita, Tomohiko Yokoyama</p>
    <p>Recommender systems play an increasingly crucial role in shaping people’s opportunities, particularly in online dating platforms. It is essential from the user’s perspective to increase the probability of matching with a suitable partner while ensuring an appropriate level of fairness in the matching opportunities. We investigate reciprocal recommendation in two-sided matching markets between agents divided into two sides (e.g., men and women). We assume that a successful match requires mutual interest from both men and women involved, and agents would desire to be positioned favorably in the others’ the recommendation lists. We define each agent’s opportunity to be recommended and introduce its fairness criterion, envy-freeness, from the perspective of the fair division theory. The recommendations that approximately maximize the expected number of matches between men and women, empirically obtained by heuristic algorithms, are likely to result in significant unfairness of opportunity. Therefore, there can be a trade-off between maximizing the expected matches and ensuring fairness of opportunity. We propose a method to find a policy that is close to being envy-free by leveraging the Nash social welfare function. Experiments on synthetic and a real-world dataset demonstrate the effectiveness of our approach in achieving both relatively high expected matches and fairness for opportunities of both men and women in reciprocal recommender systems.</p>
    <p><strong>Categories:</strong> Fairness in Recommendations, Reciprocal Recommendation, Matching Markets, Online Dating Platforms, Envy-Freeness, Nash Social Welfare Function, Two-Sided Markets, Opportunity Fairness, Mutual Interest, Algorithmic Recommendations (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1036/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Transparently Serving the Public: Enhancing Public Service Media Values through Exploration (2023)</h3>
    <p><strong>Authors:</strong> Andreas Grün, Xenija Neufeld</p>
    <p>In the last few years, we have reportedly underlined the importance of the Public Service Media Remit for ZDF as a Public Service Media provider. Offering fair, diverse, and useful recommendations to users is just as important for us as being transparent about our understanding of these values, the metrics that we are using to evaluate their extent, and the algorithms in our system that produce such recommendations. This year, we have made a major step towards transparency of our algorithms and metrics describing them for a broader audience, offering the possibility for the audience to learn details about our systems and to provide direct feedback to us. Having the possibility to measure and track PSM metrics, we have started to improve our algorithms towards PSM values. In this work, we describe these steps and the results of actively debasing and adding exploration into our recommendations to achieve more fairness.</p>
    <p><strong>Categories:</strong> Public Service Media (PSM), Recommendation Systems, Transparency in Algorithms, Fairness in Recommendations, Diversity of Recommendations, Algorithm Evaluation, User Feedback, Exploration Strategies (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1015/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Challenges for Anonymous Session-Based Recommender Systems in Indoor Environments (2023)</h3>
    <p><strong>Authors:</strong> Alessio Ferrato</p>
    <p>Recommender Systems (RSs) have gained widespread popularity for providing personalized recommendations in manifold domains. However, considering the growing user privacy concerns, the development of recommender systems that prioritize data protection has become increasingly important. In indoor environments, RSs face unique challenges, and ongoing research is being conducted to address them. Anonymous Session-Based Recommender Systems (ASBRSs) can represent a possible solution to address these challenges while ensuring user privacy. This paper aims to bridge the gap between existing RS research and the demand for privacy-preserving recommender systems, especially in indoor settings, where significant research efforts are underway. Therefore, it proposes three research questions: How does user modeling based on implicit feedback impact on ASBRSs, considering different embedding extraction networks? How can short sessions be leveraged to start the recommendation process in ASBRSs? To what extent can ASBRSs generate fair recommendations? By investigating these questions, this study establishes the foundations for applying ASBRSs in indoor environments, safeguarding user privacy, and contributing to the ongoing research in this field.</p>
    <p><strong>Categories:</strong> Recommender Systems, Privacy Preservation, Session-Based Recommenders, Indoor Environments, Implicit Feedback, User Modeling, Short Sessions Handling, Real-World Applications, Data Protection, Methodology, Fairness in Recommendations, Review/Foundational Research (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/979/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Exploring the Impact of Temporal Bias in Point-of-Interest Recommendation (2022)</h3>
    <p><strong>Authors:</strong> Ali Tourani, Hossein A. Rahmani, Yashar Deldjoo, Mohammadmehdi Naghiaei</p>
    <p>Recommending appropriate travel destinations to consumers based on contextual information such as their check-in time and location is a primary objective of Point-of-Interest (POI) recommender systems. However, the issue of contextual bias (i.e., how much consumers prefer one situation over another) has received little attention from the research community. This paper examines the effect of temporal bias, defined as the difference between users’ check-in hours, leisure vs. work hours, on the consumer-side fairness of context-aware recommendation algorithms. We believe that eliminating this type of temporal (and geographical) bias might contribute to a drop in traffic-related air pollution, noting that rush-hour traffic may be more congested. To surface effective POI recommendation, we evaluated the sensitivity of state-of-the-art context-aware models to the temporal bias contained in users’ check-in activities on two POI datasets, namely Gowalla and Yelp. The findings show that the examined context-aware recommendation models prefer one group of users over another based on the time of check-in and that this preference persists even when users have the same amount of interactions.</p>
    <p><strong>Categories:</strong> Context-Aware Recommendation, Temporal Bias, Fairness in Recommendations, Environmental Impact, Location-Based Services, POI Recommendation, Temporal Dynamics, User Behavior, Scalability, User-Centered Design, Real-World Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/789/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Two-sided Calibration for Quality-aware Responsible Recommendation (2023)</h3>
    <p><strong>Authors:</strong> Min Zhang, Chao Deng, Yiqun Liu, Haitao Zeng, Yankai Liu, Yuanqing Yu, Chenyang Wang, Weizhi Ma, Junlan Feng</p>
    <p>Calibration in recommender systems ensures that the user’s interests distribution over groups of items is reflected with their corresponding proportions in the recommendation, which has gained increasing attention recently. For example, a user who watched 80 entertainment videos and 20 knowledge videos is expected to receive recommendations comprising about 80% entertainment and 20% knowledge videos as well.  However, with the increasing calls for quality-aware responsible recommendation, it has become inadequate to just match users’ historical behaviors, which could still lead to undesired effects at the system level (e.g., overwhelming clickbaits). In this paper, we envision the two-sided calibration task that not only matches the users’ past interests distribution (user-level calibration) but also guarantees an overall target exposure distribution of different item groups (system-level calibration).  The target group exposure distribution can be explicitly pursued by users, platform owners, and even the law (e.g., the platform owners expect about 50% knowledge video recommendation on the whole). To support this scenario, we propose a post-processing method named PCT. PCT first solves personalized calibration targets that minimize the changes in users’ historical interest distributions while ensuring the overall target group exposure distribution. Then, PCT reranks the original recommendation lists according to personalized calibration targets to generate both relevant and two-sided calibrated recommendations. Extensive experiments demonstrate the superior performance of the proposed method compared to calibrated and fairness-aware recommendation approaches. We also release a new dataset with item quality annotations to support further studies about quality-aware responsible recommendation.</p>
    <p><strong>Categories:</strong> Calibration, Quality-aware Recommendation, Responsible Recommendation, Two-sided Calibration, User-level calibration, System-level calibration, Re-ranking Methods, Fairness in Recommendations, Implicit Feedback, Personalization, Content Exposure Management, Beyond Accuracy (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/890/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Translating the Public Service Media Remit into Metrics and Algorithms (2022)</h3>
    <p><strong>Authors:</strong> Andreas Grün, Xenija Neufeld</p>
    <p>After multiple years of providing automated video recommendations in the ZDFmediathek, ZDF has established a solid ground for the usage of recommender systems. Being a Public Service Media (PSM) provider, our most important driver on this journey is our Public Service Media Remit (PSMR). We are committed to cultivate PSM values such as diversity, fairness, and transparency while providing fresh and relevant content. Therefore, it is important for us to not only measure the success of our recommender systems in terms of basic business Key Performance Indicators (KPIs) such as clicks and viewing minutes but also to ensure and to measure the achievement of PSM values. While speaking about PSM values, however, it is important to keep in mind that there is no easy way to directly measure values as such. In order to be able to measure their extent in a recommender system, we need to translate these values into public value metrics. However, not only the final results are essential for the PSMR. Additionally, it is highly important to establish transparency while working towards these results, that is, while defining the data, the algorithms, and the pipelines used in recommender systems. In our talk we will provide a deeper insight into how we approach this task with Model Cards and give an overview of some models, their Model Cards, and metrics that we are currently using for ZDFmediathek.</p>
    <p><strong>Categories:</strong> Public Service Media, Metrics for Public Value, Fairness in Recommendations, Model Transparency, Recommendation Algorithms, Algorithm Design, Diversity of Recommendations, Beyond Accuracy Evaluation, Public Service Values, Metrics Design, Recommendation System Design, Transparency in Machine Learning (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/830/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Long-term fairness for Group Recommender Systems with Large Groups (2022)</h3>
    <p><strong>Authors:</strong> Patrik Dokoupil</p>
    <p>Group recommender systems (GRS) focus on recommending items to groups of users. GRS need to tackle the heterogeneity of group members’ preferences and produce recommendations of high overall utility while also considering some sense of fairness among group members. This work plans to aim for novel applications of GRS involving construction of large-scale groups of users and focusing on the long-term fairness of these groups which is in contrast with current research that concentrates on small groups of ephemeral nature. We believe that these directions could bring results of significant societal impact and scope of the effect expanding beyond currently considered GRS domains, e.g., helping to mitigate the filter bubble problem</p>
    <p><strong>Categories:</strong> Group Recommender Systems, Fairness in Recommendations, Large-scale Groups, Long-term Effects, Societal Impact, Scalability, Real-World Applications, Personalization, Diversity of Recommendations, Beyond Accuracy (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/808/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Toward Fair Federated Recommendation Learning: Characterizing the Inter-Dependence of System and Data Heterogeneity (2022)</h3>
    <p><strong>Authors:</strong> John Nguyen, Luca Melis, Kiwan Maeng, Carole-Jean Wu, Haiyu Lu, Mike Rabbat</p>
    <p>Federated learning (FL) is an effective mechanism for data privacy in recommender systems that runs machine learning model training on-device. While prior FL optimizations tackled the data and system heterogeneity challenges, they assume the two are independent of each other. This fundamental assumption is not reflective of real-world, large-scale recommender systems — data and system heterogeneity are tightly intertwined. This paper takes a data-driven approach to show the inter-dependence of data and system heterogeneity in real-world data and quantifies its impact on the overall model quality and fairness. We design a framework, RF2, to model the inter-dependence and evaluate its impact on state-of-the-art model optimization techniques for federated recommendation tasks. We demonstrate that the impact on fairness can be severe under realistic heterogeneity scenarios, by up to 15.8–41 × compared to a simple setup assumed in most (if not all) prior work. The result shows that modeling realistic system-induced data heterogeneity is essential to achieving fair federated recommendation learning.</p>
    <p><strong>Categories:</strong> Federated Learning, Recommender Systems, Fairness in Recommendations, Data Heterogeneity, System Heterogeneity, Inter-Domain Interactions, Model Quality Evaluation, Real-World Applications, Privacy in Machine Learning, Algorithm Design and Optimization (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/779/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Adversary or Friend? An adversarial Approach to Improving Recommender Systems (2022)</h3>
    <p><strong>Authors:</strong> Pannaga Shivaswamy, Dario Garcia Garcia</p>
    <p>Typical recommender systems models are trained to have good average performance across all users or items. In practice, this results in model performance that is good for some users but sub-optimal for many users. In this work, we consider adversarially trained machine learning models and extend them to recommender systems problems. The adversarial models are trained with no additional demographic or other information than already available to the learning algorithm. We show that adversarially reweighted learning models give more emphasis to dense areas of the feature-space that incur high loss during training. We show that a straightforward adversarial model adapted to recommender systems can fail to perform well and that a carefully designed adversarial model can perform much better. The proposed models are trained using a standard gradient descent/ascent approach that can be easily adapted to many recommender problems. We compare our results with an inverse propensity weighting based baseline that also works well in practice. We delve deep into the underlying experimental results and show that, for the users who are under-served by the baseline model, the adversarial models can achieve significantly better results.</p>
    <p><strong>Categories:</strong> Adversarial Training, Recommender Systems, Machine Learning, Recommendation Algorithms, User-Specific Recommendations, Improving Recommendation Accuracy, Fairness in AI, Bias Mitigation, Beyond Accuracy, Algorithmic Fairness, Optimization Techniques, Fairness in Recommendations, Bias in AI, Algorithmic Bias (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/748/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Elsevier, Fairness in Recommendations (2021)</h3>
    <p><strong>Authors:</strong> Daniel Kershaw</p>
    <p>At Elsevier we aim to help scientists further their research. On the one hand, by offering a platform for publishing ground-breaking research, and on the other by helping researchers discover relevant content to assist their work. In our team, Editorial Data Science, we support the former goal by providing tools to help editors in the decisions that they make, from finding reviewers to recommending transfers for manuscripts to more appropriate journals. To improve the workflows for publishing research and help editors in finding relevant reviewers we developed a reviewer recommender. The reviewer recommender will, based on a submitted manuscript, recommend reviewers that optimise the fit between the manuscript, the journal, and the reviewer themselves. To achieve this, the reviewer recommender is built on top of the three principles of quality data, expert knowledge, and experimentation. We leverage data from across Elsevier, ranging from Scopus profiles which includes publication histories and academic impact data, to data from our publishing platform which includes historic invites and subsequent actions around submitted manuscripts. All this data is used to develop and evaluate appropriate models for recommending reviewers. There are many factors to consider when deciding if someone is relevant to review a submitted manuscript. To name a few, the topic of interest of the reviewer, level of expertise of the reviewer, fit of the reviewer to the journal. But we are ultimately looking for the fit between the three stake holders: the manuscript, the journal, and the reviewer. However, it is not as simple as looking for a fit purely on topic or relevance. We can find evidence for that in the growth of research into recommender systems for multi-sided market [ 7] places where all sides within the systems must be considered. In our case, in addition to topical fit, we must consider fairness in recommendations, where minority groups get equal and fair treatment. For this reason, we consider two ’additional’ constraints when developing recommender systems at Elsevier, fairness in the recommendations and respecting the reviewers time and privacy. Fairness in Recommendations is important in recommending people particular within academic fields where there are known biases against minority groups. This bias is present in data which we use. [REF] showed that women and minorities over their academic and research careers are disadvantaged compared to members of the majority groups, either through less research output due to more teaching [ 6] or taking time out to raise families which resulted in lower number of research output [8]. Though bias is not only limited to quantities of research output [ 4 ] showed that women receive fewer citations even when controlling to quantity of research output and gender. This effect of receiving proportionally fewer citation is also amplified [ 5] identified that women are disproportionally ranked lower on the author list of a paper. These observations are not limited to gender but also race and age. Using these features derived from bibliometric analysis without due care within a model could lead to adverse impact against minority groups, resulting in the reviewers from these groups being recommended less and thus less invites to review manuscripts, whilst making sure any interventions does not create additional harms. Respecting user privacy and time. We know through user research that there is also the need to respect the researcher whom we are asking to review these manuscripts. Either because they are too busy to accept another review, or that they feel the manuscript is out of their scope and they are invited to review irrelevant papers. In this talk we show how at Elsevier we build fairness and understanding of stake holders into our recommenders, particularly the reviewer recommender. We experimented with a variety of technologies, ranging from simple search to sophisticated ML models. They all follow the common IR approach of candidate selection followed by re-ranking. All approaches use roughly the same features representing the reviewer (topics they have published in), the journal (topics the journal publishes and metrics about the reviewers they have selected before), the manuscript (key concepts within the manuscript). Through iterating over different models and techniques, we understand the importance of different features, along with their impact on model performance. But more specifically which features have inherent bias built into them, which feature are most predictive , and which fairness aware methods have been used to mitigate these biases [1, 2]. We will then discuss the methods and challenges of including bias metrics in the evaluation and development of the reviewer recommender. In this we focus on how we evaluate fairness from historical data along with group-wise fairness metrics such as demographic parity and equal opportunity. We then discuss the challenges and successes of monitoring these metrics in production [3]. This combination of feature understanding and fairness evaluation at every stage in the model development means we have a deep understanding of how the model works, who it impacts, and how. Resulting in a fair model which keeps the reviewers at the heart of the review process. However, we must not forget, this work sits within the wider academic review system, where bias is introduced on many levels through hiring, promotions and research engagement. Even though a reviewer recommender can not correct the underlying bias, it can contribute to a fairer and more equal outcome.</p>
    <p><strong>Categories:</strong> Fairness in Recommendations, Academic Publishing, Reviewer Recommenders, Machine Learning Models, Information Retrieval, Bias Mitigation, Gender Bias, Minority Groups, Group-wise Fairness Metrics, Demographic Parity, Equal Opportunity, Systemic Bias, Feature Analysis, Model Transparency, Reviewer Privacy, Time Management (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/720/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Top-K Contextual Bandits with Equity of Exposure (2021)</h3>
    <p><strong>Authors:</strong> Olivier Jeunen, Bart Goethals</p>
    <p>The contextual bandit paradigm provides a general framework for decision-making under uncertainty. It is theoretically well-defined and well-studied, and many personalisation use-cases can be cast as a bandit learning problem. Because this allows for the direct optimisation of utility metrics that rely on online interventions (such as click-through-rate (CTR)), this framework has become an attractive choice to practitioners. Historically, the literature on this topic has focused on a one-sided, user-focused notion of utility, overall disregarding the perspective of content providers in online marketplaces (for example, musical artists on streaming services). If not properly taken into account – recommendation systems in such environments are known to lead to unfair distributions of attention and exposure, which can directly affect the income of the providers. Recent work has shed a light on this, and there is now a growing consensus that some notion of “equity of exposure” might be preferable to implement in many recommendation use-cases.<br>We study how the top-K contextual bandit problem relates to issues of disparate exposure, and how this disparity can be minimised. The predominant approach in practice is to greedily rank the top-K items according to their estimated utility, as this is optimal according to the well-known Probability Ranking Principle. Instead, we introduce a configurable tolerance parameter that defines an acceptable decrease in utility for a maximal increase in fairness of exposure. We propose a personalised exposure-aware arm selection algorithm that handles this relevance-fairness trade-off on a user-level, as recent work suggests that users’ openness to randomisation may vary greatly over the global populace. Our model-agnostic algorithm deals with arm selection instead of utility modelling, and can therefore be implemented on top of any existing bandit system with minimal changes. We conclude with a case study on carousel personalisation in music recommendation: empirical observations highlight the effectiveness of our proposed method and show that exposure disparity can be significantly reduced with a negligible impact on user utility.</p>
    <p><strong>Categories:</strong> Contextual Bandits, Equity of Exposure, Personalization, Fairness in Recommendations, Top-K Selection, Music Recommendation, Algorithm Design, User-Centered Design, Relevance-Fairness Trade-off, Evaluation Methods, Exposure Disparity, Machine Learning, Bandit Algorithms, Case Study. (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/670/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>The Connection Between Popularity Bias, Calibration, and Fairness in Recommendation (2020)</h3>
    <p><strong>Authors:</strong> Bamshad Mobasher, Robin Burke, Himan Abdollahpouri, Masoud Mansoury</p>
    <p>Recently there has been a growing interest in fairness-aware recommender systems including fairness in providing consistent performance across different users or groups of users. A recommender system could be considered unfair if the recommendations do not fairly represent the tastes of a certain group of users while other groups receive recommendations that are consistent with their preferences. In this paper, we use a metric called miscalibration for measuring how a recommendation algorithm is responsive to users’ true preferences and we consider how various algorithms may result in different degrees of miscalibration for different users. In particular, we conjecture that popularity bias which is a well-known phenomenon in recommendation is one important factor leading to miscalibration in recommendation. Our experimental results using two real-world datasets show that there is a connection between how different user groups are affected by algorithmic popularity bias and their level of interest in popular items. Moreover, we show that the more a group is affected by the algorithmic popularity bias, the more their recommendations are miscalibrated.</p>
    <p><strong>Categories:</strong> Fairness in Recommendations, Popularity Bias, Miscalibration, Real-World Applications, Algorithm Evaluation, User Group Analysis, Recommendation Fairness, Consistency in Recommendations, Algorithmic Impact, Recommendation Dynamics (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/611/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Personalized Fairness-aware Re-ranking for Microlending (2019)</h3>
    <p><strong>Authors:</strong> Robin Burke, Shengyu Zhang, Weiwen Liu, Nasim Sonboli, Jun Guo</p>
    <p>Microlending can lead to improved access to capital in impoverished countries. Recommender systems could be used in microlending to provide efficient and personalized service to lenders. However, increasing concerns about discrimination in machine learning hinder the application of recommender systems to the microfinance industry. Most previous recommender systems focus on pure personalization, with fairness issue largely ignored. A desirable fairness property in microlending is to give borrowers from different demographic groups a fair chance of being recommended, as stated by Kiva. To achieve this goal, we propose a Fairness-Aware Re-ranking (FAR) algorithm to balance ranking quality and borrower-side fairness. Furthermore, we take into consideration that lenders may differ in their receptivity to the diversification of recommended loans, and develop a Personalized Fairness-Aware Re-ranking (PFAR) algorithm. Experiments on a real-world dataset from Kiva.org show that our re-ranking algorithm can significantly promote fairness with little sacrifice in accuracy, and be attentive to individual lender preference on loan diversity. i>Presentation: Tuesday Poster Lunch</i</p>
    <p><strong>Categories:</strong> Fairness in Recommendations, Re-ranking, Microlending, Finance/Microfinance, Personalized Fairness-Aware Re-Ranking (PFAR), Social Impact, Bias Mitigation, Diversity of Recommendations, Beyond Accuracy, Evaluation Metrics, Real-World Applications, Algorithmic Fairness, Recommender Systems (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/491/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>