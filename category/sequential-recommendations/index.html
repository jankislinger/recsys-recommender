<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/evaluation-metrics/">Evaluation Metrics</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/user-behavior-analysis/">User Behavior Analysis</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/scalability/">Scalability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/popularity-bias/">Popularity Bias</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/recommendation-quality/">Recommendation Quality</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/evaluation-methods/">Evaluation Methods</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Scalable Cross-Entropy Loss for Sequential Recommendations with Large Item Catalogs (2024)</h3>
    <p><strong>Authors:</strong> Gleb Mezentsev, Danil Gusak, Ivan Oseledets, Evgeny Frolov</p>
    <p>Scalability issue plays a crucial role in productionizing modern recommender systems. Even lightweight architectures may suffer from high computational overload due to intermediate calculations, limiting their practicality in real-world applications. Specifically, applying full Cross-Entropy (CE) loss often yields state-of-the-art performance in terms of recommendations quality. Still, it suffers from excessive GPU memory utilization when dealing with large item catalogs. This paper introduces a novel Scalable Cross-Entropy (SCE) loss function in the sequential learning setup. It approximates the CE loss for datasets with large-size catalogs, enhancing both time efficiency and memory usage without compromising recommendations quality. Unlike traditional negative sampling methods, our approach utilizes a selective GPU-efficient computation strategy, focusing on the most informative elements of the catalog, particularly those most likely to be false positives. This is achieved by approximating the softmax distribution over a subset of the model outputs through the maximum inner product search. Experimental results on multiple datasets demonstrate the effectiveness of SCE in reducing peak memory usage by a factor of up to 100 compared to the alternatives, retaining or even exceeding their metrics values. The proposed approach also opens new perspectives for large-scale developments in different domains, such as large language models.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Loss Functions, Scalability, Sequential Recommendations, GPU Memory Optimization, Large Item Catalogs, Efficiency and Performance, False Positives Handling, Max Inner Product Search (MIPS), Large Language Models (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1059/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>TLRec: A Transfer Learning Framework to Enhance Large Language Models for Sequential Recommendation Tasks (2024)</h3>
    <p><strong>Authors:</strong> Shuang Peng, Jiaye Lin, Zhong Zhang, Peilin Zhao</p>
    <p>Recently, Large Language Models (LLMs) have garnered significant attention in recommendation systems, improving recommendation performance through in-context learning or parameter-efficient fine-tuning. However, cross-domain generalization, i.e., model training in one scenario (source domain) but inference in another (target domain), is underexplored. In this paper, we present TLRec, a transfer learning framework aimed at enhancing LLMs for sequential recommendation tasks. TLRec specifically focuses on text inputs to mitigate the challenge of limited transferability across diverse domains, offering promising advantages over traditional recommendation models that heavily depend on unique identities (IDs) like user IDs and item IDs. Moreover, we leverage the source domain data to further enhance LLMs’ performance in the target domain. Initially, we employ powerful closed-source LLMs (e.g., GPT-4) and chain-of-thought techniques to construct instruction tuning data from the third-party scenario (source domain). Subsequently, we apply curriculum learning to fine-tune LLMs for effective knowledge injection and perform recommendations in the target domain. Experimental results demonstrate that TLRec achieves superior performance under the zero-shot and few-shot settings.</p>
    <p><strong>Categories:</strong> Transfer Learning, Large Language Models, Recommendation Systems, Cross-Domain Recommendations, Instruction Tuning, Curriculum Learning, Fine-Tuning, Domain Adaptation, Zero-Shot Learning, Few-Shot Learning, Text-Based Recommendations, Sequential Recommendations, Chain of Thought (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1203/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Does It Look Sequential? An Analysis of Datasets for Evaluation of Sequential Recommendations. (2024)</h3>
    <p><strong>Authors:</strong> Anna Volodkevich, Alexey Vasilev, Anton Klenitskiy, Anton Pembek</p>
    <p>Sequential recommender systems are an important and demanded area of research. Such systems aim to use the order of interactions in a user’s history to predict future interactions. The premise is that the order of interactions and sequential patterns play an important role. Therefore, it is crucial to use datasets that exhibit a sequential structure for a proper evaluation of sequential recommenders. We apply several methods based on the random shuffling of the user’s sequence of interactions to assess the strength of sequential structure across 15 datasets, frequently used for sequential recommender systems evaluation in recent research papers presented at top-tier conferences. As shuffling explicitly breaks sequential dependencies inherent in datasets, we estimate the strength of sequential patterns by comparing metrics for shuffled and original versions of the dataset. Our findings show that several popular datasets have a rather weak sequential structure.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Sequential Recommendations, Dataset Evaluation, Methodology Analysis, Strength of Sequential Patterns, Importance of Proper Evaluation, Research Practices, Challenges in Evaluation (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1087/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Enhancing Sequential Music Recommendation with Negative Feedback-informed Contrastive Learning (2024)</h3>
    <p><strong>Authors:</strong> Shahrzad Shashaani, Pavan Seshadri, Peter Knees</p>
    <p>Modern music streaming services are heavily based on recommen- dation engines to serve continuous content to users. Sequential recommendation—continuously providing new items within a sin- gle session in a contextually coherent manner—has been an emerg- ing topic in current literature. User feedback—a positive or negative response to the item presented—is used to drive content recom- mendations by learning user preferences. We extend this idea to the session-based recommendation domain to improve learning of context-coherent music recommendations by modelling negative user feedback, i.e., skips, in the loss function. To this end, we propose a sequence-aware contrastive sub-task to structure item embeddings in session-based music recommen- dation, such that true next-positive items (ignoring skipped items) are structured closer in the embedding space, while skipped tracks are structured farther away from all items in the session. Since this causes skipped item embeddings in a session to be farther than unskipped items in the learned space, this directly affects item rankings using a K-nearest-neighbors search for next-item recom- mendations, while also promoting the rank of the true next item. Experiments incorporating this task into SoTA methods for sequen- tial item recommendation show consistent performance gains in terms of next-item hit rate, item ranking, and skip down-ranking on three music recommendation datasets, strongly benefiting from increasing presence of user feedback.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Music Recommendation, Sequential Recommendations, Negative Feedback, Contrastive Learning, User Feedback, Session-Based Recommendation, Evaluation Metrics, Embeddings, Music Streaming Services (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1088/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Pay Attention to Attention for Sequential Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Xiaojing Liu, Yuli Liu, Min Liu</p>
    <p>Transformer-based approaches have demonstrated remarkable success in various sequence-based tasks. However, traditional self-attention models may not sufficiently capture the intricate dependencies within items in sequential recommendation scenarios. This is due to the lack of explicit emphasis on attention weights, which play a critical role in allocating attention and understanding item-to-item correlations. To better exploit the potential of attention weights and improve the capability of sequential recommendation in learning high-order dependencies, we propose a novel sequential recommendation (SR) approach called attention weight refinement (AWRSR). AWRSR enhances the effectiveness of self-attention by additionally paying attention to attention weights, allowing for more refined attention distributions of correlations among items. We conduct comprehensive experiments on multiple real-world datasets, demonstrating that our approach consistently outperforms state-of-the-art SR models. Moreover, we provide a thorough analysis of AWRSR’s effectiveness in capturing higher-level dependencies. These findings suggest that AWRSR offers a promising new direction for enhancing the performance of self-attention architecture in SR tasks, with potential applications in other sequence-based problems as well.</p>
    <p><strong>Categories:</strong> Sequential Recommendations, Attention Mechanisms, Transformer-Based Models, Recommendation Systems, Higher-Order Dependencies, Model Performance, Experimental Analysis, Item Correlations, Real-World Applications, Novel Methods, Attention Weight Refinement, Self-Attention Architecture, Machine Learning for Recommendations, Potential Applications in Other Domains (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1104/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Scaling Law of Large Sequential Recommendation Models (2024)</h3>
    <p><strong>Authors:</strong> Hongyu Lu, Yu Chen, Wayne Xin Zhao, Gaowei Zhang, Yupeng Hou, Ji-Rong Wen</p>
    <p>Scaling of neural networks has recently shown great potential to improve the model capacity in various fields. Specifically, model performance has a power-law relationship with model size or data size, which provides important guidance for the development of large-scale models. However, there is still limited understanding on the scaling effect of user behavior models in recommender systems, where the unique data characteristics (e.g., data scarcity and sparsity) pose new challenges in recommendation tasks. In this work, we focus on investigating the scaling laws in large sequential recommendation models. Specifically, we consider a pure ID-based task formulation, where the interaction history of a user is formatted as a chronological sequence of item IDs. We don’t incorporate any side information (e.g., item text), to delve into the scaling law’s applicability from the perspective of user behavior. We successfully scale up the model size to 0.8B parameters, making it feasible to explore the scaling effect in a diverse range of model sizes. As the major findings, we empirically show that the scaling law still holds for these trained models, even in data-constrained scenarios. We then fit the curve for scaling law, and successfully predict the test loss of the two largest tested model scales. Furthermore, we examine the performance advantage of scaling effect on five challenging recommendation tasks, considering the unique issues (e.g., cold start, robustness, long-term preference) in recommender systems. We find that scaling up the model size can greatly boost the performance on these challenging tasks, which again verifies the benefits of large recommendation models.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Scaling Laws, Sequential Recommendations, Model Scalability, Large-Scale Models, User Behavior Modeling, Cold Start Problem, Evaluation Metrics, Data Sparsity, Model Capacity (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1065/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>User knowledge prompt for sequential recommendation (2024)</h3>
    <p><strong>Authors:</strong> Yuuki Tachioka</p>
    <p>The large language model (LLM) based recommendation system is effective for sequential recommendation, because general knowledge of popular items is included in the LLM. To add domain knowledge of items, the conventional method uses a knowledge prompt obtained from the item knowledge graphs and has achieved SOTA performance. However, for personalized recommendation, it is necessary to consider user knowledge, which the conventional method does not fully consider because user knowledge is not included in the item knowledge graphs; thus, we propose a user knowledge prompt, which converts a user knowledge graph into a prompt using the relationship template. The existing prompt denoising framework is extended to prevent hallucination caused by undesirable interactions between knowledge graph prompts. We propose user knowledge prompts of user traits and user preferences and associate relevant items. Experiments on three types of dataset (movie, music, and book) show the significant and consistent improvement of our proposed user knowledge prompt.</p>
    <p><strong>Categories:</strong> Large Language Models (LLMs), Knowledge Graphs, Sequential Recommendations, Personalized Recommendation, Prompt Engineering, Movies, Music, Books, User Modeling, Empirical Evaluation, Prompt Denoising (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1198/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Self-Attentive Sequential Recommendations with Hyperbolic Representations (2024)</h3>
    <p><strong>Authors:</strong> Tatyana Matveeva, Evgeny Frolov, Ivan Oseledets, Leyla Mirvakhabova</p>
    <p>In recent years, self-attentive sequential learning models have surpassed conventional collaborative filtering techniques in next-item recommendation tasks. However, Euclidean geometry utilized in these models may not be optimal for capturing a complex structure of behavioral data. Building on recent advances in the application of hyperbolic geometry to collaborative filtering tasks, we propose a novel approach that leverages hyperbolic geometry in the sequential learning setting. Our approach replaces final output of the Euclidean models with a linear predictor in the non-linear hyperbolic space, which increases the representational capacity and improves recommendation quality.</p>
    <p><strong>Categories:</strong> Self-Attention, Transformer-Based Models, Hyperbolic Geometry, Sequential Recommendations, Recommendation Systems, Model Architecture, Representation Learning, Geometric Deep Learning, Output Layer Design, Recommendation Quality (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1113/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Transformers Meet ACT-R: Repeat-Aware and Sequential Listening Session Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Bruno Sguerra, Viet-Anh Tran, Romain Hennequin, Guillaume Salha-Galvan</p>
    <p>Music streaming services often leverage sequential recommender systems to predict the best music to showcase to users based on past sequences of listening sessions. Nonetheless, most sequential recommendation methods ignore or insufficiently account for repetitive behaviors. This is a crucial limitation for music recommendation, as repeatedly listening to the same song over time is a common phenomenon, that can even change the way users perceive this song. In this paper, we introduce PISA (Psychology-Informed Session embedding using ACT-R), a session-level sequential recommender system that overcomes this limitation. PISA employs a Transformer architecture learning embedding representations of listening sessions and users using attention mechanisms inspired by Anderson’s ACT-R (Adaptive Control of Though-Rational), a cognitive architecture modeling human information access and memory dynamics. This approach enables us to capture dynamic and repetitive patterns from user behaviors, allowing us to effectively predict the songs they will listen to in subsequent sessions, whether they are repeated or new ones. We demonstrate the empirical relevance of PISA using public listening data from Last.fm and proprietary data from a global music streaming service, thereby confirming the critical importance of repetition modeling for sequential listening session recommendation. Along with this paper, we publicly release our proprietary dataset to foster future research in this field, as well as the source code of PISA to facilitate its future use.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Music Recommendation, Sequential Recommendations, Transformer Architecture, User Behavior Analysis, Repetitive Behaviors, Empirical Evaluation, Real World Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1067/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Integrating the ACT-R Framework with Collaborative Filtering for Explainable Sequential Music Recommendation (2023)</h3>
    <p><strong>Authors:</strong> Christian Wallmann, Markus Schedl, Elisabeth Lex, Dominik Kowald, Marta Moscati, Markus Reiter-Haas</p>
    <p>Music listening sessions often consist of sequences including repeating tracks. Modeling such relistening behavior with models of human memory has been proven effective in predicting the next track of a session. However, these models intrinsically lack the capability of recommending novel tracks that the target user has not listened to in the past. Collaborative filtering strategies, on the contrary, provide novel recommendations by leveraging past collective behaviors but are often limited in their ability to provide explanations. To narrow this gap, we propose four hybrid algorithms that integrate collaborative filtering with the cognitive architecture ACT-R. We compare their performance in terms of accuracy, novelty, diversity, and popularity bias, to baselines of different types, including pure ACT-R, kNN-based, and neural-networks-based approaches. We show that the proposed algorithms are able to achieve the best performances in terms of novelty and diversity, and simultaneously achieve a higher accuracy of recommendation with respect to pure ACT-R models. Furthermore, we illustrate how the proposed models can provide explainable recommendations.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Music, ACT-R Framework, Collaborative Filtering, Explainability, Accuracy, Novelty, Diversity, Popularity Bias, kNN-Based Algorithms, Neural Networks, Sequential Recommendations, Real-World Applications, Hybrid Methods, Cold Start (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/919/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Turning Dross Into Gold Loss: is BERT4Rec really better than SASRec? (2023)</h3>
    <p><strong>Authors:</strong> Alexey Vasilev, Anton Klenitskiy</p>
    <p>Recently sequential recommendations and next-item prediction task has become increasingly popular in the field of recommender systems. Currently, two state-of-the-art baselines are Transformer-based models SASRec and BERT4Rec. Over the past few years, there have been quite a few publications comparing these two algorithms and proposing new state-of-the-art models. In most of the publications, BERT4Rec achieves better performance than SASRec. But BERT4Rec uses cross-entropy over softmax for all items, while SASRec uses negative sampling and calculates binary cross-entropy loss for one positive and one negative item. In our work, we show that if both models are trained with the same loss, which is used by BERT4Rec, then SASRec will significantly outperform BERT4Rec both in terms of quality and training speed. In addition, we show that SASRec could be effectively trained with negative sampling and still outperform BERT4Rec, but the number of negative examples should be much larger than one.</p>
    <p><strong>Categories:</strong> Sequential Recommendations, Transformer-Based Models, BERT4Rec, SASRec, Algorithm Comparison, Loss Functions, Evaluation Metrics, State-of-the-Art Models, Training Efficiency, Model Performance, Recommender Systems, Negative Sampling, Cross-Entropy Loss, Binary Cross-Entropy Loss (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/966/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Incorporating Time in Sequential Recommendation Models (2023)</h3>
    <p><strong>Authors:</strong> James Caverlee, Mostafa Rahmani, Fei Wang</p>
    <p>Sequential models are designed to learn sequential patterns in data based on the chronological order of user interactions. However, they often ignore the timestamps of these interactions. Incorporating time is crucial because many sequential patterns are time-dependent, and the model cannot make time-aware recommendations without considering time. This article demonstrates that providing a rich representation of time can significantly improve the performance of sequential models. The existing literature treats time as a one-dimensional time-series obtained by quantizing time. In this study, we propose treating time as a multi-dimensional time-series and explore representation learning methods, including  a kernel based method and an embedding-based algorithm. Experiments on multiple datasets show that the inclusion of time significantly enhances the model’s performance, and multi-dimensional methods outperform the one-dimensional method by a substantial margin.</p>
    <p><strong>Categories:</strong> Sequential Models, Time Series, Kernel-based Methods, Embedding-based Algorithms, Sequential Recommendations, Temporal Dynamics, Recommendation Systems, Model Enhancement, Representation Learning, Performance Analysis, Methodological Innovations, Applied Research, User Interaction Patterns, Multi-dimensional Time Series, Data Analysis Techniques, Experimental Evaluation, Scalability, Cold Start, Diversity of Recommendations, Beyond Accuracy (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/907/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Beyond the Sequence: Statistics-driven Pre-training for Stabilizing Sequential Recommendation Model (2023)</h3>
    <p><strong>Authors:</strong> Hongzhi Zhang, Sirui Wang, Yunsen Xian, Peiguang Li</p>
    <p>The sequential recommendation task aims to predict the item that user is interested in according to his/her historical action sequence. However, inevitable random action, i.e. user randomly accesses an item among multiple candidates or clicks several items at random order, cause the sequence fails to provide stable and high-quality signals. To alleviate the issue, we propose the StatisTics-Driven Pre-traing framework (called STDP briefly). The main idea of the work lies in the exploration of utilizing the statistics information along with the pre-training paradigm to stabilize the optimization of recommendation model. Specifically, we derive two types of statistical information: item co-occurrence across sequence and attribute frequency within the sequence. And we design the following pre-training tasks: 1) The co-occurred items prediction task, which encourages the model to distribute its attention on multiple suitable targets instead of just focusing on the next item that may be unstable. 2) We generate a paired sequence by replacing items with their co-occurred items and enforce its representation close with the original one, thus enhancing the model’s robustness to the random noise. 3) To reduce the impact of random on user’s long-term preferences, we encourage the model to capture sequence-level frequent attributes. The significant improvement over six datasets demonstrates the effectiveness and superiority of the proposal, and further analysis verified the generalization of the STDP framework on other models.</p>
    <p><strong>Categories:</strong> Sequential Recommendations, Pre-Training Methods, Recommendation Quality, User Behavior Analysis, Co-Occurrence Analysis, Random Action Mitigation, Statistical Information Utilization, Model Robustness, Attribute Frequency Analysis, Machine Learning, Recommender Systems, User Preferences (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/899/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>A Multi-view Graph Contrastive Learning Framework for Cross-Domain Sequential Recommendation (2023)</h3>
    <p><strong>Authors:</strong> Zitao Xu, Weike Pan, Zhong Ming</p>
    <p>Sequential recommendation methods play an irreplaceable role in recommender systems which can capture the users’ dynamic preferences from the behavior sequences. Despite their success, these works usually suffer from the sparsity problem commonly existed in real applications. Cross-domain sequential recommendation aims to alleviate this problem by introducing relatively richer source-domain data. However, most existing methods capture the users’ preferences independently of each domain, which may neglect the item transition patterns across sequences from different domains, i.e., a user’s interaction in one domain may influence his/her next interaction in other domains. Moreover, the data sparsity problem still exists since some items in the target and source domains are interacted with only a limited number of times. To address these issues, in this paper we propose a generic framework named multi-view graph contrastive learning (MGCL). Specifically, we adopt the contrastive mechanism in an intra-domain item representation view and an inter-domain user preference view. The former is to jointly learn the dynamic sequential information in the user sequence graph and the static collaborative information in the cross-domain global graph, while the latter is to capture the complementary information of the user’s preferences from different domains. Extensive empirical studies on three real-world datasets demonstrate that our MGCL significantly outperforms the state-of-the-art methods.</p>
    <p><strong>Categories:</strong> Graph-Based Methods, Sequential Recommendations, Cross-Domain, Contrastive Learning, Multi-View Learning, User Behavior Modeling, Data Sparsity, Collaborative Filtering (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/853/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>STRec: Sparse Transformer for Sequential Recommendations (2023)</h3>
    <p><strong>Authors:</strong> Lixin Zou, Yiqi Wang, Qidong Liu, Wanyu Wang, Wenqi Fan, Yejing Wang, Chengxi Li, Qing Li, Xiangyu Zhao</p>
    <p>With the rapid evolution of transformer architectures, an increasing number of researchers are exploring their application in sequential recommender systems (SRSs). Compared with the former SRS models, the transformer-based models get promising performance on SRS tasks. Existing transformer-based SRS frameworks, however, retain the vanilla attention mechanism, which calculates the attention scores between all item-item pairs in each layer, i.e., item interactions. Consequently, redundant item interactions may downgrade the inference speed and cause high memory costs for the model. In this paper, we first identify the sparse information phenomenon in transformer-based SRS scenarios and propose an efficient model, i.e., Sparse Transformer sequential Recommendation model (STRec). First, we devise a cross-attention-based sparse transformer for efficient sequential recommendation. Then, a novel sampling strategy is derived to  preserve the necessary interactions. Extensive experimental results validate the effectiveness of our framework, which could outperform the state-of-the-art accuracy while reducing 54% inference time and 70% memory cost. Besides, we provide massive extended experiments to further investigate the property of our framework. Our code is available to ease reproducibility.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Sequential Recommendations, Transformer Models, Sparse Attention Mechanism, Efficiency Optimization, Inference Speed, Memory Management, Experimental Results, Model Design, Attention Mechanisms, Evaluation Metrics, Real-World Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/879/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Effective and Efficient Training for Sequential Recommendation using Recency Sampling (2022)</h3>
    <p><strong>Authors:</strong> Aleksandr Petrov, Craig Macdonald</p>
    <p>Many modern sequential recommender systems use deep neural networks, which can effectively estimate the relevance of items but require a lot of time to train. Slow training increases expenses, hinders product development timescales and prevents the model from being regularly updated to adapt to changing user preferences. Training such sequential models involves appropriately sampling past user interactions to create a realistic training objective. The existing training objectives have limitations. For instance, next item prediction never uses the beginning of the sequence as a learning target, thereby potentially discarding valuable data. On the other hand, the item masking used by BERT4Rec is only weakly related to the goal of the sequential recommendation; therefore, it requires much more time to obtain an effective model. Hence, we propose a novel Recency-based Sampling of Sequences training objective that addresses both limitations. We apply our method to various recent and state-of-the-art model architectures – such as GRU4Rec, Caser, and SASRec. We show that the models enhanced with our method can achieve performances exceeding or very close to state-of-the-art BERT4Rec, but with much less training time.</p>
    <p><strong>Categories:</strong> Sequential Recommendations, Training Techniques, Recency Sampling, Model Architecture, Efficiency, Scalability, Algorithm Improvements, Optimization, Deep Learning, Real-World Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/764/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>