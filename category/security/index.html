<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/bias-mitigation/">Bias Mitigation</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/real-world-applications/">Real-world Applications</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/offline-evaluation/">Offline Evaluation</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Improving the Shortest Plank: Vulnerability-Aware Adversarial Training for Robust Recommender System (2024)</h3>
    <p><strong>Authors:</strong> Fei Sun, Qi Cao, Yunfan Wu, Huawei Shen, Xueqi Cheng, Kaike Zhang</p>
    <p>Recommender systems play a pivotal role in mitigating information overload in diverse fields. Nonetheless, the inherent openness of these systems introduces vulnerabilities, allowing attackers to insert fake users to skew the exposure of certain items, known as poisoning attacks. Adversarial training emerges as a notable defense mechanism against such poisoning attacks within recommender systems. Traditional adversarial training methods apply perturbations with the same scale across all users to their embeddings to maintain system robustness against the worst-case attacks. Yet, in reality, attacks often affect only a subset of users who are actually vulnerable to the specific attacks. These indiscriminate perturbations make it difficult to balance effective protection for vulnerable users and avoidance of recommendation quality degradation for those who are not. To address this issue, our research delves into understanding user vulnerability. Considering that poisoning attacks pollutes the training data, we observe that the extent of a recommender system’s fit to users’ training data, particularly when high, correlates with an increased likelihood of users incorporating attack information, thus indicating their vulnerability. Leveraging these insights, we introduce the Vulnerability-aware Adversarial Training (VAT) method, designed to counteract poisoning attacks in recommender systems. VAT employs a novel vulnerability-aware function to estimate users’ vulnerability based on the degree to which they are fitted by the system. Guided by this evaluation, VAT applies user-specific perturbations to embeddings. thereby not only reducing the success rate of attacks but also preserving—and potentially enhancing—the quality of recommendations. Comprehensive experiments confirm VAT’s superior defensive capabilities against various attacks and recommendation models.</p>
    <p><strong>Categories:</strong> Adversarial Training, Poisoning Attacks, Robustness, Security, User Vulnerability Analysis, Defense Mechanisms, Recommender Systems, Machine Learning, Adversarial Machine Learning, Security in Recommender Systems (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1045/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>RecAD: Towards A Unified Library for Recommender Attack and Defense (2023)</h3>
    <p><strong>Authors:</strong> Chongming Gao, Wenjie Wang, Jianbai Ye, Xiangnan He, Changsheng Wang, Fuli Feng</p>
    <p>In recent years, recommender systems have become a ubiquitous part of our daily lives, while they suffer from a high risk of being attacked due to the growing commercial and social values. Despite significant research progress in recommender attack and defense, there is a lack of a widely-recognized benchmarking standard in the field, leading to unfair performance comparison and limited credibility of experiments. To address this, we propose RecAD, a unified library aiming at establishing an open benchmark for recommender attack and defense. RecAD takes an initial step to set up a unified benchmarking pipeline for reproducible research by integrating diverse datasets, standard source codes, hyper-parameter settings, running logs, attack knowledge, attack budget, and evaluation results. The benchmark is designed to be comprehensive and sustainable, covering both attack, defense, and evaluation tasks, enabling more researchers to easily follow and contribute to this promising field. RecAD will drive more solid and reproducible research on recommender systems attack and defense, reduce the redundant efforts of researchers, and ultimately increase the credibility and practical value of recommender attack and defense. The project and documents are released at https://github.com/gusye1234/recad.</p>
    <p><strong>Categories:</strong> Recommender Systems, Security, Attack, Defense, Benchmarking, Library, Reproducibility, Research Methodology, Open Source (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/941/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Exploring Unlearning Methods to Ensure the Privacy, Security, and Usability of Recommender Systems (2023)</h3>
    <p><strong>Authors:</strong> Jens Leysen</p>
    <p>Machine learning algorithms have proven highly effective in analyzing large amounts of data and identifying complex patterns and relationships. One application of machine learning that has received significant attention in recent years is recommender systems, which are algorithms that analyze user behavior and other data to suggest items or content that a user may be interested in. However useful, these systems may unintentionally retain sensitive, outdated, or faulty information. Posing a risk to user privacy, system security, and limiting a system’s usability. In this research proposal, we aim to address these challenges by investigating methods for machine “unlearning”, which would allow information to be efficiently “forgotten” or “unlearned” from machine learning models. The main objective of this proposal is to develop the foundation for future machine unlearning methods. We first evaluate current unlearning methods and explore novel adversarial attacks on these methods’ verifiability, efficiency, and accuracy to gain new insights and further develop the theory of machine unlearning. Using our gathered insights, we seek to create novel unlearning methods that are verifiable, efficient, and limit unnecessary accuracy degradation. Through this research, we seek to make significant contributions to the theoretical foundations of machine unlearning while also developing unlearning methods that can be applied to real-world problems.</p>
    <p><strong>Categories:</strong> Unlearning Methods, Privacy, Security, Recommender Systems, Machine Learning, Data Privacy, User Behavior, Adversarial Attacks, Theoretical Foundations, Applications, Efficiency (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/981/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Stability of Explainable Recommendation (2023)</h3>
    <p><strong>Authors:</strong> Sairamvinay Vijayaraghavan, Prasant Mohapatra</p>
    <p>Explainable Recommendation has been gaining attention over the last few years in industry and academia. Explanations provided along with recommendations for each user in a recommender system framework have many uses: particularly reasoning why a suggestion is provided and how well an item aligns with a user’s personalized preferences. Hence, explanations can play a huge role in influencing users to purchase products. However, the reliability of the explanations under varying scenarios has not been strictly verified in an empirical perspective. Unreliable explanations can bear strong consequences such as attackers leveraging explanations for manipulating and tempting users to purchase target items: that the attackers would want to promote. In this paper, we study the vulnerability of existent feature-oriented explainable recommenders, particularly analyzing their performance under different levels of external noises added into model parameters. We conducted experiments by analyzing three important state-of-the-art explainable recommenders when trained on two widely used e-commerce based recommendation datasets of different scales. We observe that all the explainable models are vulnerable to increased noise levels. Experimental results verify our hypothesis that the ability to explain recommendations does decrease along with increasing noise levels and particularly adversarial noise does contribute to a much stronger decrease. Our study presents an empirical verification on the topic of robust explanations in recommender systems which can be extended to different types of explainable recommenders in RS.</p>
    <p><strong>Categories:</strong> Explainability, Security, Adversarial Attacks, Robustness, Recommender Systems, Empirical Evaluation, E-commerce, Model Stability, Trustworthiness, Algorithmic Approaches, System Design, Noise Impact, Methodology (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/934/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Privacy Preserving Collaborative Filtering by Distributed Mediation (2021)</h3>
    <p><strong>Authors:</strong> Tamir Tassa, Alon Ben Horin</p>
    <p>Recommender systems have become very influential in our everyday decision making, e.g., helping us choose a movie from a content platform, or offering us suitable products on e-commerce websites. While most vendors who utilize recommender systems rely exclusively on training data consisting of past transactions that took place through them, the accuracy of recommendations can be improved if several vendors conjoin their datasets. Alas, such data sharing poses grave privacy concerns for both the vendors and the users. In this study we present secure multi-party protocols that enable several vendors to share their data, in a privacy-preserving manner, in order to allow more accurate Collaborative Filtering (CF). Shmueli and Tassa (RecSys 2017) introduced privacy-preserving CF protocols that rely on a mediator; namely, a third party that assists in performing the computations. They demonstrated the significant advantages of mediation in that context. We take here the mediation approach into the next level by using several independent mediators. Such distributed mediation maintains all of the advantages that were identified by Shmueli and Tassa, and offers additional ones, in comparison with the single-mediator protocols: stronger security and dramatically shorter runtimes. In addition, while all prior art assumed limited and unrealistic settings, in which each user can purchase any given item through only one vendor, we consider here a general and more realistic setting, which encompasses all previously considered settings, where users can choose between different competing vendors. We demonstrate the appealing performance of our protocols through extensive experimentation.</p>
    <p><strong>Categories:</strong> Privacy Preservation, Collaborative Filtering, Recommendation Systems, Multi-Vendor Recommendations, Secure Multi-party Computation, Distributed Mediation, Scalability, Security, Evaluation Metrics, Real-World Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/655/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Revisiting Adversarially Learned Injection Attacks Against Recommender Systems (2020)</h3>
    <p><strong>Authors:</strong> Ke Wang, Jiaxi Tang, Hongyi Wen</p>
    <p>Recommender systems play an important role in modern information and e-commerce applications. While increasing research is dedicated to improving the relevance and diversity of the recommendations, the potential risks of state-of-the-art recommendation models are under-explored, that is, these models could be subject to attacks from malicious third parties, through injecting fake user interactions to achieve their purposes. This paper revisits the adversarially-learned injection attack problem, where the injected fake user ‘behaviors’ are learned locally by the attackers with their own model – one that is potentially different from the model under attack, but shares similar properties to allow attack transfer. We found that most existing works in literature suffer from two major limitations: (1) they do not solve the optimization problem precisely, making the attack less harmful than it could be, (2) they assume perfect knowledge for the attack, causing the lack of understanding for realistic attack capabilities. We demonstrate that the exact solution for generating fake users as an optimization problem could lead to a much larger impact. Our experiments on a real-world dataset reveal important properties of the attack, including attack transferability and its limitations. These findings can inspire useful defensive methods against this possible existing attack.</p>
    <p><strong>Categories:</strong> Security, Recommender Systems, Adversarial Attacks, Injection Attacks, Fake User Interactions, Attack Transferability, Optimization for Attacks, Real-World Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/553/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Securing Tag-based recommender systems against profile injection attacks: A comparative study (2018)</h3>
    <p><strong>Authors:</strong> Heri Ramampiaro, Helge Langseth, Georgios Pitsilis</p>
    <p>This work addresses challenges related to attacks on social tagging systems, which often comes in a form of malicious annotations or profile injection attacks. In particular, we study various countermeasures against two types of threats for such systems, the Overload and the Piggyback attacks. The studied countermeasures include baseline classifiers such as, Naive Bayes filter and Support Vector Machine, as well as a deep learning-based approach. Our evaluation performed over synthetic spam data, generated from del.icio.us, shows that in most cases, the deep learning-based approach provides the best protection against threats.</p>
    <p><strong>Categories:</strong> Security, Recommender Systems, Profile Injection Attacks, Tag-based Recommendations, Deep Learning, Support Vector Machine, Naive Bayes, Anomaly Detection, Synthetic Data, Evaluation, Trust &amp; Privacy, Overload Attack, Piggyback Attack, Countermeasures (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/414/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>A Secure Shopping Experience Based on Blockchain and Beacon Technology (2016)</h3>
    <p><strong>Authors:</strong> Denis Vučkovac, Remo Manuel Frey, Alexander Ilic</p>
    <p>The present work proposes a novel approach for a future shopping system. Customers’ personal data are protected by a blockchain-based storage network. Based on the bitcoin protocol, the system transacts encrypted data in a tamper-proof way and is able to run secure multiparty computations while no one but the data owner has access to the input data. Thus, a potential customer is able to allow a company to apply functions like a recommendation algorithm without revealing personal data. In combination with a low-energy transmitter (beacon), a completely new shopping experience arises. The beacon automatically triggers a recommendation process based on encrypted personal data. The resulting outcome is a recommendation system, a self-checkout system, and a payment system all in one, thereby full anonymity is guaranteed and the customer never lose control on her data.</p>
    <p><strong>Categories:</strong> Blockchain, Security, Privacy, Recommendation Systems, Shopping/Commerce, Beacon Technology, Data Encryption, User Anonymity, Decentralized Systems, Real-World Applications, User Control Over Data, Tamper-Proof Systems (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/236/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Nuke `Em Till They Go: Investigating Power User Attacks to Disparage Items in Collaborative Recommenders (2015)</h3>
    <p><strong>Authors:</strong> David C. Wilson, Carlos E. Seminario</p>
    <p>Recommender Systems (RSs) can be vulnerable to manipulation by malicious users who successfully bias recommendations for their own benefit or pleasure. These are known as on RSs and are typically used to either promote (“push”) or disparage (“nuke”) targeted items contained within the recommender’s user-item dataset. Our recent work with the Power User Attack (PUA) attack model, determined that attackers disguised as can mount successful (from the attacker’s viewpoint) against user-based, item-based, and SVD-based recommenders. However, the success of push attack vectors may not be symmetric for , which target the opposite effect — the likelihood that target items appear in users’ top-N lists. The asymmetry between push and nuke attacks is highlighted when evaluating these attacks using traditional robustness metrics such as Rank and Prediction Shift. This paper examines the PUA attack model in the context of nuke attacks, in order to investigate the differences between push and nuke attack orientations, as well as how they are evaluated. In this work we show that the PUA is able to mount successful nuke attacks against commonly-used recommender algorithms highlighting the “nuke vs. push” asymmetry in the results.</p>
    <p><strong>Categories:</strong> Recommender Systems, Security, Power User Attack, User Attacks, Nuke Attacks, Push Attacks, Evaluation Metrics, Algorithm Vulnerability, Collaborative Filtering (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/124/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Attacking Item-Based Recommender Systems with Power Items (2014)</h3>
    <p><strong>Authors:</strong> David Wilson, Carlos Seminario</p>
    <p>Recommender Systems (RS) are vulnerable to attack by malicious users who intend to bias the recommendations for their own benefit. Research in this area has developed attack models, detection methods, and mitigation schemes to understand and protect against such attacks. For Collaborative Filtering RSs, model-based approaches such as item-based and matrix-factorization were found to be more robust to many types of attack. Advice in designing for system robustness has thus been to employ model-based approaches. Our recent work with the Power User Attack (PUA), however, determined that attackers disguised as influential users can successfully attack (from the attacker’s viewpoint) SVD-based recommenders, as well as user-based. Though item-based systems remained robust to the PUA. In this paper we investigate a new, complementary attack model, the Power Item Attack (PIA), that uses influential items to successfully attack RSs. We show that the PIA is able to impact not only user-based and SVD-based recommenders but also the heretofore highly robust item-based approach, using a novel multi-target attack vector.</p>
    <p><strong>Categories:</strong> Recommender Systems, Security, Attacks, Item-Based Recommenders, User-Based Recommenders, Matrix Factorization, Robustness, Vulnerability, System Exploits, Multi-Target Attack Vector, Power Items, Innovation (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/2/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>