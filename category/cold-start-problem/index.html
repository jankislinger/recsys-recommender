<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">Cold Start Problem</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/explainability/">Explainability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/user-behavior/">User Behavior</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/evaluation-methods/">Evaluation Methods</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Biased User History Synthesis for Personalized Long-Tail Item Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Elan Markowitz, Greg Ver Steeg, Abdulla Alshabanah, Keshav Balasubramanian, Murali Annavaram</p>
    <p>Recommendation systems connect users to items and create value chains in the internet economy. Recommendation systems learn from past user-item interaction histories. As such, items that have short interaction histories, either because they are new or not popular, are disproportionately under-recommended. This long-tail item problem can exacerbate model bias, and reinforce poor recommendation of tail items. In this paper, we propose a novel training algorithm, <i>biased user history synthesis</i>, to not only address this problem but also achieve better personalization in recommendation systems. As a result, we concurrently improve tail and head item recommendation performance. Our approach is built on a tail item biased User Interaction History (UIH) sampling strategy and a synthesis model that produces an augmented user representation from the sampled user history. We provide a theoretical justification for our approach using information theory and demonstrate through extensive experimentation, that our model outperforms state-of-the-art baselines on tail, head, and overall recommendation. the source code is available at https://github.com/lkp411/BiasedUserHistorySynthesis.</p>
    <p><strong>Categories:</strong> Algorithm, Personalization, Long-Tail Item Recommendation, Cold Start Problem, User Interaction History, Data Augmentation, Recommendation Systems, Evaluation Metrics, Theoretical Justification, Scalability and Performance (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1026/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>GLAMOR: Graph-based LAnguage MOdel embedding for citation Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Guilin Qi, Adam A. Q. Mohammed, Irfan Ullah, Khan Muhammad, Pavlos Kefalas, Zafar Ali</p>
    <p>Digital publishing’s exponential growth has created vast scholarly collections. Guiding researchers to relevant resources is crucial, and knowledge graphs (KGs) are key tools for unlocking hidden knowledge. However, current methods focus on external links between concepts, ignoring the rich information within individual papers. Challenges like insufficient multi-relational data, name ambiguity, and cold-start issues further limit existing KG-based methods, failing to capture the intricate attributes of diverse entities. To solve these issues, we propose GLAMOR, a robust KG framework encompassing entities e.g., authors, papers, fields of study, and concepts, along with their semantic interconnections. GLAMOR uses a novel random walk-based KG text generation method and then fine-tunes the language model using the generated text. Subsequently, the acquired context-preserving embeddings facilitate superior top@k predictions. Evaluation results on two public benchmark datasets demonstrate our GLAMOR’s superiority against state-of-the-art methods especially in solving the cold-start problem.</p>
    <p><strong>Categories:</strong> Knowledge Graphs, Citation Recommendation, Graph-based Methods, Language Models, Random Walk, Cold Start Problem, Embeddings, Research Tools/Methods, Scholarly Publishing (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1096/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Reproducibility of LLM-based Recommender Systems: the case study of P5 paradigm (2024)</h3>
    <p><strong>Authors:</strong> Marco Polignano, Cataldo Musto, Giovanni Semeraro, Pasquale Lops, Antonio Silletti</p>
    <p>Recommender systems field may greatly benefit of the availability of pretrained Large Language Models (LLMs), which can serve as the core mechanism to generate recommendations based on detailed user and item data, such as textual descriptions, user reviews, and metadata.  On one hand this new generation of LLM-based recommender systems paves the way to deal with traditional limitations, such as cold-start and data sparsity, but on the other hand this poses fundamental challenges for their accountability.  Reproducing experiments in the new context of LLM-based recommender systems is very challenging for several reasons. New approaches are published at an unprecedented pace, which makes difficult to have a clear picture of the main protocols and good practices in the experimental evaluation. Moreover, the lack of proper frameworks for LLM-based recommendation development and evaluation makes the process of benchmarking models complex and uncertain. In this work, we discuss the main issues encountered when trying to reproduce P5 (Pretrain, Personalized Prompt, and Prediction Paradigm), one of the first works unifying different recommendation tasks in a shared language modeling and natural language generation framework.  Starting from this study, we have developed OurFramework4LLM (anonymized name), a framework for training and evaluating LLMs, specifically for the recommendation task. It has been used to perform several experiments to assess the impact of different LLMs, personalization and novel set of more informative prompts on the overall performance of recommendations, in a fully reproducible environment.</p>
    <p><strong>Categories:</strong> Reproducibility, Large Language Models (LLMs), Recommender Systems, Framework Development, Benchmarking, Experimental Evaluation, P5 Paradigm, Natural Language Generation (NLG), Language Modeling, Cold-Start Problem, Data Sparsity, Personalization (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1130/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Discerning Canonical User Representation for Cross-Domain Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Sherry Sahebi, Siqian Zhao</p>
    <p>Cross-domain recommender systems have emerged, to address the cold-start problem and enhance recommendation outcomes by leveraging information transfer across different domains. Existing cross-domain recommender systems have investigated the learning of both domain-specific and domain-shared user preferences to enhance recommendation performance. However, these models typically allow the disparities between shared and distinct user preferences to emerge freely in any space, lacking sufficient constraints to identify differences between two domains and ensure that both domains are considered simultaneously. Canonical Correlation Analysis (CCA) has shown promise for transferring information between domains, by mapping their user representations into the same space. But, CCA only models domain similarities, and fails to capture the potential differences between user preferences in different domains. In this paper, we propose Discerning Canonical User Representation Learning for Cross-Domain Recommendation (DICUCDR), a generative adversarial networks (GAN) based method that learns both domain-shared and domain-specific user representations. DICUCDR introduces Discerning Canonical Correlation User Representation Learning (DCCRL), a novel design of non-linear Canonical Correlation mappings that creates a shared transformation for simultaneously mapping similarities between different domains and separating domain differences from domains. We compare DICUCDR against several state-of-the-art approaches using two real-world datasets. Our extensive experiments demonstrate the superiority of separately learning shared and specific user representations via DCCRL.</p>
    <p><strong>Categories:</strong> Cross-Domain Recommendation, Cold Start Problem, Transfer Learning, Multi-Domain, Canonical Correlation Analysis (CCA), Generative Adversarial Networks (GANs), Deep Learning, Representation Learning, Domain Adaptation, Real-World Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1033/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Enhancing Recommendation Quality of the SASRec Model by Mitigating Popularity Bias (2024)</h3>
    <p><strong>Authors:</strong> Xenija Neufeld, Sebastian Loth, Andreas Grün, Venkata Harshit Koneru</p>
    <p>ZDF is a Public Service Media (PSM) broadcaster in Germany that uses recommender systems on its streaming service platform ZDFmediathek. One of the main use cases within the ZDFmediathek is Next Video, which is currently based on a Self-Attention based Sequential Recommendation model (SASRec). For this use case, we modified the loss function, the sampling method of negative items, and introduced the top-k negative sampling strategy and compared this to the vanilla SASRec model. We show that this not only reduces popularity bias, but also increases clicks and the viewing volume compared to that of the vanilla version.</p>
    <p><strong>Categories:</strong> SASRec, Popularity Bias, Recommendation Quality, Streaming Services, Public Service Media, Clicks/Engagement Metrics, Loss Function Modification, Negative Sampling Strategies, Real World Applications, A/B Testing, User Experience Optimization, Cold Start Problem (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1164/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Enhancing Cross-Domain Recommender Systems with LLMs: Evaluating Bias and Beyond-Accuracy Measures (2024)</h3>
    <p><strong>Authors:</strong> Thomas Elmar Kolb</p>
    <p>The research domain of recommender systems is rapidly evolving. Initially, optimization efforts focused primarily on accuracy. However, recent research has highlighted the importance of addressing bias and beyond-accuracy measures such as novelty, diversity, and serendipity. With the rise of multi-domain recommender systems, the need to re-examine bias and beyond-accuracy measures in cross-domain settings has become crucial. Traditional methods face challenges such as cold-start problems, which can potentially be mitigated by leveraging LLMs. This proposed work investigates how LLM-based recommendation methods can enhance cross-domain recommender systems, focusing on identifying, measuring, and mitigating bias while evaluating the impact of beyond-accuracy measures. We aim to provide new insights by comparing traditional and LLM-based systems within a real-world environment encompassing the domains of news, books, and various lifestyle areas. Our research seeks to address the outlined gaps and develop effective evaluation strategies for the unique challenges posed by LLMs in cross-domain recommender systems.</p>
    <p><strong>Categories:</strong> Cross-Domain Recommender Systems, Large Language Models (LLMs), Bias, Novelty, Diversity, Serendipity, Beyond Accuracy, Cold Start Problem, News Domain, Books Domain, Lifestyle Domain, Real-World Applications, Traditional Recommenders, Neural Networks, Multi-Domain Evaluation, Enhancing Recommender Systems (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1137/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Scaling Law of Large Sequential Recommendation Models (2024)</h3>
    <p><strong>Authors:</strong> Hongyu Lu, Yu Chen, Wayne Xin Zhao, Gaowei Zhang, Yupeng Hou, Ji-Rong Wen</p>
    <p>Scaling of neural networks has recently shown great potential to improve the model capacity in various fields. Specifically, model performance has a power-law relationship with model size or data size, which provides important guidance for the development of large-scale models. However, there is still limited understanding on the scaling effect of user behavior models in recommender systems, where the unique data characteristics (e.g., data scarcity and sparsity) pose new challenges in recommendation tasks. In this work, we focus on investigating the scaling laws in large sequential recommendation models. Specifically, we consider a pure ID-based task formulation, where the interaction history of a user is formatted as a chronological sequence of item IDs. We don’t incorporate any side information (e.g., item text), to delve into the scaling law’s applicability from the perspective of user behavior. We successfully scale up the model size to 0.8B parameters, making it feasible to explore the scaling effect in a diverse range of model sizes. As the major findings, we empirically show that the scaling law still holds for these trained models, even in data-constrained scenarios. We then fit the curve for scaling law, and successfully predict the test loss of the two largest tested model scales. Furthermore, we examine the performance advantage of scaling effect on five challenging recommendation tasks, considering the unique issues (e.g., cold start, robustness, long-term preference) in recommender systems. We find that scaling up the model size can greatly boost the performance on these challenging tasks, which again verifies the benefits of large recommendation models.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Scaling Laws, Sequential Recommendations, Model Scalability, Large-Scale Models, User Behavior Modeling, Cold Start Problem, Evaluation Metrics, Data Sparsity, Model Capacity (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1065/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>FairCRS: Towards User-oriented Fairness in Conversational Recommendation Systems (2024)</h3>
    <p><strong>Authors:</strong> Tianlong Gu, Qin Liu, Xuan Feng, Xiaoli Liu</p>
    <p>Conversational Recommendation Systems (CRSs) enable recommender systems to explicitly acquire user preferences during multiturn interactions, providing more accurate and personalized recommendations. However, the data imbalance in CRSs, due to inconsistent interaction history among users, may lead to disparate treatment for disadvantaged user groups. In this paper, we investigate the discriminate issues in CRS from the user’s perspective, called as user-oriented fairness. To reveal the unfairness problems of different user groups in CRS, we conduct extensive empirical analyses. To mitigate user unfairness, we propose a user-oriented fairness framework, named FairCRS, which is a model-agnostic framework. In particular, we develop a user-embedding reconstruction mechanism that enriches user embeddings by incorporating more interaction information, and design a user-oriented fairness strategy that optimizes the recommendation quality differences among user groups while alleviating unfairness. Extensive experimental results on English and Chinese datasets show that FairCRS outperforms state-of-the-art CRSs in terms of overall recommendation performance and user fairness.</p>
    <p><strong>Categories:</strong> Conversational Recommendation Systems (CRS), User-Oriented Fairness, Fairness in AI/ML, Data Imbalance, Model-Agnostic Frameworks, User Embedding, Recommender Systems, Multi-Turn Interaction, Cold Start Problem, User Group Analysis, Evaluation Metrics, Empirical Analysis (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1035/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Bayesian Optimization with LLM-Based Acquisition Functions for Natural Language Preference Elicitation (2024)</h3>
    <p><strong>Authors:</strong> Armin Toroghi, David Austin, Anton Korikov, Scott Sanner</p>
    <p>Designing preference elicitation (PE) methodologies that can quickly ascertain a user’s top item preferences in a cold-start setting is a key challenge for building effective and personalized conversational recommendation (ConvRec) systems. While large language models (LLMs) constitute a novel technology that enables fully natural language (NL) PE dialogues, we hypothesize that monolithic LLM NL-PE approaches lack the multi-turn, decision-theoretic reasoning required to effectively balance the NL exploration and exploitation of user preferences towards an arbitrary item set. In contrast, traditional Bayesian optimization PE methods define theoretically optimal PE strategies, but fail to use NL item descriptions or generate NL queries, unrealistically assuming users can express preferences with direct item ratings and comparisons. To overcome the limitations of both approaches, we formulate NL-PE in a Bayesian Optimization (BO) framework that seeks to actively elicit natural language feedback to reduce uncertainty over item utilities to identify the best recommendation. Key challenges in generalizing BO to deal with natural language feedback include determining (a) how to leverage LLMs to model the likelihood of NL preference feedback as a function of item utilities and (b) how to design an acquisition function that works for language-based BO that can elicit in the infinite space of language. We demonstrate our framework in a novel NL-PE algorithm, PEBOL, which uses Natural Language Inference (NLI) between user preference utterances and NL item descriptions to maintain preference beliefs and BO strategies such as Thompson Sampling (TS) and Upper Confidence Bound (UCB) to guide LLM query generation. We numerically evaluate our methods in controlled experiments, finding that PEBOL achieves up to 131% improvement in MAP@10 after 10 turns of cold start NL-PE dialogue compared to monolithic GPT-3.5, despite relying on a much smaller 400M parameter NLI model for preference inference.</p>
    <p><strong>Categories:</strong> Bayesian Optimization, Large Language Models (LLMs), Natural Language Processing (NLP), Conversational Recommendation Systems, Preference Elicitation, Cold Start Problem, Algorithm Selection, Multi-Armed Bandits, Thompson Sampling, Upper Confidence Bound, Natural Language Inference (NLI), Evaluation Metrics, Machine Learning (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1020/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>One-class recommendation systems  with  the hinge pairwise distance loss and orthogonal representations (2024)</h3>
    <p><strong>Authors:</strong> Ramin Raziperchikolaei, Young-joo Chung</p>
    <p>In one-class recommendation systems, the goal is to learn a model from a small set of interacted users and items and then identify the positively-related (i.e., similar) user-item pairs among a large number of pairs with unknown interactions. Most loss functions in the literature rely on dissimilar pairs of users and items, which are selected from the ones with unknown interactions, to obtain better prediction performance. The main issue of this strategy is that it needs a large number of dissimilar pairs, which increases the training time significantly. In this paper, the goal is to only use the similar set to train the models and discard the dissimilar set.  We highlight three trivial solutions that the models converge to when they are trained only on similar pairs: collapsed, dimensional collapsed, and shrinking solutions. We propose a hinge pairwise loss and an orthogonality term that can be added to the objective functions in the literature to avoid these trivial solutions. We conduct experiments on various tasks on public and real-world datasets, which show that our approach using only similar pairs can be trained several times faster than the state-of-the-art methods while achieving competitive results.</p>
    <p><strong>Categories:</strong> Recommendation Systems, One-class Learning, Algorithm Design, Loss Function Design, Optimization Techniques, User-Item Interaction, Similarity-based Recommendations, Cold Start Problem, Efficiency, Real-world Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1102/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Overcoming Recommendation Limitations with Neuro-Symbolic Integration (2023)</h3>
    <p><strong>Authors:</strong> Tommaso Carraro</p>
    <p>Despite being studied for over twenty years, Recommender Systems (RSs) still suffer from important issues that limit their applicability in real-world scenarios. Data sparsity, cold start, and explainability are some of the most impacting problems. Intuitively, these historical limitations can be mitigated by injecting prior knowledge into recommendation models. Neuro-Symbolic (NeSy) approaches are suitable candidates for achieving this goal. Specifically, they aim to integrate learning (e.g., neural networks) with symbolic reasoning (e.g., logical reasoning). Generally, the integration lets a neural model interact with a logical knowledge base, enabling reasoning capabilities. In particular, NeSy approaches have been shown to deal well with poor training data, and their symbolic component could enhance model transparency. This gives insights that NeSy systems could potentially mitigate the aforementioned RSs limitations. However, the application of such systems to RSs is still in its early stages, and most of the proposed architectures do not really exploit the advantages of a NeSy approach. To this end, we conducted preliminary experiments with a Logic Tensor Network (LTN), a novel NeSy framework. We used the LTN to train a vanilla Matrix Factorization model using a First-Order Logic knowledge base as an objective. In particular, we encoded facts to enable the regularization of the latent factors using content information, obtaining promising results. In this paper, we review existing NeSy recommenders, argue about their limitations, show our preliminary results with the LTN, and propose interesting future works in this novel research area. In particular, we show how the LTN can be intuitively used to regularize models, perform cross-domain recommendation, ensemble learning, and explainable recommendation, reduce popularity bias, and easily define the loss function of a model.</p>
    <p><strong>Categories:</strong> Neuro-Symbolic Integration, Recommender Systems, Data Sparsity, Cold Start Problem, Explainability, Neural Networks, Symbolic Reasoning, Logic Tensor Networks (LTN), Matrix Factorization, Content-Based Recommendations, Model Transparency, Regularization, Cross-Domain Recommendation, Ensemble Learning, Popularity Bias (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/986/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Beyond Labels: Leveraging Deep Learning and LLMs for Content Metadata (2023)</h3>
    <p><strong>Authors:</strong> Jaya Kawale, Saurabh Agrawal, John Trenkle</p>
    <p>Content metadata plays a very important role in movie recommender systems as it provides valuable information about various aspects of a movie such as genre, cast, plot synopsis, box office summary, etc. Analyzing the metadata can help understand the user preferences and generate personalized recommendations catering to the niche tastes of the users. It can also help with content cold starting when the recommender system has little or no interaction data available to perform collaborative filtering. In this talk, we will focus on one particular type of metadata – genre labels. Genre labels associated with a movie or a TV series such as “horror” or “comedy” or “romance” help categorize a collection of movies into different themes and correspondingly setting up the audience expectation for a title. We present some of the challenges associated with using genre label information via traditional methods and propose a new way of examining the genre information that we call as the Genre Spectrum. The Genre Spectrum helps capture the various nuanced genres in a title and our offline and online experiments corroborate the effectiveness of the approach.</p>
    <p><strong>Categories:</strong> Deep Learning, Large Language Models (LLMs), Recommendation Systems, Content Metadata, Genre Analysis, Cold Start Problem, Personalized Recommendations, User Preferences, Offline Experiments, Online Experiments (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/993/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Optimizing product recommendations for millions of merchants (2022)</h3>
    <p><strong>Authors:</strong> Chen Karako, Kim Peiter Jorgensen</p>
    <p>At Shopify, we serve product recommendations to customers across millions of merchants’ online stores. It is a challenge to provide optimized recommendations to all of these independent merchants; one model might lead to an overall improvement in our metrics on aggregate, but significantly degrade recommendations for some stores. To ensure we provide high quality recommendations to all merchant segments, we develop several models that work best in different situations as determined in offline evaluation. Learning which strategy best works for a given segment also allows us to start off new stores with good recommendations, without necessarily needing to rely on an individual store amassing large amounts of traffic. In production, the system will start out with the best strategy for a given merchant, and then adjust to the current environment using multi-armed bandits. Collectively, this methodology allows us to optimize the types of recommendations served on each store.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Scalability, Cross-Domain Recommendations, Personalization, Multi-Armed Bandits, Cold Start Problem, Evaluation Metrics, Optimization Techniques, Model Deployment, Real-World Applications, E-commerce. (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/826/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Denoising User-aware Memory Network for Recommendation (2021)</h3>
    <p><strong>Authors:</strong> Zhenqi Sun, Xiaolong Li, Kaikui Liu, Junjie Tang, Zhi Bian, Hao Fu, Qihong Yang, Shaojun Zhou, Guiquan Liu</p>
    <p>For better user satisfaction and business effectiveness, more and more attention has been paid to the sequence-based recommendation system, which is used to infer the evolution of users’ dynamic preferences, and recent studies have noticed that the evolution of users’ preferences can be better understood from the implicit and explicit feedback sequences. However, most of the existing recommendation techniques do not consider the noise contained in implicit feedback, which will lead to the biased representation of user interest and a suboptimal recommendation performance. Meanwhile, the existing methods utilize item sequence for capturing the evolution of user interest. The performance of these methods is limited by the length of the sequence, and can not effectively model the long-term interest in a long period of time. Based on this observation, we propose a novel CTR model named denoising user-aware memory network (DUMN). Specifically, the framework: (i) proposes a feature purification module based on orthogonal mapping, which use the representation of explicit feedback to purify the representation of implicit feedback, and effectively denoise the implicit feedback; (ii) designs a user memory network to model the long-term interests in a fine-grained way by improving the memory network, which is ignored by the existing methods; and (iii) develops a preference-aware interactive representation component to fuse the long-term and short-term interests of users based on gating to understand the evolution of unbiased preferences of users. Extensive experiments on two real e-commerce user behavior datasets show that DUMN has a significant improvement over the state-of-the-art baselines.</p>
    <p><strong>Categories:</strong> Deep Learning, Matrix Factorization, Sequence-based Recommendations, E-commerce, Hybrid Recommendation, Implicit Feedback, Explicit Feedback, Long-term Recommendations, Short-term Recommendations, Denoising Techniques, Memory Networks, Personalized Recommendations, User Modeling, Cold Start Problem, Accuracy Metrics, Beyond Accuracy, A/B Test. (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/634/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Recommending the Video to Watch Next: An Offline and Online Evaluation at YOUTV.de (2020)</h3>
    <p><strong>Authors:</strong> Markus Zanker, Daniel Morandini, Ludovik Coba, Panagiotis Symeonidis, Andrea Janes, Dmitry Chaltsev, Andreas Unterhuber, Philip Giuliani</p>
    <p>The task “recommend a video to watch next?” has been in the focus of recommender systems’ research for a long time. However, adequately exploiting the clues hidden in the sequences of actions of user sessions in order to reveal users’ short-term intentions moved only recently into the focus of research. Based on a real-world application scenario, in this paper, we propose a Markov Chain-based transition probability matrix to efficiently reveal the short-term preferences of individuals. We experimentally evaluated our proposed method by comparing it against state-of-the-art algorithms in an offline as well as a live evaluation setting. In both cases our method not only demonstrated its superiority over its competitors, but exposed a clearly stronger engagement of users on the platform. In the online setting, our method improved the click-through rate by up to 93.61%. This paper therefore contributes real-world evidence for improving the recommendation effectiveness, by considering sequence-awareness, since capturing the short-term preferences of users is crucial in the light of items with a short life span such as tv programs (news, tv shows, etc.).</p>
    <p><strong>Categories:</strong> Markov Chain, Media/TV, Sequence-Aware Recommendations, Recommendation Systems, Offline Evaluation, Online Evaluation, Real-World Application, Click-Through Rate (CTR), User Engagement, State-of-the-Art Comparison, Probabilistic Models, Cold Start Problem (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/552/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Towards Multi-Language Recipe Personalisation and Recommendation (2020)</h3>
    <p><strong>Authors:</strong> Niall Twomey, Andrey Ponikar, Nadine Sarraf, Mikhail Fain</p>
    <p>Multi-language recipe personalisation and recommendation is an under-explored field of information retrieval in academic and production systems. The existing gaps in our current understanding are numerous, even on fundamental questions such as whether consistent and high-quality recipe recommendation can be delivered across languages. Motivated by this need, we consider the multi-language recipe recommendation setting and present grounding results that will help to establish the potential and absolute value of future work in this area. Our work draws on several billion events from millions of recipes, with published recipes and users incorporating several languages, including Arabic, English, Indonesian, Russian, and Spanish. We represent recipes using a combination of normalised ingredients, standardised skills and image embeddings obtained without human intervention. In modelling, we take a classical approach based on optimising an embedded bi-linear user-item metric space towards the interactions that most strongly elicit cooking intent. For users without interaction histories, a bespoke content-based cold-start model that predicts context and recipe affinity is introduced. We show that our approach to personalisation is stable and scales well to new languages. A robust cross-validation campaign is employed and consistently rejects baseline models and representations, strongly favouring those we propose. Our results are presented in a language-oriented (as opposed to model-oriented) fashion to emphasise the language-based goals of this work. We believe that this is the first large-scale work that evaluates the value and potential of multi-language recipe recommendation and personalisation.</p>
    <p><strong>Categories:</strong> Multi-Language Recommendations, Recipe Recommendation, Personalization, Content-Based Filtering, Cross-Validation, Recommendation Systems, Cold Start Problem, Large-Scale Data Analysis, Embeddings, User-Item Interaction Models, Cross-Lingual Evaluation (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/614/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>