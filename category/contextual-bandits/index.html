<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">Contextual Bandits</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/bias-mitigation/">Bias Mitigation</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/real-world-applications/">Real-world Applications</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/offline-evaluation/">Offline Evaluation</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Effective Off-Policy Evaluation and Learning in Contextual Combinatorial Bandits (2024)</h3>
    <p><strong>Authors:</strong> Yuta Saito, Tatsuhiro Shimizu, Ren Kishimoto, Masahiro Nomura, Koichi Tanaka, Haruka Kiyohara</p>
    <p>We explore off-policy evaluation and learning in contextual combinatorial bandits (CCB), where a policy selects a subset in the action space. For example, it might choose a set of furniture pieces (a bed and a drawer) from available items (bed, drawer, chair, etc.) for interior design sales. This setting is widespread in fields such as recommender systems and healthcare, yet OPE/L of CCB remains unexplored in the relevant literature. Standard OPE methods typically employ regression and importance sampling in the action subset space. However, they often face significant challenges due to high bias or variance, exacerbated by the exponential growth in the number of available subsets. To address these challenges, we introduce a concept of factored action space, which allows us to decompose each subset into binary indicators. These indicators signify whether each action is included in the selected subset. This formulation allows us to distinguish between the “main effect” derived from the main actions, and the “residual effect”, originating from the supplemental actions, facilitating more effective OPE. Specifically, our estimator, called OPCB, leverages an importance sampling-based approach to unbiasedly estimate the main effect, while employing regression-based approach to deal with the residual effect with low variance. OPCB achieves substantial variance reduction compared to conventional importance sampling methods and bias reduction relative to regression methods under certain conditions, as illustrated in our theoretical analysis. Experiments on both synthetic and real-world datasets demonstrate OPCB’s superior performance over the typical methods, particularly when navigating the complexities of a large action subset space.</p>
    <p><strong>Categories:</strong> Combinatorial Bandits, Contextual Bandits, Off-Policy Evaluation, Importance Sampling, Regression Methods, Recommender Systems, Healthcare, Scalability, Evaluation Metrics, Multi-Armed Bandits, Real-World Applications, Theoretical Analysis. (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1040/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Optimal Baseline Corrections for Off-Policy Contextual Bandits (2024)</h3>
    <p><strong>Authors:</strong> Shashank Gupta, Olivier Jeunen, Maarten de Rijke, Harrie Oosterhuis</p>
    <p>The off-policy learning paradigm allows for recommender systems and general ranking applications to be framed as decision-making problems, where we aim to learn decision policies that optimize an unbiased offline estimate of an online reward metric. With unbiasedness comes potentially high variance, and prevalent methods exist to reduce estimation variance. These methods typically make use of control variates, either additive (i.e., baseline corrections or doubly robust methods) or multiplicative (i.e., self-normalisation). Our work unifies these approaches by proposing a single framework built on their equivalence in learning scenarios. The foundation of our framework is the derivation of an equivalent baseline correction for all of the existing control variates. Consequently, our framework enables us to characterize the variance-optimal unbiased estimator and provide a closed-form solution for it. This optimal estimator brings significantly improved performance in both evaluation and learning, and minimizes data requirements. Empirical observations corroborate our theoretical findings.</p>
    <p><strong>Categories:</strong> Contextual Bandits, Off-Policy Learning, Control Variates, Baseline Corrections, Doubly Robust Methods, Variance Reduction, Optimal Estimation, Recommender Systems, Ranking, Reinforcement Learning (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1052/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Dynamic Product Image Generation and Recommendation at Scale for Personalized E-commerce (2024)</h3>
    <p><strong>Authors:</strong> Ádám Tibor Czapp, Bálint Domián, Balázs Hidasi, Mátyás Jani</p>
    <p>Coupling latent diffusion based image generation with contextual bandits enables creating eye-catching personalized product images at a scale that was previously either impossible or too expensive. In this paper we showcase how we utilized these technologies to increase user engagement with recommendations in online retargeting campaigns for e-commerce.</p>
    <p><strong>Categories:</strong> Latent Diffusion Models, Contextual Bandits, Multi-Armed Bandits, Recommendation Systems, E-commerce, Personalization, User Engagement, Image Generation, Retargeting Campaigns, Machine Learning, Real-World Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1159/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Why the Shooting in the Dark Method Dominates Recommender Systems Practice (2024)</h3>
    <p><strong>Authors:</strong> David Rohde</p>
    <p>The introduction of A/B Testing represented a great leap forward in recommender systems research. Like the randomized control trial for evaluating drug efficacy; A/B Testing has equipped recommender systems practitioners with a protocol for measuring performance as defined by actual business metrics and with minimal assumptions. While A/B testing provided a way to measure the performance of two or more candidate systems, it provides no guide for determining what policy we should test. The focus of this industry talk is to better understand, why the development of A/B testing was the last great leap forward in the development of reward optimizing recommender systems despite more than a decade of efforts in both industry and academia. The talk will survey: industry best practice, standard theories and tools including: collaborative filtering (MovieLens RecSys), contextual bandits, attribution, off-policy estimation, causal inference, click through rate models and will explain why we have converged on a fundamentally heuristic solution or guess and check type method. The talk will offer opinions about which of these theories are useful, and which are not and make a concrete proposal to make progress based on a non-standard use of deep learning tools.</p>
    <p><strong>Categories:</strong> A/B Testing, Evaluation Methods, Industry Practices, Challenges, Collaborative Filtering, Contextual Bandits, Attribution, Off-Policy Estimation, Causal Inference, Click-Through Rate Models, Methodologies, Theory Application, Future Directions (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1183/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Progressive Horizon Learning: Adaptive Long Term Optimization for Personalized Recommendation (2023)</h3>
    <p><strong>Authors:</strong> Zijian Ni, Shreya Chakrabarti, Congrui Yi, David Zumwalt</p>
    <p>As B2C companies such as Amazon, Netflix, Spotify scale, personalized recommender systems are often needed to further drive long term business growth in acquisition, engagement, and retention of customers. However, long-term metrics associated with these goals can require several months to mature. Additionally, deep personalization also demands a large volume of training data that take a long time to collect. These factors incur substantial lead time for training a model to optimize a long-term metric. Before such model is deployed, a recommender system has to rely on a simple policy (e.g. random) to collect customer feedback data for training, inflicting high opportunity cost and delaying optimization of the target metric. Besides, as customer preferences can shift over time, a large temporal gap between inputs and outcome poses a high risk of data staleness and suboptimal learning. Existing approaches involve various compromises. For instance, contextual bandits often optimize short-term surrogate metrics with simple model structure, which can be suboptimal in the long run, while Reinforcement Learning approaches rely on an abundance of historical data for offline training, which essentially means long lead time before deployment. To address these problems, we propose Progressive Horizon Learning Recommender (PHLRec), a personalized model that can progressively learn metric patterns and adaptively evolve from short- to long-term optimization over time. Through simulations and real data experiments, we demonstrated that PHLRec outperforms competing methods, achieving optimality in both deployment speed and long-term metric performances.</p>
    <p><strong>Categories:</strong> Personalized Recommendation, Long-Term Metrics, Adaptive Methods, Contextual Bandits, Reinforcement Learning (RL), Progressive Learning, Deployment Optimization, Business Applications, Real-World Experiments, Scalability, Opportunity Cost, Temporal Dynamics (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/932/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>AdaptEx: a self-service contextual bandit platform (2023)</h3>
    <p><strong>Authors:</strong> William Black, Andrea Marchini, Ercument Ilhan, Vilda Markeviciute</p>
    <p>This paper presents AdaptEx, a self-service contextual bandit platform widely used at Expedia Group, that leverages multi-armed bandit algorithms to personalize user experiences at scale. AdaptEx considers the unique context of each visitor to select the optimal variants and learns quickly from every interaction they make. It offers a powerful solution to improve user experiences while minimizing the costs and time associated with traditional testing methods. The platform unlocks the ability to iterate towards optimal product solutions quickly, even in ever-changing content and continuous “cold start” situations gracefully.</p>
    <p><strong>Categories:</strong> Multi-Armed Bandits, Personalization, Real-World Applications, User Experience Optimization, Experimentation, Cold Start, Product Optimization, Scalability, Rapid Learning, Contextual Bandits (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/989/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Incentivizing Exploration in Linear Contextual Bandits under Information Gap (2023)</h3>
    <p><strong>Authors:</strong> Zhiyuan Liu, Hongning Wang, Huazheng Wang, Chuanhao Li, Haifeng Xu</p>
    <p>Contextual bandit algorithms have been popularly used to address interactive recommendation, where the users are assumed to be cooperative to explore all recommendations from a system. In this paper, we relax this strong assumption and study the problem of incentivized exploration with myopic users, where the users are only interested in recommendations with their currently highest estimated reward. As a result, in order to obtain long-term optimality, the system needs to offer compensation to incentivize the users to take the exploratory recommendations. We consider a new and practically motivated setting where the context features employed by the user are more <i>informative</i> than those used by the system: for example, features based on users’ private information are not accessible by the system. We develop an effective solution for incentivized exploration under such an information gap, and prove that the method achieves a sublinear rate in both regret and compensation. We theoretically and empirically analyze the added compensation due to the information gap, compared with the case where the system has access to the same context features as the user does, i.e., without information gap. Moreover, we also provide a compensation lower bound of this problem.</p>
    <p><strong>Categories:</strong> Contextual Bandits, Reinforcement Learning, Exploration vs Exploitation, Incentivizing Exploration, User Behavior Modeling, Interactive Recommendations, Information Gap, Regret Analysis, Compensation Mechanisms, Theoretical Analysis, Empirical Evaluation, Multi-Armed Bandits (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/874/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Top-K Contextual Bandits with Equity of Exposure (2021)</h3>
    <p><strong>Authors:</strong> Olivier Jeunen, Bart Goethals</p>
    <p>The contextual bandit paradigm provides a general framework for decision-making under uncertainty. It is theoretically well-defined and well-studied, and many personalisation use-cases can be cast as a bandit learning problem. Because this allows for the direct optimisation of utility metrics that rely on online interventions (such as click-through-rate (CTR)), this framework has become an attractive choice to practitioners. Historically, the literature on this topic has focused on a one-sided, user-focused notion of utility, overall disregarding the perspective of content providers in online marketplaces (for example, musical artists on streaming services). If not properly taken into account – recommendation systems in such environments are known to lead to unfair distributions of attention and exposure, which can directly affect the income of the providers. Recent work has shed a light on this, and there is now a growing consensus that some notion of “equity of exposure” might be preferable to implement in many recommendation use-cases.<br>We study how the top-K contextual bandit problem relates to issues of disparate exposure, and how this disparity can be minimised. The predominant approach in practice is to greedily rank the top-K items according to their estimated utility, as this is optimal according to the well-known Probability Ranking Principle. Instead, we introduce a configurable tolerance parameter that defines an acceptable decrease in utility for a maximal increase in fairness of exposure. We propose a personalised exposure-aware arm selection algorithm that handles this relevance-fairness trade-off on a user-level, as recent work suggests that users’ openness to randomisation may vary greatly over the global populace. Our model-agnostic algorithm deals with arm selection instead of utility modelling, and can therefore be implemented on top of any existing bandit system with minimal changes. We conclude with a case study on carousel personalisation in music recommendation: empirical observations highlight the effectiveness of our proposed method and show that exposure disparity can be significantly reduced with a negligible impact on user utility.</p>
    <p><strong>Categories:</strong> Contextual Bandits, Equity of Exposure, Personalization, Fairness in Recommendations, Top-K Selection, Music Recommendation, Algorithm Design, User-Centered Design, Relevance-Fairness Trade-off, Evaluation Methods, Exposure Disparity, Machine Learning, Bandit Algorithms, Case Study. (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/670/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Deep Bayesian Bandits: Exploring in Online Personalized Recommendations (2020)</h3>
    <p><strong>Authors:</strong> Dalin Guo, Sourav Das, Michael Kneier, Pranay Kumar Kumar Myana, Alykhan Tejani, Sofia Ira Ira Ktena, Ferenc Huszar, Wenzhe Shi</p>
    <p>Recommender systems trained in a continuous learning fashion are plagued by the feedback loop problem, also known as algorithmic bias. This causes a newly trained model to act greedily and favor items that have already been engaged by users. This behavior is particularly harmful in personalised ads recommendations, as it can also cause new campaigns to remain unexplored. Exploration aims to address this limitation by providing new information about the environment, which encompasses user preference, and can lead to higher long-term reward. In this work, we formulate a display advertising recommender as a contextual bandit and implement exploration techniques that require sampling from the posterior distribution of click-through-rates in a computationally tractable manner. Traditional large-scale deep learning models do not provide uncertainty estimates by default. We approximate these uncertainty measurements of the predictions by employing a bootstrapped model with multiple heads and dropout units. We benchmark a number of different models in an offline simulation environment using a publicly available dataset of user-ads engagements. We test our proposed deep Bayesian bandits algorithm in the offline simulation and online AB setting with large-scale production traffic, where we demonstrate a positive gain of our exploration model.</p>
    <p><strong>Categories:</strong> Recommender Systems, Contextual Bandits, Bayesian Methods, Personalization, Deep Learning, Exploration vs Exploitation, Algorithmic Bias, Display Advertising, Uncertainty Estimation, Feedback Loop, Online AB Testing, Large-Scale Applications. (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/571/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Balancing relevance and discovery to inspire customers in the IKEA App (2020)</h3>
    <p><strong>Authors:</strong> Balazs Toth</p>
    <p>IKEA stores are designed to engage and inspire customers as they make their way from the entrance to check-out. To enable a great experience for every individual, our home-furnishing experts strive to provide the best possible input and advice, catering to individual needs and helping customers realise their vision for life at home.<br>As we move rapidly towards a multi touch point shopping vision with digital at heart, data and machine learning are crucial to 1) enable a consistent journey across, and 2) extend the domain expertise found in stores to our purely digital touch points, like the new IKEA App. A specific example of a digital product where we aim to inspire is our newly launched Inspirational Feed. In the Feed, customers browse their way through ”shoppable” images, in which our products are displayed in purposefully atmospheric settings for a wide range of room types. In this talk, we present our newest algorithmic approach to personalise the Feed based on contextual bandits [2, 3, 4], and highlight some of the technical challenges regarding implementation at scale.<br>The past decade has led to an explosion in research on recommender systems, often with personalised content as a motivating factor [1]. To identify a suitable class of algorithmic strategies for the Feed, note that: To capture the described trade-off between relevance and discovery, as well as matching data for how customers on average shop with IKEA, contextual bandits with batch learning from logged bandit feedback inspired by [4] have shown especially promising results among numerous other exploitation-exploration strategies. This approach induces a fair amount of exploration and minimizes the risk of showing irrelevant images to the customer based on the principle of counterfactual risk minimisation. In our contextual bandit setup, a bandit session is defined as a user session of the Feed taking place in the App (the home of the Feed). An action is displaying an image, while a reward is generated whenever the user clicks on it. The bandit context consists of previous user behaviour collected from the App, combined with product metadata. Our model takes advantage of embeddings of the input features, similar to [1].<br>The implementation of the strategy requires us to handle several interesting edge cases, e.g. we are looking into various techniques for increasing exploration as new images are added to our dynamic content catalogue.</p>
    <p><strong>Categories:</strong> Contextual Bandits, Exploitation-Exploration Strategies, Recommender Systems, Personalization, Retail, Real World Applications, Diversity of Recommendations, Beyond Accuracy, Implementation Challenges, Embeddings, Home Furnishing, Digital Experience (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/620/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Carousel Personalization in Music Streaming Apps with Contextual Bandits (2020)</h3>
    <p><strong>Authors:</strong> Théo Bontempelli, Guillaume Salha, Walid Bendada</p>
    <p>Media services providers, such as music streaming platforms, frequently leverage swipeable carousels to recommend personalized content to their users. However, selecting the most relevant items (albums, artists, playlists...) to display in these carousels is a challenging task, as items are numerous and as users have different preferences. In this paper, we model carousel personalization as a contextual multi-armed bandit problem with multiple plays, stochastic arm display and delayed batch feedback. We empirically show the effectiveness of our framework at capturing characteristics of real-world carousels by addressing a large-scale playlist recommendation task on a global music streaming mobile app. Along with this paper, we publicly release industrial data from our experiments, as well as an open-source environment to simulate comparable carousel personalization learning problems.</p>
    <p><strong>Categories:</strong> Contextual Bandits, Multi-Armed Bandits, Recommendation Systems, Personalization, Scalability, Carousel Design, Music Streaming, Real-World Applications, A/B Testing, Open Source Tools, Data Sharing (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/563/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Exploring Clustering of Bandits for Online Recommendation System (2020)</h3>
    <p><strong>Authors:</strong> Feng Xia, Qiang Yang, Bo Liu, Kai Chen, Leyu Lin, Liu Yang</p>
    <p>Cluster-of-bandit policy leverages contextual bandits in a collaborative filtering manner and aids personalized services in the online recommendation system (RecSys). When facing insufficient observations, the cluster-of-bandit policy could achieve more outstanding performance because of knowledge sharing. Cluster-of-bandit policy aims to maximize the cumulative feedback, e.g., clicks, from users. Nevertheless, in the way of their goal exist two kinds of uncertainties. First, cluster-of-bandit algorithms make recommendations according to their uncertain estimation of user interests. Second, cluster-of-bandit algorithms transfer relevant knowledge upon uncertain and noisy user clusters. Existing algorithms only consider the first one, while leaving the latter one untouched. To address the two challenges together, in this paper, we propose the ClexB policy for online RecSys. On the one hand, ClexB estimates user clustering more accurately and with less uncertainty via explorable-clustering. On the other hand, ClexB also exploits and explores user interests by sharing information within and among user clusters. In summary, ClexB explores knowledge transfer and further aids the inferences about user interests. Besides, we provide extensive empirical experiments on both the synthetic and real-world datasets and regret analysis, further consolidating the superiority of ClexB.</p>
    <p><strong>Categories:</strong> Clustering, Multi-Armed Bandits, Recommendation System, Contextual Bandits, Collaborative Filtering, Personalized Services, Uncertainty Handling, User Clustering, Knowledge Transfer, Exploration vs Exploitation, Empirical Experiments, Algorithm Design (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/533/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Interactive Recommendation via Deep Neural Memory Augmented Contextual Bandits (2018)</h3>
    <p><strong>Authors:</strong> Yue Deng, Avik Ray, Yilin Shen, Hongxia Jin</p>
    <p>Personalized recommendation with user interactions has become increasingly popular nowadays in many applications with dynamic change of contents (news, media, etc.). Existing approaches model user interactive recommendation as a contextual bandit problem to balance the trade-off between exploration and exploitation. However, these solutions require a large number of interactions with each user to provide high quality personalized recommendations. To mitigate this limitation, we design a novel deep neural memory augmented mechanism to model and track the history state for each user based on his previous interactions. As such, the user’s preferences on new items can be quickly learned within a small number of interactions. Moreover, we develop new algorithms to leverage large amount of all users’ history data for offline model training and online model fine tuning for each user with the focus of policy evaluation. Extensive experiments on different synthetic and real-world datasets validate that our proposed approach consistently outperforms a variety of state-of-the-art approaches.</p>
    <p><strong>Categories:</strong> Contextual Bandits, Deep Neural Networks, Interactive Recommendation, News, Media, Scalability, Efficiency, Offline Training, Online Fine-Tuning, Real-World Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/341/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Kernalized Collaborative Contextual Bandits (2017)</h3>
    <p><strong>Authors:</strong> Leonardo Cella, Romaric Gaudel, Paolo Cremonesi</p>
    <p>We tackle the problem of recommending products in the online recommendation scenario, which occurs many times in real applications. The most famous and explored instances are news recommendations and advertisements. In this work we propose an extension to the state of the art Bandit models to not only take care of different users’ interactions, but also to go beyond the linearity assumption of the expected reward. As applicative case we may consider situations in which the number of actions (products) is too big to sample all of them even once, and at the same time we have several changing users to serve content to.</p>
    <p><strong>Categories:</strong> Collaborative Filtering, Contextual Bandits, Recommendation Systems, Scalability, Kernel Methods, Non-Linear Models, Real World Applications, User Adaptation, Implicit Feedback, Beyond Accuracy, Online Learning, Large Action Space (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/328/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>A Scalable Approach for Periodical Personalized Recommendations (2016)</h3>
    <p><strong>Authors:</strong> Ish Rishabh, John Carnahan, Zhen Qin</p>
    <p>We develop a highly scalable and effective contextual bandit approach towards periodical personalized recommendations. The online bootstrapping-based technique provides a principled way for UCB-type exploitation-exploration algorithms, while being able to handle arbitrary sized datasets, well suited to learn the ever evolving user preference drift from streaming data, and essentially parameter-free. We further introduce techniques to handle arbitrary sized feature spaces using feature hashing, leverage existing state-of-art machine learning via learning reduction, and increase cache hits by managing bootstrapped models in memory effectively. The resulted model trains on millions of examples and billions of features within minutes on a single personal computer. It shows persistent performance in both offline and online evaluation. We observe around 10% click through rate (CTR) and conversion lift over a collaborative filtering approach in real-world A/B testing across more than 40 million users on the major Ticketmaster email recommendation product.</p>
    <p><strong>Categories:</strong> Scalability, Contextual Bandits, Periodical Recommendations, Exploitation-Exploration, Feature Hashing, Cache Management, Learning Reduction, Collaborative Filtering, Evaluation Metrics, Real-World Applications, A/B Testing, Email Recommendations, Big Data Processing, High-Dimensional Features (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/195/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Ensemble Contextual Bandits for Personalized Recommendation (2014)</h3>
    <p><strong>Authors:</strong> Tao Li, Lei Li, Yexi Jiang, Liang Tang</p>
    <p>The cold-start problem has attracted extensive attention among various online services that provide personalized recommendation. Many online vendors employ contextual bandit strategies to tackle the so-called exploration/exploitation dilemma rooted from the cold-start problem. However, due to high-dimensional user/item features and the underlying characteristics of bandit policies, it is often difficult for service providers to obtain and deploy an appropriate algorithm to achieve acceptable and robust economic profit. In this paper, we explore ensemble strategies of multiple contextual bandit algorithms to obtain robust predicted click-through rate (CTR) of web objects. Specifically, the ensemble is acquired by aggregating different pulling policies of bandit algorithms, rather than forcing the agreement of prediction results or learning a unified predictive model. To this end, we employ a meta-bandit paradigm that places a hyper bandit over the base bandits, to explicitly explore/exploit the relative importance of base bandits based on user feedbacks. Extensive empirical experiments on two real-world data sets (news recommendation and online advertising) demonstrate the effectiveness of our proposed approach in terms of CTR.</p>
    <p><strong>Categories:</strong> Contextual Bandits, Ensemble Methods, Personalized Recommendation, Cold Start Problem, Exploration/Exploitation Dilemma, Meta-Bandit, Click-Through Rate (CTR), News Recommendation, Online Advertising, Evaluation Metrics, Practical Applications, Scalability (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/10/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>