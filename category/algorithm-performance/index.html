<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/explainability/">Explainability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/evaluation-metrics/">Evaluation Metrics</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reinforcement-learning/">Reinforcement Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/e-commerce/">E-commerce</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Informed Dataset Selection with ‘Algorithm Performance Spaces’ (2024)</h3>
    <p><strong>Authors:</strong> Lien Michiels, Lukas Wegmeth, Joeran Beel, Steffen Schulz</p>
    <p>When designing recommender-systems experiments, a key question that has been largely overlooked is the choice of datasets. In a brief survey of ACM RecSys papers, we found that authors typically justified their dataset choices by labelling them as public, benchmark, or ‘real-world’ without further explanation. We propose the Algorithm Performance Space (APS) as a novel method for informed dataset selection. The APS is an n-dimensional space where each dimension represents the performance of a different algorithm. Each dataset is depicted as an n-dimensional vector, with greater distances indicating higher diversity. In our experiment, we ran 29 algorithms on 95 datasets to construct an actual APS. Our findings show that many datasets, including most Amazon datasets, are clustered closely in the APS, i.e. they are not diverse. However, other datasets, such as MovieLens and Docear, are more dispersed. The APS also enables the grouping of datasets based on the solvability of the underlying problem. Datasets in the top right corner of the APS are considered ’solved problems’ because all algorithms perform well on them. Conversely, datasets in the bottom left corner lack well-performing algorithms, making them ideal candidates for new recommender-system research due to the challenges they present.</p>
    <p><strong>Categories:</strong> Algorithm Performance, Dataset Selection, Recommender Systems, Evaluation Metrics, Experimental Design, Real-World Applications, Research Methods (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1194/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Towards Green Recommender Systems: Investigating the Impact of Data Reduction on Carbon Footprint and Algorithm Performances (2024)</h3>
    <p><strong>Authors:</strong> Michela Milano, Cataldo Musto, Allegra De Filippo, Giuseppe Spillo, Giovanni Semeraro</p>
    <p>This work investigates the path toward green recommender systems by examining the impact of data reduction on both model performance and carbon footprint. In the pursuit of developing energy-efficient recommender systems, we investigated whether and how reducting training data impacts the performances of several representative recommendation models. In order to obtain a fair comparison, all the models were run based on the implementations available in a popular recommendation library, i.e., RecBole, and used the same experimental settings. Results indicate that: (a) data reduction can be a promising strategy to make recommender systems more sustainable at the cost of a lower accuracy; (b) training recommender systems with less data makes the suggestions more diverse and less biased. Overall, this study contributes to the ongoing discourse on the development of recommendation models that meet the principles of SDGs, laying the groundwork for the adoption of more sustainable practices in the field.</p>
    <p><strong>Categories:</strong> Sustainability, Energy Efficiency, Data Reduction Techniques, Algorithm Performance, Carbon Footprint, Recommendation Diversity, Bias in Recommendations, Recommender Systems (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1118/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Nonlinear Bandits Exploration for Recommendations (2023)</h3>
    <p><strong>Authors:</strong> Minmin Chen, Yi Su</p>
    <p>The paradigm of framing recommendations as (sequential) decision-making processes has gained significant interest. To achieve long-term user satisfaction, these interactive systems need to strikes a balance between exploitation (recommending high-reward items) and exploration (exploring uncertain regions for potentially better items). Classical bandit algorithms like Upper-Confidence-Bound and Thompson Sampling, and their contextual extensions with linear payoffs have exhibited strong theoretical guarantees and empirical success in managing the exploration-exploitation trade-off. Building efficient exploration-based systems for deep neural network powered real-world, large-scale industrial recommender systems remains under studied. In addition, these systems are often multi-stage, multi-objective and response time sensitive.  In this talk, we share our experience in addressing these challenges in building exploration based industrial recommender systems. Specifically, we adopt the Neural Linear Bandit algorithm, which effectively combines the representation power of deep neural networks, with the simplicity of linear bandits to incorporate exploration in DNN based recommender systems. We introduce  exploration capability to both the nomination and ranking stage of the industrial recommender system.  In the context of the ranking stage, we delve into the extension of this algorithm to accommodate the multi-task setup, enabling exploration in systems with multiple objectives. Moving on to the nomination stage, we will address the development of efficient bandit algorithms tailored to factorized bi-linear models. These algorithms play a crucial role in facilitating maximum inner product search, which is commonly employed in large-scale retrieval systems. We validate our algorithms and present findings from real-world live experiments.</p>
    <p><strong>Categories:</strong> Multi-Armed Bandits, Recommendation Systems, Deep Learning, Exploration, Exploitation, Neural Linear Bandit, Industrial Recommender Systems, Nomination Stage, Ranking Stage, Multi-Task Learning, Large Scale Systems, Real World Applications, Efficiency, Beyond Accuracy, Evaluation Metrics, Performance Evaluation, Algorithm Performance (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1012/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Performance of Hyperbolic Geometry Models in Top-N Recommendation Tasks (2020)</h3>
    <p><strong>Authors:</strong> Alexander Tuzhilin, Evgeny Frolov, Leyla Mirvakhabova, Valentin Khrulkov, Ivan Oseledets</p>
    <p>We introduce a simple autoencoder based on hyperbolic geometry for solving standard collaborative filtering problem. In contrast to many modern deep learning techniques, we build our solution using only a single hidden layer. Remarkably, even with such a minimalistic approach, we not only outperform the Euclidean counterpart but also achieve a competitive performance with respect to the current state-of-the-art. We additionally explore the effects of space curvature on the quality of hyperbolic models and propose an efficient data-driven method for estimating its optimal value.</p>
    <p><strong>Categories:</strong> Hyperbolic Geometry, Recommendation Systems, Collaborative Filtering, Autoencoder, Deep Learning Techniques, Top-N Recommendations, Algorithm Performance, Space Curvature, Optimization Methods, Performance Evaluation, Matrix Factorization, Real-World Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/580/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Asymmetric Bayesian Personalized Ranking for One-Class Collaborative Filtering (2019)</h3>
    <p><strong>Authors:</strong> Zhong Ming, Lin Li, Weike Pan, Shan Ouyang</p>
    <p>In this paper, we propose a novel preference assumption for modeling users’ one-class feedback such as ‘thumb up’ in an important recommendation problem called one-class collaborative filtering (OCCF). Specifically, we address a fundamental limitation of a recent symmetric pairwise preference assumption and propose a novel and first asymmetric one, which is able to make the preferences of different users more comparable. With the proposed asymmetric pairwise preference assumption, we further design a novel recommendation algorithm called asymmetric Bayesian personalized ranking (ABPR). Extensive empirical studies on two large and public datasets show that our ABPR performs significantly better than several state-of-the-art recommendation methods with either pointwise preference assumption or pairwise preference assumption. i>Presentation: Monday Poster Lunch</i</p>
    <p><strong>Categories:</strong> Asymmetric BPR, One-Class Collaborative Filtering, Bayesian Personalized Ranking, Preference Assumption, Asymmetric Pairwise Preferences, Real-World Applications, Evaluation Metrics, Algorithm Performance, Recommendation Systems, Cold Start (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/461/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches (2019)</h3>
    <p><strong>Authors:</strong> Dietmar Jannach, Paolo Cremonesi, Maurizio Ferrari Dacrema</p>
    <p>Deep learning techniques have become the method of choice for researchers working on algorithmic aspects of recommender systems. With the strongly increased interest in machine learning in general, it has, as a result, become difficult to keep track of what represents the state-of-the-art at the moment, e.g., for top-n recommendation tasks. At the same time, several recent publications point out problems in today’s research practice in applied machine learning, e.g., in terms of the reproducibility of the results or the choice of the baselines when proposing new models. In this work,we report the results of a systematic analysis of algorithmic proposals for top-n recommendation tasks. Specifically, we considered 18 algorithms that were presented at top-level research conferences in the last years. Only 7 of them could be reproduced based on the provided code. For these methods, it however turned out that 6 of them can be often outperformed with comparably simple heuristic methods based on nearest-neighbor techniques. The remaining one clearly outperformed the baselines but did not consistently outperform a well-tuned non-neural linear ranking method. Overall, our work sheds light on a number of potential problems in today’s machine learning scholarship and calls for improved scientific practices in this area. ,</p>
    <p><strong>Categories:</strong> Reproducibility, Recommendation Systems, Neural Networks, Evaluation Practices, Research Methods, Machine Learning, Deep Learning, Algorithm Performance, Reproducibility Crisis, Heuristic Methods (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/433/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Data Masking for Recommender Systems: Prediction Performance and Rating Hiding (2019)</h3>
    <p><strong>Authors:</strong> Martha Larson, Alan Hanjalic, Manel Slokom</p>
    <p>Data science challenges allow companies, and other data holders, to collaborate with the wider research community. In the area of recommender systems, the potential of such challenges to move forward the state of the art is limited due to concerns about releasing user interaction data. This paper investigates the potential of privacy-preserving data publishing for supporting recommender system challenges. We propose a data masking algorithm, Shuffle-NNN, with two steps: Neighborhood selection and value swapping. Neighborhood selection preserves valuable item similarity information. The data shuffling technique hides (ie, changes) ratings of users for individual items. Our experimental results demonstrate that the relative performance of algorithms, which is the key property that a data science challenge must measure, is comparable between the original data and the data masked with Shuffle-NNN.</p>
    <p><strong>Categories:</strong> Privacy-Preserving Techniques, Algorithm Design &amp; Implementation, Recommender Systems Challenges, Data Processing in Recommendations, Item Similarity, Privacy Concerns, Rating Hiding, Data Publishing, Recommendation Evaluation, Algorithm Performance (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/516/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Word2vec applied to Recommendation: Hyperparameters Matter (2018)</h3>
    <p><strong>Authors:</strong> Jimena Royo-Letelier, Hugo Caselles-Dupré, Florian Lesaint</p>
    <p>Skip-gram with negative sampling, a popular variant of Word2vec originally designed and tuned to create word embeddings for Natural Language Processing, has been used to create item embeddings with successful applications in recommendation. While these fields do not share the same type of data, neither evaluate on the same tasks, recommendation applications tend to use the same already tuned hyperparameters values, even if optimal hyperparameters values are often known to be data and task dependent. We thus investigate the marginal importance of each hyperparameter in a recommendation setting through large hyperparameter grid searches on various datasets. Results reveal that optimizing neglected hyperparameters, namely negative sampling distribution, number of epochs, subsampling parameter and window-size, significantly improves performance on a recommendation task, and can increase it by an order of magnitude. Importantly, we find that optimal hyperparameters configurations for Natural Language Processing tasks and Recommendation tasks are noticeably different.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Word Embeddings, Collaborative Filtering, Algorithm Performance, Hyperparameter Optimization, Item Embeddings, Cross-Domain Applications, Beyond Accuracy, Explicit Feedback, Implicit Feedback, Domain Adaptation (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/407/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Who is Really Affected by Fraudulent Reviews? (2018)</h3>
    <p><strong>Authors:</strong> Maria Soledad Pera, Anu Shrestha, Francesca Spezzano</p>
    <p>We present the results of an initial analysis conducted on a real-life setting to quantify the effect of shilling attacks on recommender systems. We focus on both algorithm performance as well as the types of users who are most affected by these attacks.</p>
    <p><strong>Categories:</strong> Fraudulent Reviews, Real-World Applications, Recommendation Systems, User Impact, Algorithm Performance, Vulnerable Users, Fraud Detection, Recommendation Evaluation, Trust and Credibility, Personalization (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/425/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>