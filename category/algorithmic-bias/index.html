<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reinforcement-learning/">Reinforcement Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/evaluation-metrics/">Evaluation Metrics</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/evaluation-methods/">Evaluation Methods</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Do Recommender Systems Promote Local Music? A Reproducibility Study Using Music Streaming Data (2024)</h3>
    <p><strong>Authors:</strong> Manuel Moussallam, Kristina Matrosova, Thomas Louail, Lilian Marey, Guillaume Salha-Galvan, Olivier Bodini</p>
    <p>This paper examines the influence of recommender systems on local music representation, discussing prior findings from an empirical study on the LFM-2b public dataset. This prior study argued that different recommender systems exhibit algorithmic biases shifting music consumption either towards or against local content. However, LFM-2b users do not reflect the diverse audience of music streaming services. To assess the robustness of this study’s conclusions, we conduct a comparative analysis using proprietary listening data from a global music streaming service, which we publicly release alongside this paper. We observe significant differences in local music consumption patterns between our dataset and LFM-2b, suggesting that caution should be exercised when drawing conclusions on local music based solely on LFM-2b. Moreover, we show that the algorithmic biases exhibited in the original work vary in our dataset, and that several unexplored model parameters can significantly influence these biases and affect the study’s conclusion on both datasets. Finally, we discuss the complexity of accurately labeling local music, emphasizing the risk of misleading conclusions due to unreliable, biased, or incomplete labels.</p>
    <p><strong>Categories:</strong> Recommendation Systems Analysis, Music Streaming, Reproducibility Analysis, Algorithmic Bias, Dataset Comparison, Content Labeling, Evaluation Metrics, Real-World Applications, Industry Data, Model Parameters (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1121/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>It’s Not You, It’s Me: The Impact of Choice Models and Ranking Strategies on Gender Imbalance in Music Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Michael Ekstrand, Andres Ferraro, Christine Bauer</p>
    <p>As recommender systems are prone to various biases, mitigation approaches are needed to ensure that recommendations are fair to various stakeholders. One particular concern in music recommendation is artist gender fairness. Recent work has shown that the gender imbalance in the sector translates to the output of music recommender systems, creating a feedback loop that can reinforce gender biases over time. In this work, we examine whether algorithmic strategies or user behavior are a greater contributor to ongoing improvement (or loss) in fairness as models are repeatedly re-trained on new user feedback data. We simulate this repeated process to investigate the effects of ranking strategies and user choice models on gender fairness metrics. We find re-ranking strategies have a greater effect than user choice models on recommendation fairness over time</p>
    <p><strong>Categories:</strong> Re-ranking, Recommendation Algorithms, Music, Gender Imbalance, Bias Mitigation, Fairness Metrics, Algorithmic Bias, Feedback Loops, Evaluation of Algorithms, Model Retraining (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1094/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Oh, Behave! Country Representation Dynamics Created by Feedback Loops in Music Recommender Systems (2024)</h3>
    <p><strong>Authors:</strong> Dominik Kowald, Markus Schedl, Oleg Lesota, Jonas Geiger, Max Walder</p>
    <p>Recent work suggests that music recommender systems are prone to disproportionally frequent recommendations of music from countries more prominently represented in the training data, notably the US. However, it remains unclear to what extent feedback loops in music recommendation influence the dynamics of such imbalance. In this work, we investigate the dynamics of representation of local (i.e., country-specific) and US-produced music in user profiles and recommendations. To this end, we conduct a feedback loop simulation study using the standardized LFM-2b dataset. The results suggest that most of the investigated recommendation models decrease the proportion of music from local artists in their recommendations. Furthermore, we find that models preserving average proportions of US and local music do not necessarily provide country-calibrated recommendations. We also look into popularity calibration and, surprisingly, find that the most popularity-calibrated model in our study (ItemKNN) provides the least country-calibrated recommendations. In addition, users from less represented countries (e.g., Finland) are, in the long term, most affected by the under-representation of their local music in recommendations.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Music Recommender, Country Representation, User Behavior, Feedback Loops, Bias, Fairness, Algorithmic Bias, Empirical Study, Real World Applications, Cultural Dynamics, Calibration, Evaluation Metrics, Performance Analysis, User Impact (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1099/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Reproducibility of Multi-Objective Reinforcement Learning Recommendation: Interplay between Effectiveness and Beyond-Accuracy Perspectives (2023)</h3>
    <p><strong>Authors:</strong> Vincenzo Paparella, Ludovico Boratto, Vito Walter Anelli, Tommaso Di Noia</p>
    <p>Providing effective suggestions is of predominant importance for successful Recommender Systems (RSs). Nonetheless, the need of accounting for additional multiple objectives has become prominent, from both the final users’ and the item providers’ points of view. This need has led to a new class of RSs, called Multi-Objective Recommender Systems (MORSs). These systems are designed to provide suggestions by considering multiple (conflicting) objectives simultaneously, such as diverse, novel, and fairness-aware recommendations. In this work, we reproduce a state-of-the-art study on MORSs that exploits a reinforcement learning agent to satisfy three objectives, i.e., accuracy, diversity, and novelty of recommendations. The selected study is one of the few MORSs where the source code and datasets are released to ensure the reproducibility of the proposed approach. Interestingly, we find that some challenges arise when replicating the results of the original work, due to the nature of multiple-objective problems. We also extend the evaluation of the approach to analyze the impact of improving user-centred objectives of recommendations (i.e., diversity and novelty) in terms of algorithmic bias. To this end, we take into consideration both popularity and category of the items. We discover some interesting trends in the recommendation performance according to different evaluation metrics. In addition, we see that the multi-objective reinforcement learning approach is responsible for increasing the bias disparity in the output of the recommendation algorithm for those items belonging to positively/negatively biased categories. We publicly release datasets and codes in the following GitHub repository: https://anonymous.4open.science/r/MORS_reproducibility-BD60</p>
    <p><strong>Categories:</strong> Recommender Systems, Multi-Objective Recommender Systems, Reinforcement Learning, Reproducibility, Algorithmic Bias, Diversity of Recommendations, Novelty, Fairness-aware Recommendations, Beyond Accuracy, Evaluation Metrics, Multi-Objective Optimization, Open Source (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/946/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Adversary or Friend? An adversarial Approach to Improving Recommender Systems (2022)</h3>
    <p><strong>Authors:</strong> Pannaga Shivaswamy, Dario Garcia Garcia</p>
    <p>Typical recommender systems models are trained to have good average performance across all users or items. In practice, this results in model performance that is good for some users but sub-optimal for many users. In this work, we consider adversarially trained machine learning models and extend them to recommender systems problems. The adversarial models are trained with no additional demographic or other information than already available to the learning algorithm. We show that adversarially reweighted learning models give more emphasis to dense areas of the feature-space that incur high loss during training. We show that a straightforward adversarial model adapted to recommender systems can fail to perform well and that a carefully designed adversarial model can perform much better. The proposed models are trained using a standard gradient descent/ascent approach that can be easily adapted to many recommender problems. We compare our results with an inverse propensity weighting based baseline that also works well in practice. We delve deep into the underlying experimental results and show that, for the users who are under-served by the baseline model, the adversarial models can achieve significantly better results.</p>
    <p><strong>Categories:</strong> Adversarial Training, Recommender Systems, Machine Learning, Recommendation Algorithms, User-Specific Recommendations, Improving Recommendation Accuracy, Fairness in AI, Bias Mitigation, Beyond Accuracy, Algorithmic Fairness, Optimization Techniques, Fairness in Recommendations, Bias in AI, Algorithmic Bias (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/748/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>User Bias in Beyond-Accuracy Measurement of Recommendation Algorithms (2021)</h3>
    <p><strong>Authors:</strong> Ningxia Wang, Li Chen</p>
    <p>There are various biases in recommender systems. Recognizing biases, as well as unfairness caused by problematic biases, is the first step of system optimization. Related studies on algorithmic biases are mainly from the perspective of either items or users. For the latter (we call it “algorithmic user bias”), existing works have considered algorithms’ accuracy performances measured by accuracy metrics like RMSE. However, algorithmic user biases in beyond-accuracy measurements have rarely been studied, even though beyond-accuracy oriented recommendation algorithms have been increasingly investigated, with the purpose of breaking through the personalization limits of traditional accuracy-oriented algorithms (such as the typical “filter bubble” phenomenon). To fill in the research gap, in this work, we employ a large-scale survey dataset collected from a commercial platform, in which more than 11,000 users’ ratings on the recommendation’s 5 performance objectives (i.e., relevance, diversity, novelty, unexpectedness, and serendipity) and 8 kinds of user characteristics (i.e., gender, age, big-5 personality traits, and curiosity) are available. We study user biases of four algorithms (i.e., HOT, Rel-CF, Nov-CF, and Ser-CF) in terms of those five measurements between user groups of the eight user characteristics. We further look into users’ behavior patterns like the preference of using more positive ratings, in order to interpret the observed biases. Finally, based on the observed algorithmic user bias and users’ behavior patterns, we analyze the possible factors leading to the biases and recognize problematic biases that may lead to unfairness.</p>
    <p><strong>Categories:</strong> Algorithmic Bias, User Bias, Beyond Accuracy, Recommendation Algorithms, Algorithm Evaluation, User Characteristics, Fairness, Survey Methods, Diversity, Personalization (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/669/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>The Idiosyncratic Effects of Adversarial Training on Bias in Personalized Recommendation Learning (2021)</h3>
    <p><strong>Authors:</strong> Felice Antonio Merra, Tommaso Di Noia, Vito Walter Anelli</p>
    <p>Recently, recommendation systems have been proven to be susceptible to malicious perturbations of the model weights. To overcome this vulnerability, Adversarial Regularization emerged as one of the most effective solutions. Interestingly, the technique not only robustifies the model, but also significantly increases its accuracy. To date, unfortunately, the effect of Adversarial Regularization beyond-accuracy evaluation dimensions is unknown. This paper sheds light on these aspects and investigates how Adversarial Regularization impacts the amplification of popularity bias, and the deterioration of novelty and coverage of the recommendation list. The results highlight that, with imbalanced data distribution, Adversarial Regularization amplifies the popularity bias. Moreover, the empirical validation on five datasets confirms that it degrades the diversity and novelty of the generated recommendation. Code and data are available at https://github.com/sisinflab/The-Idiosyncratic-Effects-of-Adversarial-Training.</p>
    <p><strong>Categories:</strong> Adversarial Training, Adversarial Regularization, Recommendation Systems, Personalized Recommendations, Bias in AI, Algorithmic Bias, Popularity Bias, Diversity of Recommendations, Novelty of Recommendations, Coverage of Recommendations, Beyond Accuracy, Imbalanced Data Distribution, Adversarial Attacks, Quality of Recommendations (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/698/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Keeping Dataset Biases out of the Simulation: A Debiased Simulator for Reinforcement Learning based Recommender Systems (2020)</h3>
    <p><strong>Authors:</strong> Herke van Hoof, Jin Huang, Maarten de Rijke, Harrie Oosterhuis</p>
    <p>Reinforcement learning for recommendation (RL4Rec) methods are increasingly receiving attention as an effective way to improve long-term user engagement. However, applying RL4Rec online comes with risks: exploration may lead to periods of detrimental user experience. Moreover, few researchers have access to real-world recommender systems. Simulations have been put forward as a solution where user feedback is simulated based on logged historical user data, thus enabling optimization and evaluation without being run online. While simulators do not risk the user experience and are widely accessible, we identify an important limitation of existing simulation methods. They ignore the interaction biases present in logged user data, and consequently, these biases affect the resulting simulation. As a solution to this issue, we introduce a debiasing step in the simulation pipeline, which corrects for the biases present in the logged data before it is used to simulate user behavior. To evaluate the effects of bias on RL4Rec simulations, we propose a novel evaluation approach for simulators that considers the performance of policies optimized with the simulator. Our results reveal that the biases from logged data negatively impact the resulting policies, unless corrected for with our debiasing method. While our debiasing methods can be applied to any simulator, we make our complete pipeline publicly available as the Simulator for OFfline leArning and evaluation (SOFA): the first simulator that accounts for interaction biases prior to optimization and evaluation.</p>
    <p><strong>Categories:</strong> Reinforcement Learning, Recommendation Systems, Bias Correction, Simulation Methods, Evaluation Metrics, Offline Learning, User Experience, Recommender Evaluation, Algorithmic Bias, Policy Optimization (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/541/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Deep Bayesian Bandits: Exploring in Online Personalized Recommendations (2020)</h3>
    <p><strong>Authors:</strong> Dalin Guo, Sourav Das, Michael Kneier, Pranay Kumar Kumar Myana, Alykhan Tejani, Sofia Ira Ira Ktena, Ferenc Huszar, Wenzhe Shi</p>
    <p>Recommender systems trained in a continuous learning fashion are plagued by the feedback loop problem, also known as algorithmic bias. This causes a newly trained model to act greedily and favor items that have already been engaged by users. This behavior is particularly harmful in personalised ads recommendations, as it can also cause new campaigns to remain unexplored. Exploration aims to address this limitation by providing new information about the environment, which encompasses user preference, and can lead to higher long-term reward. In this work, we formulate a display advertising recommender as a contextual bandit and implement exploration techniques that require sampling from the posterior distribution of click-through-rates in a computationally tractable manner. Traditional large-scale deep learning models do not provide uncertainty estimates by default. We approximate these uncertainty measurements of the predictions by employing a bootstrapped model with multiple heads and dropout units. We benchmark a number of different models in an offline simulation environment using a publicly available dataset of user-ads engagements. We test our proposed deep Bayesian bandits algorithm in the offline simulation and online AB setting with large-scale production traffic, where we demonstrate a positive gain of our exploration model.</p>
    <p><strong>Categories:</strong> Recommender Systems, Contextual Bandits, Bayesian Methods, Personalization, Deep Learning, Exploration vs Exploitation, Algorithmic Bias, Display Advertising, Uncertainty Estimation, Feedback Loop, Online AB Testing, Large-Scale Applications. (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/571/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>How Algorithmic Confounding in Recommendation Systems Increases Homogeneity and Decreases Utility (2018)</h3>
    <p><strong>Authors:</strong> Allison Chaney, Brandon Stewart, Barbara Engelhardt</p>
    <p>Recommendation systems are ubiquitous and impact many domains; they have the potential to influence product consumption, individuals’ perceptions of the world, and life-altering decisions. These systems are often part of a feedback loop: models are evaluated or trained with observed data that are confounded because users are already exposed to algorithmic recommendations. In this paper, we use simulations to show that using confounded data may amplify homogenization of user behavior without increasing utility.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Diversity of Recommendations, Feedback Loops, Algorithmic Bias, Confounded Data, Beyond Accuracy, User Behavior, Ethical Considerations (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/344/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>