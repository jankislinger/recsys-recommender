<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/bias-mitigation/">Bias Mitigation</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/offline-evaluation/">Offline Evaluation</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/real-world-applications/">Real-world Applications</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>What We Evaluate When We Evaluate Recommender Systems: Understanding Recommender Systems’ Performance using Item Response Theory (2023)</h3>
    <p><strong>Authors:</strong> Yang Liu, Alan Medlar, Dorota Glowacka</p>
    <p>Current practices in offline evaluation use rank-based metrics to measure the quality of recommendation lists. This approach has practical benefits as it centers assessment on the output of the recommender system and, therefore, measures performance from the perspective of end-users. However, this methodology neglects how recommender systems more broadly model user preferences, which is not captured by only considering the top-n recommendations. In this article, we use item response theory (IRT), a family of latent variable models used in psychometric assessment, to gain a comprehensive understanding of offline evaluation. We used IRT to jointly estimate the latent abilities of 51 recommendation algorithms and the characteristics of 3 commonly used benchmark data sets. For all data sets, the latent abilities estimated by IRT suggest that higher scores from traditional rank-based metrics do not reflect improvements in modeling user preferences. Furthermore, we show the top-n recommendations with the most discriminatory power are biased towards lower difficulty items, leaving much room for improvement. Lastly, we highlight the role of popularity in evaluation by investigating how user engagement and item popularity influence recommendation difficulty.</p>
    <p><strong>Categories:</strong> Evaluation Metrics, Item Response Theory, Recommendation Algorithms, Multi-Algorithm Analysis, User Modeling, Latent Variable Models, Offline Evaluation, Beyond Accuracy, Benchmark Datasets, Popularity Bias, Difficulty Modeling, User Engagement (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/889/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Countering Popularity Bias by Regularizing Score Differences (2022)</h3>
    <p><strong>Authors:</strong> Sung Min Cho, Wondo Rhee, Bongwon Suh</p>
    <p>Recommendation system often suffers from popularity bias. Often the training data inherently exhibits long-tail distribution in item popularity (data bias). Moreover, the recommendation systems could give unfairly higher recommendation scores to popular items even among items a user equally liked, resulting in over-recommendation of popular items (model bias). In this study we propose a novel method to reduce the model bias while maintaining accuracy by directly regularizing the recommendation scores to be equal across items a user preferred. Akin to contrastive learning, we extend the widely used pairwise loss (BPR loss) which maximizes the score differences between preferred and unpreferred items, with a regularization term that minimizes the score differences within preferred and unpreferred items, respectively, thereby achieving both high debias and high accuracy performance with no additional training. To test the effectiveness of the proposed method, we design an experiment using a synthetic dataset which induces model bias with baseline training; we showed applying the proposed method resulted in drastic reduction of model bias while maintaining accuracy. Comprehensive comparison with earlier debias methods showed the proposed method had advantages in terms of computational validity and efficiency. Further empirical experiments utilizing four benchmark datasets and four recommendation models indicated the proposed method showed general improvements over performances of earlier debias methods. We hope that our method could help users enjoy diverse recommendations promoting serendipitous findings. Code available at https://github.com/stillpsy/popbias.</p>
    <p><strong>Categories:</strong> Popularity Bias, Recommendation Systems, Regularization, Pairwise Loss, Loss Functions, Evaluation Metrics, Fairness, Diversity of Recommendations, Model Accuracy, Bias Mitigation, Benchmark Datasets (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/753/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>On the Discriminative power of Hyper-parameters in Cross-Validation and How to Choose Them (2019)</h3>
    <p><strong>Authors:</strong> Claudio Pomo, Tommaso Di Noia, Vito Walter Anelli, Azzurra Ragone, Eugenio Di Sciascio</p>
    <p>Hyper-parameters tuning is a crucial task to make a model perform at its best. However, despite the well-established methodologies, some aspects of the tuning remain unexplored. As an example, it may affect not just accuracy but also novelty as well as it may depend on the adopted dataset. Moreover, sometimes it could be sufficient to concentrate on a single parameter only (or a few of them) instead of their overall set. In this paper we report on our investigation on hyper-parameters tuning by performing an extensive 10-Folds Cross-Validation on MovieLens and Amazon Movies for three well-known baselines: User-kNN, Item-kNN, BPR-MF. We adopted a grid search strategy considering approximately 15 values for each parameter, and we then evaluated each combination of parameters in terms of accuracy and novelty. We investigated the discriminative power of nDCG, Precision, Recall, MRR, EFD, EPC, and, finally, we analyzed the role of parameters on model evaluation for Cross-Validation. i>Presentation: Tuesday Poster Lunch</i</p>
    <p><strong>Categories:</strong> Model Optimization, Recommendation Systems, Cross-Validation, Grid Search, Evaluation Metrics, Matrix Factorization, Collaborative Filtering, Movies, Benchmark Datasets, User-kNN, Item-kNN, BPR-MF (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/480/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Field-aware Probabilistic Embedding Neural Network for CTR Prediction (2018)</h3>
    <p><strong>Authors:</strong> Yu Jinkai, Huifeng Guo, Xiuqiang He, Jiajin Li, Shengyu Zhang, Weiwen Liu, Ruiming Tang</p>
    <p>For Click-Through Rate (CTR) prediction, Field-aware Factorization Machines (FFM) have exhibited great effectiveness by considering field information. However, it is also observed that FFM suffers from the overfitting problem in many practical scenarios. In this paper, we propose a Field-aware Probabilistic Embedding Neural Network (FPENN) model with both good generalization ability and high accuracy. FPENN estimates the probability distribution of the field-aware embedding rather than using the single point estimation (the maximum a posteriori estimation) to prevent overfitting. Both low-order and high-order feature interactions are considered to improve the accuracy. FPENN consists of three components, i.e., FPE component, Quadratic component and Deep component. FPE component outputs probabilistic embedding to the other two components, where various confidence levels for feature embeddings are incorporated to enhance the robustness and the accuracy. Quadratic component is designed for extracting low-order feature interactions, while Deep component aims at capturing high-order feature interactions. Experiments are conducted on two benchmark datasets, Avazu and Criteo. The results confirm that our model alleviates the overfitting problem while has a higher accuracy.</p>
    <p><strong>Categories:</strong> Field-aware Factorization Machines, Click-Through Rate Prediction, Probabilistic Embeddings, Neural Networks, High-Order Interaction, Low-Order Interaction, Generalization, Overfitting, Accuracy Improvement, Benchmark Datasets, Feature Interactions, Recommendation Systems, Ad Click Prediction, Web Systems, Embedding Techniques (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/381/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>A Coverage-Based Approach to Recommendation Diversity On Similarity Graph (2016)</h3>
    <p><strong>Authors:</strong> Nicolas Usunier, Shameem A Puthiya Parambath, Yves Grandvalet</p>
    <p>We consider the problem of generating diverse, personalized recommendations such that a small set of recommended items covers a broad range of the user’s interests. We represent items in a similarity graph, and we formulate the relevance/diversity trade-off as finding a small set of unrated items that best covers a subset of items positively rated by the user. In contrast to previous approaches, our method does not rely on an explicit trade-off between a relevance objective and a diversity objective, as the estimations of relevance and diversity are implicit in the coverage criterion. We show on several benchmark datasets that our approach compares favorably to the state-of-the-art diversification methods according to various relevance and diversity measures.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Diversity of Recommendations, Personalized Recommendations, Similarity Graphs, Coverage-Based Approach, Relevance vs. Diversity Trade-off, Evaluation Metrics, State-of-the-Art Methods, Graph-Based Methods, Benchmark Datasets (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/161/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>