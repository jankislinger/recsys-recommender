<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">Benchmarking</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/evaluation-metrics/">Evaluation Metrics</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/explainability/">Explainability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Revisiting BPR: A Replicability Study of a Common Recommender System Baseline (2024)</h3>
    <p><strong>Authors:</strong> Aleksandr Milogradskii, Oleg Lashinin, Sergey Kolesnikov, Alexander P, Marina Ananyeva</p>
    <p>Bayesian Personalized Rank (BPR), a collaborative filtering approach based on matrix factorization, frequently serves as a benchmark for recommender systems research. However, numerous studies often overlook the nuances of BPR implementation, claiming that it performs worse than newly proposed methods across various tasks. In this paper, we thoroughly examine the features of the BPR model, indicating their impact on its performance, and investigate open-source BPR implementations. Our analysis reveals inconsistencies between these implementations and the original BPR paper, leading to a significant decrease in performance of up to 50% for specific implementations. Furthermore, through extensive experiments on real-world datasets under modern evaluation settings, we demonstrate that with proper tuning of its hyperparameters, the BPR model can achieve performance levels close to state-of-the-art methods on the top-n recommendation tasks and even outperform them on specific datasets. Specifically, on the Million Song Dataset, the BPR model with hyperparameters tuning statistically significantly outperforms Mult-VAE by 10% in NDCG@100 with binary relevance function.</p>
    <p><strong>Categories:</strong> Matrix Factorization, Bayesian Personalized Ranking (BPR), Recommender Systems, Replicability Study, Performance Evaluation, Real-World Applications, Evaluation Metrics, Benchmarking, Hyperparameter Tuning, State-of-the-Art Methods, Methodology Improvement, Top-N Recommendations (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1131/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Multimodal Representation Learning for high-quality Recommendations in Cold-start and Beyond-Accuracy (2024)</h3>
    <p><strong>Authors:</strong> Marta Moscati</p>
    <p>Recommender systems (RS) traditionally leverage the large amount of user–item interaction data. This exposes RS to a lower recommendation quality in cold-start scenarios, as well as to a low recommendation quality in terms of beyond-accuracy evaluation metrics. State-of-the-art (SotA) models for cold-start scenarios rely on the use of side information on the items or the users, therefore relating recommendation to multimodal machine learning (ML). However, the most recent techniques from multimodal ML are often not applied to the domain of recommendation. Additionally, the evaluation of SotA multimodal RS often neglects beyond-accuracy aspects of recommendation. In this work, we outline research into designing novel multimodal RS based on SotA multimodal ML architectures for cold-start recommendation, and their evaluation and benchmark with preexisting multimodal RS in terms of accuracy and beyond-accuracy aspects of recommendation quality.</p>
    <p><strong>Categories:</strong> Multimodal Representation Learning, Cold Start, Beyond Accuracy, Recommender Systems, Machine Learning, Evaluation Metrics, Scalability, Architecture Design, Benchmarking, Side Information (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1144/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Context-based Entity Recommendation for Knowledge Workers: Establishing a Benchmark on Real-life Data (2024)</h3>
    <p><strong>Authors:</strong> Mahta Bakhshizadeh, Heiko Maus, Andreas Dengel</p>
    <p>In recent decades, recommender systems have undergone significant advancements, particularly in popular domains like movies, music, and product recommendations. Yet, progress has been notably slower in leveraging these systems for personal information management and knowledge assistance. In addition to challenges that complicate the adoption of recommender systems in this domain (such as privacy concerns, heterogeneous recommendation items, and frequent context switching), a significant barrier to progress in this area has been the absence of a standardized benchmark for researchers to evaluate their approaches. In response to this gap, this paper presents a benchmark built upon a publicly available dataset of real-life knowledge work in context (RLKWiC). This benchmark focuses on evaluating context-based entity recommendation, a use case for leveraging recommender systems to support knowledge workers in their daily digital tasks. By providing this benchmark, it is aimed to facilitate and accelerate research efforts in enhancing personal knowledge assistance through recommender systems.</p>
    <p><strong>Categories:</strong> Recommender Systems, Knowledge Management, Information Management, Real-world Applications, Privacy, Heterogeneous Data, Context-based Recommendations, Benchmarking, Evaluation Metrics, Dataset Development, Workflows, User Behavior, Personalization, Knowledge Workers (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1107/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Reproducibility of LLM-based Recommender Systems: the case study of P5 paradigm (2024)</h3>
    <p><strong>Authors:</strong> Marco Polignano, Cataldo Musto, Giovanni Semeraro, Pasquale Lops, Antonio Silletti</p>
    <p>Recommender systems field may greatly benefit of the availability of pretrained Large Language Models (LLMs), which can serve as the core mechanism to generate recommendations based on detailed user and item data, such as textual descriptions, user reviews, and metadata.  On one hand this new generation of LLM-based recommender systems paves the way to deal with traditional limitations, such as cold-start and data sparsity, but on the other hand this poses fundamental challenges for their accountability.  Reproducing experiments in the new context of LLM-based recommender systems is very challenging for several reasons. New approaches are published at an unprecedented pace, which makes difficult to have a clear picture of the main protocols and good practices in the experimental evaluation. Moreover, the lack of proper frameworks for LLM-based recommendation development and evaluation makes the process of benchmarking models complex and uncertain. In this work, we discuss the main issues encountered when trying to reproduce P5 (Pretrain, Personalized Prompt, and Prediction Paradigm), one of the first works unifying different recommendation tasks in a shared language modeling and natural language generation framework.  Starting from this study, we have developed OurFramework4LLM (anonymized name), a framework for training and evaluating LLMs, specifically for the recommendation task. It has been used to perform several experiments to assess the impact of different LLMs, personalization and novel set of more informative prompts on the overall performance of recommendations, in a fully reproducible environment.</p>
    <p><strong>Categories:</strong> Reproducibility, Large Language Models (LLMs), Recommender Systems, Framework Development, Benchmarking, Experimental Evaluation, P5 Paradigm, Natural Language Generation (NLG), Language Modeling, Cold-Start Problem, Data Sparsity, Personalization (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1130/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Supporting Knowledge Workers through Personal Information Assistance with Context-aware Recommender Systems (2024)</h3>
    <p><strong>Authors:</strong> Mahta Bakhshizadeh</p>
    <p>Recommender systems are extensively employed across various domains to mitigate information overload by providing personalized content. Despite their widespread use in sectors such as streaming, social networks, and e-commerce, utilizing them for personal information assistance is a comparatively novel application. This emerging application aims to develop intelligent systems capable of proactively providing knowledge workers with the most relevant information based on their context to enhance productivity. In this paper, we explore this innovative application by first defining the scope of our study, outlining the key objectives, and introducing main challenges. We then present our current results and progress, including a comprehensive literature review, the proposal of a framework, the collection of a pioneering dataset, and the establishment of a benchmark for evaluating a recommendation scenario on our published dataset. We also discuss our ongoing efforts and future research directions.</p>
    <p><strong>Categories:</strong> Recommender Systems, Knowledge Management, Context-aware, Personal Information Assistance, Productivity Enhancement, Framework Proposal, Benchmarking, Dataset Development, Information Overload, Human-Computer Interaction (HCI), Knowledge Workers, Literature Review (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1146/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>RecAD: Towards A Unified Library for Recommender Attack and Defense (2023)</h3>
    <p><strong>Authors:</strong> Chongming Gao, Wenjie Wang, Jianbai Ye, Xiangnan He, Changsheng Wang, Fuli Feng</p>
    <p>In recent years, recommender systems have become a ubiquitous part of our daily lives, while they suffer from a high risk of being attacked due to the growing commercial and social values. Despite significant research progress in recommender attack and defense, there is a lack of a widely-recognized benchmarking standard in the field, leading to unfair performance comparison and limited credibility of experiments. To address this, we propose RecAD, a unified library aiming at establishing an open benchmark for recommender attack and defense. RecAD takes an initial step to set up a unified benchmarking pipeline for reproducible research by integrating diverse datasets, standard source codes, hyper-parameter settings, running logs, attack knowledge, attack budget, and evaluation results. The benchmark is designed to be comprehensive and sustainable, covering both attack, defense, and evaluation tasks, enabling more researchers to easily follow and contribute to this promising field. RecAD will drive more solid and reproducible research on recommender systems attack and defense, reduce the redundant efforts of researchers, and ultimately increase the credibility and practical value of recommender attack and defense. The project and documents are released at https://github.com/gusye1234/recad.</p>
    <p><strong>Categories:</strong> Recommender Systems, Security, Attack, Defense, Benchmarking, Library, Reproducibility, Research Methodology, Open Source (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/941/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>The effect of third party implementations on reproducibility (2023)</h3>
    <p><strong>Authors:</strong> Ádám Tibor Czapp, Balázs Hidasi</p>
    <p>Reproducibility of recommender systems research has come under scrutiny during recent years. Along with works focusing on repeating experiments with certain algorithms, the research community has also started discussing various aspects of evaluation and how these affect reproducibility. We add a novel angle to this discussion by examining how unofficial third-party implementations could benefit or hinder reproducibility. Besides giving a general overview, we thoroughly examine six third-party implementations of a popular recommender algorithm and compare them to the official version on five public datasets. In the light of our alarming findings we aim to draw the attention of the research community to this neglected aspect of reproducibility.</p>
    <p><strong>Categories:</strong> Reproducibility, Recommender Systems, Implementation Details, Evaluation Methods, Research Practices, Third-Party Software, Research Methodology, Benchmarking, Empirical Analysis, Neglected Aspects, Practical Implications, Software Tools (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/951/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Dynamic Surrogate Switching: Sample-Efficient Search for Factorization Machine Configurations in Online Recommendations (2022)</h3>
    <p><strong>Authors:</strong> Jure Ferlež, Adi Schwartz, Naama Ziporin, Blaz Skrlj</p>
    <p>Hyperparameter optimization is the process of identifying the appropriate hyperparameter configuration of a given machine learning model with regard to a given learning task. For smaller data sets, an exhaustive search is possible; However, when the data size and model complexity increase, the number of configuration evaluations becomes the main computational bottleneck. A promising paradigm for tackling this type of problem is surrogate-based optimization. The main idea underlying this paradigm considers an incrementally updated model of the relation between the hyperparameter space and the output (target) space; the data for this model are obtained by evaluating the main learning engine, which is, for example, a factorization machine-based model. By learning to approximate the hyperparameter-target relation, the surrogate (machine learning) model can be used to score large amounts of hyperparameter configurations, exploring parts of the configuration space beyond the reach of direct machine learning engine evaluation. Commonly, a surrogate is selected prior to optimization initialization and remains the same during the search. We investigated whether dynamic switching of surrogates during the optimization itself is a sensible idea of practical relevance for selecting the most appropriate factorization machine-based models for large-scale online recommendation. We conducted benchmarks on data sets containing hundreds of millions of instances against established baselines such as Random Forest- and Gaussian process-based surrogates. The results indicate that surrogate switching can offer good performance while considering fewer learning engine evaluations.</p>
    <p><strong>Categories:</strong> Dynamic Surrogate Switching, Hyperparameter Optimization, Factorization Machines, Online Recommendations, Large-Scale Recommendations, Surrogate Models, Random Forest, Gaussian Process, Evaluation Metrics, Beyond Accuracy, Sample Efficiency, Search Strategies, Benchmarking, Real-World Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/831/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Learning to Ride a Buy-Cycle: A Hyper-Convolutional Model for Next Basket Repurchase Recommendation (2022)</h3>
    <p><strong>Authors:</strong> Ori Katz, Nir Zabari, Oren Barkan, Noam Koenigstein</p>
    <p>The problem of Next Basket Recommendation (NBR) addresses the challenge of recommending items for the next basket of a user, based on her sequence of prior baskets. In this paper, we focus on a variation of this problem in which we aim to predict repurchases, i.e. we wish to recommend a user only items she had purchased before. We coin this problem Next Basket Repurchase Recommendation (NBRR). Over the years, a variety of models have been proposed to address the problem of NBR, however, the problem of NBRR has been overlooked. Although being highly related problems, which are often solved by the same methods, the problem of repurchase recommendation calls for a different approach. In this paper, we share insights from our experience of facing the challenge of NBRR. In light of these insights, we propose a novel hyper-convolutional model to leverage the behavioral patterns of repeated purchases. We demonstrate the effectiveness of the proposed model on three publicly available datasets, where it is shown to outperform other existing methods across multiple metrics.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Next Basket Recommendation, Repurchase Recommendation, Hyper-Convolutional Models, Sequential Behavior Analysis, User Behavior Modeling, Evaluation Metrics, Retail Applications, Scalability, Benchmarking, Personalized Retail, Specialized Recommendation Problems (<i>deepseek-r1:70b</i>)</p>
    <p><a href="/recsys-recommender/response/760/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Modeling User Repeat Consumption Behavior for Online Novel Recommendation (2022)</h3>
    <p><strong>Authors:</strong> Cunxiang Yin, Leeven Luo, Yuncong Li, Yancheng He, Guoqiang Xu, Jing Cai, Sheng-hua Zhong</p>
    <p>Given a user’s historical interaction sequence, online novel recommendation suggests the next novel the user may be interested in. Online novel recommendation is important but underexplored. In this paper, we concentrate on recommending online novels to new users of an online novel reading platform, whose first visits to the platform occurred in the last seven days. We have two observations about online novel recommendation for new users. First, repeat novel consumption of new users is a common phenomenon. Second, interactions between users and novels are informative. To accurately predict whether a user will reconsume a novel, it is crucial to characterize each interaction at a fine-grained level. Based on these two observations, we propose a neural network for online novel recommendation, called NovelNet. NovelNet can recommend the next novel from both the user’s consumed novels and new novels simultaneously. Specifically, an interaction encoder is used to obtain accurate interaction representation considering fine-grained attributes of interaction, and a pointer network with a pointwise loss is incorporated into NovelNet to recommend previously-consumed novels. Moreover, an online novel recommendation dataset is built from a well-known online novel reading platform and is released for public use as a benchmark. Experimental results on the dataset demonstrate the effectiveness of NovelNet 1.</p>
    <p><strong>Categories:</strong> Repeat Consumption, User Behavior, Online Platforms, Neural Networks, Books, Cold Start, Recommendation Accuracy, Personalization, Interaction Encoding, Experimental Analysis, Dataset Construction, Benchmarking, Web Systems (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/776/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Are We Evaluating Rigorously? Benchmarking Recommendation for Reproducible Evaluation and Fair Comparison (2020)</h3>
    <p><strong>Authors:</strong> Zhu Sun, Jie Zhang, Di Yu, Xinghua Qu, Hui Fang, Jie Yang, Cong Geng</p>
    <p>With tremendous amount of recommendation algorithms proposed every year, one critical issue has attracted a considerable amount of attention: there are no effective benchmarks for evaluation, which leads to two major concerns, i.e., unreproducible evaluation and unfair comparison. This paper aims to conduct rigorous (i.e., reproducible and fair) evaluation for implicit-feedback based top-N recommendation algorithms. We first systematically review 85 recommendation papers published at eight top-tier conferences (e.g., RecSys, SIGIR) to summarize important evaluation factors, e.g., data splitting and parameter tuning strategies, etc. Through a holistic empirical study, the impacts of different factors on recommendation performance are then analyzed in-depth. Following that, we create benchmarks with standardized procedures and provide the performance of seven well-tuned state-of-the-arts across six metrics on six widely-used datasets as a reference for later study. Additionally, we release a user-friendly Python toolkit, which differs from existing ones in addressing the broad scope of rigorous evaluation for recommendation. Overall, our work sheds light on the issues in recommendation evaluation and lays the foundation for further investigation. Our code and datasets are available at GitHub (https://github.com/AmazingDD/daisyRec).</p>
    <p><strong>Categories:</strong> Recommender Systems, Methodology, Reproducibility in Science, Benchmarking, Evaluation Protocols, Implicit Feedback, Fairness in AI, Performance Metrics, Reproducible Research, Open Source Tools, Literature Review, RecSys, SIGIR, Research Methodology (<i>deepseek-r1:70b</i>)</p>
    <p><a href="/recsys-recommender/response/584/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>BETA-Rec: Build, Evaluate and Tune Automated Recommender Systems (2020)</h3>
    <p><strong>Authors:</strong> Iadh Ounis, Yaxiong Wu, Shangsong Liang, Zaiqiao Meng, Craig Macdonald, Guangtao Zeng, Richard McCreadie, Siwei Liu, Yucheng Liang, Qiang Zhang, Junhua Liang</p>
    <p>The field of recommender systems has rapidly evolved over the last few years, with significant advances made due to the in-flux of deep learning techniques. However, as a result of this rapid progress, escalating barriers-to-entry for new researchers is emerging. In particular, state-of-the-art approaches have fragmented into a large number of code-bases, often requiring different input formats, pre-processing stages and evaluating with different metric packages. Hence, it is time-consuming for new researchers to reach the point of having both an effective baseline set and a sound comparative environment. As a step towards elevating this problem, we have developed BETA-Rec, an open source project for Building, Evaluating and Tuning Automated Recommender Systems. BETA-Rec aims to provide a practical data toolkit for building end-to-end recommendation systems in a standardized way. It provides means for dataset preparation and splitting using common strategies, a generalized model engine for implementing recommender models using Pytorch with 9 models available out-of-the-box, as well as a unified training, validation, tuning and testing pipeline. Furthermore, BETA-Rec is designed to be both modular and extensible, enabling new models to be quickly added to the framework. It is deployable in a wide range of environments via pre-built docker containers and supports distributed parameter tuning using Ray. In this demo, we will illustrate the deployment and use of BETA-Rec for researchers and practitioners on a number of standard recommendation datasets. The source code of the project is available at github: https://github.com/beta-team/beta-recsys.</p>
    <p><strong>Categories:</strong> Recommender Systems, Machine Learning Methods, Open Source Tools, Pre-trained Models, Hyperparameter Optimization, Deployment, Practical Applications, Modular Frameworks, Distributed Computing, Data Preprocessing, Benchmarking (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/594/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Recommendations as Graph Explorations (2020)</h3>
    <p><strong>Authors:</strong> Marialena Kyriakidi, Yannis Ioannidis, Georgia Koutrika</p>
    <p>We argue that most recommendation approaches can be abstracted as a graph exploration problem. In particular, we describe a graph-theoretic framework with two primary parts: (a) a recommendation graph, modeling all the elements of an (application) domain from a recommendation perspective, including the subjects and objects of recommendations as well as the relationships between them; (b) a set of path operations, inferring new edges, i.e., implicit or unknown relationships, by traversing and combining paths on the graph. The resulting path algebra model provides an abstraction and a common foundation that is beneficial to three aspects of recommendations: (a) expressive power - expression and subsequent use of several significantly different, existing but also novel recommendation approaches is reduced to parameterizing a unique model; (b) usability - by capturing part of the recommendation mechanisms in the underlying path algebra semantics, specification of recommendation approaches becomes easier and less tedious; (c) processing speed - implementing recommender systems on top of graph engines opens up the door for several optimizations that speed up execution. We demonstrate the above benefits by expressing several categories of recommendation approaches in the path algebra model and benchmarking some of them in a recommender system implemented on top of Neo4J, a widely used graph system.</p>
    <p><strong>Categories:</strong> Graph Theory, Recommendation Systems, Domain Modeling, Path Operations, Relationship Inference, Expressive Power, Model Abstraction, Usability, Graph Databases, Benchmarking, Common Frameworks, Processing Speed, Optimization, Path Algebra, Performance Analysis (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/550/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>PDMFRec: A Decentralised Matrix Factorisation with Tunable User-centric Privacy (2019)</h3>
    <p><strong>Authors:</strong> James Geraci, Aonghus Lawlor, Francisco J. Peña, Panagiotis Symeonidis, Barry Smyth, Elias Z. Tragos, Erika Duriakova, Neil Hurley</p>
    <p>Conventional approaches to matrix factorisation (MF) typically rely on a centralised collection of user data for building a MF model. This approach introduces an increased risk when it comes to user privacy. In this short paper we propose an alternative, user-centric, privacy enhanced, decentralised approach to MF. Our method pushes the computation of the recommendation model to the user’s device, and eliminates the need to exchange sensitive personal information; instead only the loss gradients of local device-based) MF models need to be shared. Moreover, users can select the amount and type of information to be shared, for enhanced privacy. We demonstrate the effectiveness of this approach by considering different levels of user privacy in comparison with state-of-the-art alternatives. i>Presentation: Tuesday Poster Lunch</i</p>
    <p><strong>Categories:</strong> Matrix Factorisation, Recommendation Systems, Decentralised Computing, Privacy Preservation, Data Privacy, Edge Computing, Secure Communication, User-Centric Design, Evaluation Methods, Benchmarking, Privacy-Preserving Machine Learning (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/489/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>rrecsys: an R-package for prototyping recommendation algorithms (2016)</h3>
    <p><strong>Authors:</strong> Markus Zanker, Ludovik Çoba</p>
    <p>We introduce rrecsys, an open source extension package in R for rapid prototyping and intuitive assessment of recommender system algorithms. As the only currently available R package for recommender algorithms (recommenderlab) did not include popular algorithm implementations such as matrix factorization or One-class Collaborative Filtering algorithms we developed rrecsys as an easily accessible tool that can, for instance, be employed for interactive demonstrations when teaching. This package replicates state-of-the-art Collaborative Filtering algorithms for rating and binary data and we compare results with the Java-based LensKit implementation and recommederlab for the purpose of benchmarking the implementation. Therefore this work can also be seen as a contribution in the context of replication of algorithm implementations and reproduction of evaluation results.</p>
    <p><strong>Categories:</strong> R Package, Recommendation Algorithms, Matrix Factorization, Prototyping Tools, Recommender Systems, Collaborative Filtering, Open Source, Education, Benchmarking, Algorithm Implementation, Reproducibility (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/243/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Idomaar: A Framework for Multi-dimensional Benchmarking of Recommender Algorithms (2016)</h3>
    <p><strong>Authors:</strong> Andras Sereny, Davide Malagoli, Till Plumbaum, Andreas Lommatzsch, Benjamin Kille, Martha Larson, Frank Hopfgartner, Mario Scriminaci</p>
    <p>In real-world scenarios, recommenders face non-functional requirements of technical nature and must handle dynamic data in the form of sequential streams. Evaluation of recommender systems must take these issues into account in order to be maximally informative. In this paper, we present a framework called Idomaar which enables the efficient multi-dimensional benchmarking of recommender algorithms. Idomaar goes beyond current academic research practices by creating a realistic evaluation environment and computing both effectiveness and technical metrics for stream-based as well as set based evaluation. The potentials of the framework are illustrated in a scenario that focuses on the “research to prototyping to productization” cycle at a company. We show that Idomaar simplifies the testing with different configurations and supports the flexible integration of different data.</p>
    <p><strong>Categories:</strong> Framework, Benchmarking, Recommender Systems, Evaluation, Performance Analysis, Real-time Processing, Stream Data, Scalability, Prototyping, Productization, Data Integration, Best Practices (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/242/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>