<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/Large%20Language%20Models%20(LLMs)/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/Transfer%20Learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/Explainability/">Explainability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/Sequential%20Recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/AB%20Testing/">AB Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/Reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/Fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/AB%20Test/">AB Test</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/Reinforcement%20Learning/">Reinforcement Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/Data%20Sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">Cold Start</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/Evaluation%20Metrics/">Evaluation Metrics</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems (2024)</h3>
    <p><strong>Authors:</strong> Vojtěch Vančura, Milan Straka, Pavel Kordík</p>
    <p>Recommender systems often use text-side information to improve their predictions, especially in cold-start or zero-shot recommendation scenarios, where traditional collaborative filtering approaches cannot be used. Many approaches to text-mining side information for recommender systems have been proposed over recent years, with sentence Transformers being the most prominent one. However, these models are trained to predict semantic similarity without utilizing interaction data with hidden patterns specific to recommender systems. In this paper, we propose beeFormer, a framework for training sentence Transformer models with interaction data. We demonstrate that our models trained with beeFormer can transfer knowledge between datasets while outperforming not only semantic similarity sentence Transformers but also traditional collaborative filtering methods. We also show that training on multiple datasets from different domains accumulates knowledge in a single model, unlocking the possibility of training universal, domain-agnostic sentence Transformer models to mine text representations for recommender systems. We release the source code, trained models, and additional details allowing replication of our experiments at https://github.com/recombee/beeformer.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Text Mining/Side Information, Sentence Transformers, Interaction Data, Cold Start, Transfer Learning, Universal Models, Diversity of Recommendations, Beyond Accuracy (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1188/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>MARec: Metadata Alignment for cold-start Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Anirban Majumder, Wentao Lu, Julien Monteil, Volodymyr Vaskovych, Anton van den Hengel</p>
    <p>For many recommender systems the primary data source is a historical record of user clicks. The associated click matrix which is often very sparse, however, as the number of users x products can be far larger than the number of clicks, and such sparsity is accentuated in cold-start settings. The sparsity of the click matrix is the reason matrix factorization and autoencoders techniques remain highly competitive across collaborative filtering datasets. In this work, we propose a simple approach to address cold-start recommendations by leveraging content metadata, Metadata Alignment for cold-start Recommendation (MARec). we show that this approach can readily augment existing matrix factorization and autoencoder approaches, enabling a smooth transition to top performing algorithms in warmer set-ups. Our experimental results indicate three separate contributions: first, we show that our proposed framework largely beats SOTA results on 4 cold-start datasets with different sparsity and scale characteristics, with gains ranging from +8.4% to +53.8% on reported ranking metrics; second, we provide an ablation study on the utility of semantic features, and proves the additional gain obtained by leveraging such features ranges between +46.8% and +105.5%; and third, our approach is by construction highly competitive in warm set-ups, and we propose a closed-form solution outperformed by SOTA results by only 0.8% on average.</p>
    <p><strong>Categories:</strong> Cold Start, Metadata Alignment, Matrix Factorization, Collaborative Filtering, Data Sparsity, State of the Art, Ablation Study, Beyond Accuracy, Real-World Applications, Recommendation Systems, Novel Approach, Content-Based Filtering (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1044/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>A Multi-modal Modeling Framework for Cold-start Short-video Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Kun Gai, Han Li, Ruina Sun, Jingjian Lin, Xinghua Zhang, Gaode Chen, Qi Zhang, Jiangxia Cao, Yuezihan Jiang</p>
    <p>Short video has witnessed rapid growth in the past few years in multimedia platforms. To ensure the freshness of the videos, platforms receive a large number of user-uploaded videos every day, making collaborative filtering-based recommender methods suffer from the item cold-start problem (e.g., the new-coming videos are difficult to compete with existing videos). Consequently, increasing efforts tackle the cold-start issue from the content perspective, focusing on modeling the multi-modal preferences of users, a fair way to compete with new-coming and existing videos. However, recent studies ignore the existing gap between multi-modal embedding extraction and user interest modeling as well as the discrepant intensities of user preferences for different modalities. In this paper, we propose M3CSR, a multi-modal modeling framework for cold-start short video recommendation. Specifically, we preprocess content-oriented multi-modal features for items and obtain trainable category IDs by performing clustering. In each modality, we combine modality-specific cluster ID embedding and the mapped original modality feature as modality-specific representation of the item to address the gap. Meanwhile, M3CSR measures the user modality-specific intensity based on the correlation between modality-specific interest and behavioral interest and employs pairwise loss to further decouple user multi-modal interests. Extensive experiments on four real-world datasets demonstrate the superiority of our proposed model. The framework has been deployed on a billion-user scale short video application and has shown improvements in various commercial metrics within cold-start scenarios.</p>
    <p><strong>Categories:</strong> Cold Start, Short Video, Multi-Modal Modeling, Recommender Systems, Content-Based Recommendation, User Interest Modeling, Clustering, Embedding Extraction, Pairwise Loss, Real-World Applications, User Behavior Analysis, Feature Engineering (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1013/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>The Fault in Our Recommendations: On the Perils of Optimizing the Measurable (2024)</h3>
    <p><strong>Authors:</strong> Akshit Kumar, Yash Kanoria, Omar Besbes</p>
    <p>Recommendation systems play a pivotal role on digital platforms by curating content, products and services for users. These systems are widespread, and through customized recommendations, promise to match users with options they will like. To that end,  data on engagement is collected and used, with, e.g., measurements of clicks, but also  purchases or consumption times.   Most recommendation systems are ranking-based, where they rank and recommend items based on their predicted engagement.  However, the engagement signals  are often only a crude proxy for user utility, as data on the latter is rarely collected or available. This paper explores the following critical research question: By optimizing for measurable proxies, are recommendation systems at risk of significantly under-delivering on user utility? If that is indeed the case, how can one improve utility which is seldom measured? To study these questions, we introduce a model of repeated user consumption in which, at each interaction, users select between an outside option and the best option from a recommendation set. Our model accounts for user heterogeneity, with the majority preferring “popular” content, and a minority favoring “niche” content. The system initially lacks knowledge of individual user preferences but can learn these preferences through observations of users’ choices over time. Our theoretical and numerical analysis demonstrate that optimizing for engagement signals can lead to significant utility losses. Instead, we propose a utility-aware policy that initially recommends a mix of popular and niche content. We show that such a policy substantially improves  utility despite not measuring it. In fact, in the limit of a forward-looking platform with discount factor $\delta \to 1$, our utility-aware policy achieves the best of both worlds: near-optimal user utility and near-optimal engagement simultaneously. Our study elucidates  an important feature of recommendation systems; given the ability to suggest multiple items, one can perform significant exploration without incurring significant reductions in short term engagement. By recommending high-risk, high-reward items alongside popular items, systems can enhance discovery of high utility items without significantly affecting engagement.</p>
    <p><strong>Categories:</strong> Optimization, Evaluation Metrics, Recommendation Systems, User Behavior, Engagement, Machine Learning, Reinforcement Learning, Bandit Algorithms, Ranking Algorithms, Digital Platforms, Personalization, Cold Start, Exploration vs Exploitation, Long-term User Satisfaction, Scalability, Theoretical Modeling, Numerical Analysis, User Heterogeneity, Ethical Considerations in AI, Beyond Accuracy, High-Risk High-Reward Recommendations (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1062/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Prompt Tuning for Item Cold-start Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Jingchi Wang, Gaode Chen, Qi Zhang, Jingjian Lin, Yuezihan Jiang, Kaigui Bian, Wenhan Zhang, Peng Jiang, Yinjie Jiang</p>
    <p>The item cold-start problem is crucial for online recommender systems, as the success of the cold-start phase determines whether items can transition into popular ones. Prompt learning, a powerful technique used in natural language processing (NLP) to address zero- or few-shot problems, has been adapted for recommender systems to tackle similar challenges. However, existing methods typically rely on content-based properties or text descriptions for prompting, which we argue may be suboptimal for cold-start recommendations due to 1) semantic gaps with recommender tasks, 2) model bias caused by warm-up items contribute most of positive feedback to the model, which is the core of the cold-start problem that hinder the recommender quality on cold-start items. We propose to leverage high-value positive feedback, termed pinnacle feedback as prompt information, to simultaneously resolve the above two problems. We experimentally prove that comparing to content description proposed in existing works, the positive feedback is more suitable to serve as prompt information by bridging the semantic gaps. Besides, we propose item-wise personalized prompt networks to encode pinnaclce feedback to relieve the model bias by the positive feedback dominance problem. Extensive experiments on four real-world datasets demonstrate the superiority of our model over state-of-the-art methods. Moreover, PROMO has been successfully deployed on a popular short-video sharing platform, a billion-user scale commercial short-video application, achieving remarkable performance gains across various commercial metrics within cold-start scenarios.</p>
    <p><strong>Categories:</strong> Cold Start, Prompt Learning, Natural Language Processing (NLP), Recommender Systems, Semantic Gaps, Model Bias, High-Value Feedback, Personalized Prompts, Real-World Applications, A/B Testing, Evaluation Metrics, Feedback-Based Methods (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1055/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Multimodal Representation Learning for high-quality Recommendations in Cold-start and Beyond-Accuracy (2024)</h3>
    <p><strong>Authors:</strong> Marta Moscati</p>
    <p>Recommender systems (RS) traditionally leverage the large amount of user–item interaction data. This exposes RS to a lower recommendation quality in cold-start scenarios, as well as to a low recommendation quality in terms of beyond-accuracy evaluation metrics. State-of-the-art (SotA) models for cold-start scenarios rely on the use of side information on the items or the users, therefore relating recommendation to multimodal machine learning (ML). However, the most recent techniques from multimodal ML are often not applied to the domain of recommendation. Additionally, the evaluation of SotA multimodal RS often neglects beyond-accuracy aspects of recommendation. In this work, we outline research into designing novel multimodal RS based on SotA multimodal ML architectures for cold-start recommendation, and their evaluation and benchmark with preexisting multimodal RS in terms of accuracy and beyond-accuracy aspects of recommendation quality.</p>
    <p><strong>Categories:</strong> Multimodal Representation Learning, Cold Start, Beyond Accuracy, Recommender Systems, Machine Learning, Evaluation Metrics, Scalability, Architecture Design, Benchmarking, Side Information (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1144/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>The Role of Unknown Interactions in Implicit Matrix Factorization — A Probabilistic View (2024)</h3>
    <p><strong>Authors:</strong> Bart Goethals, Joey De Pauw</p>
    <p>Matrix factorization is a well-known and effective methodology for top-k list recommendation. It became widely known during the Netflix challenge in 2006, and since then, many adapted and improved versions have been published. A particularly interesting matrix factorization algorithm called iALS (for implicit Alternating Least Squares) adapts the method for implicit feedback, i.e.\ a setting where only a very small amount of positive labels are available along with a majority of unknown labels. Compared to the classical task of rating prediction, learning from implicit feedback is applicable to many more domains, as the data is more abundant and requires less effort to elicit from users. However, the sparsity, imbalance, and implicit nature of the signal also pose unique challenges to retrieving the most relevant items to recommend. We revisit the role of unknown interactions in implicit matrix factorization. Traditionally, all unknowns are interpreted as negative samples and their importance in the training objective is then down-weighted to balance them out with the known, positive interactions. Interestingly, by adapting a probabilistic view of matrix factorization, we can retain the unknown nature of these interactions by modelling them as either positive or negative. With this new formulation that better fits the underlying data, we gain improved performance on the downstream recommendation task without any computational overhead compared to the popular iALS method. This paper outlines the key insights needed to adapt iALS to use logistic regression. Furthermore, the popular full-rank EASE model is identified as a special case of iALS. With this knowledge, EASE was trivially adapted to use logistic regression as well. An extensive experimental evaluation on several real-world datasets demonstrates the effectiveness of our approach. Additionally, a discrepancy between the need for weighting between factorization and regression models is discovered, leading towards a better understanding of these methods.</p>
    <p><strong>Categories:</strong> Matrix Factorization, Implicit Feedback, Recommendation Systems, Algorithm Adaptation, Probabilistic Models, iALS, Real-World Applications, Cold Start, Evaluation Methods, Model Interpretation (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1069/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>The MovieLens Beliefs Dataset: Collecting Pre-Choice Data for Online Recommender Systems (2024)</h3>
    <p><strong>Authors:</strong> Guy Aridor, Duarte Goncalves, Daniel Kluver, Ruoyan Kong, Joseph Konstan</p>
    <p>An increasingly important aspect of designing recommender systems involves considering how recommendations will influence consumer choices. This paper addresses this issue by introducing a method for collecting user beliefs about un-experienced items – a critical predictor of choice behavior. We implemented this method on the MovieLens platform, resulting in a rich dataset that combines user ratings, beliefs, and observed recommendations. We document challenges to such data collection, including selection bias in response and limited coverage of the product space. This unique resource empowers researchers to delve deeper into user behavior and analyze user choices absent recommendations, measure the effectiveness of recommendations, and prototype algorithms that leverage user belief data, ultimately leading to more impactful recommender systems.</p>
    <p><strong>Categories:</strong> User Behavior, Pre-Choice Data, Recommender Systems, MovieLens, Dataset, User Feedback, Recommendation Algorithms, Research Methods, Movies, Data Collection Challenges, Recommender Systems Design, Evaluation Metrics, Algorithm Development, Belief Modeling, User Choices, Cold Start (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1106/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>A Tool for Explainable Pension Fund Recommendations using Large Language Models (2024)</h3>
    <p><strong>Authors:</strong> Edleno Silva de Moura, Eduardo Alves da Silva, Leandro Balby Marinho, Altigran Soares da Silva</p>
    <p>In this demo, we present a prototype tool designed to help financial advisors recommend private pension funds to investors based on their preferences, offering personalized investment suggestions. The tool leverages Large Language Models (LLMs), which enhance explainability by providing clear and understandable rationales for recommendations and effectively handles both sequential and cold-start scenarios. We outline the design, implementation, and results of a user-based evaluation using real-world data. The evaluation shows a high recommendation acceptance rate among financial advisors, highlighting the tool’s potential to improve decision-making in financial advisory services.</p>
    <p><strong>Categories:</strong> Explainability, Large Language Models, Financial Services, Wealth Management, Personalization, Cold Start, Pension Fund Recommendations, Decision-Making, User Evaluation, Real-World Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1205/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>“More to Read” at the Los Angeles Times: Solving a Cold Start Problem with LLMs to Improve Story Discovery (2024)</h3>
    <p><strong>Authors:</strong> David Kaufman, Franklin Horn, Aurelia Alston, Won You</p>
    <p>News publishers, who are seeking to grow their digital audience, face a challenge in providing relevant content recommendations for unregistered users arriving directly to article pages. In these cold start scenarios, classic techniques, like asking a user to register and select topics of interest, fall short. We present a contextual targeting approach that leverages the user’s current article choice itself as an implicit signal of user interests. We designed and developed an interface with recommendations to help users discover more articles. Our online A/B testing demonstrated that our models increased click-through rates by 39.4% over a popularity baseline. One of them, a large language model (LLM), generates relevant recommendations that balance immersion and novelty. We discuss the implications of using LLMs for responsibly enhancing user experiences while upholding editorial standards. We identify key opportunities in detecting nuanced user preferences and identifying and interrupting filter bubbles on news publisher sites.</p>
    <p><strong>Categories:</strong> Cold Start, Large Language Models (LLMs), News, Recommendation Systems, Implicit Feedback, Contextual Targeting, A/B Test, Online Media, User Engagement, Editorial Standards, Filter Bubbles, Content Diversity, Personalization (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1150/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Scene-wise Adaptive Network for Dynamic Cold-start Scenes Optimization in CTR Prediction (2024)</h3>
    <p><strong>Authors:</strong> Chao Tang, Wenhao Li, Jie Zhou, Shixiong Zhao, Chuan Luo, Kun Zhang</p>
    <p>In the realm of modern mobile E-commerce, providing users with nearby commercial service recommendations through location-based online services has become increasingly vital. While machine learning approaches have shown promise in multi-scene recommendation, existing methodologies often struggle to address cold-start problems in unprecedented scenes: the increasing diversity of commercial choices, along with the short online lifespan of scenes, give rise to the complexity of effective recommendations in online and dynamic scenes. In this work, we propose Scene-wise Adaptive Network (SwAN), a novel approach that emphasizes high-performance cold-start online recommendations for new scenes. Our approach introduces several crucial capabilities, including scene similarity learning, user-specific scene transition cognition, scene-specific information construction for the new scene, and enhancing the diverged logical information between scenes. We demonstrate SwAN’s potential to optimize dynamic multi-scene recommendation problems by effectively online handling cold-start recommendations for any newly arrived scenes. More encouragingly, SwAN has been successfully deployed in Company M’s online catering recommendation service, which serves millions of customers per day, and SwAN has achieved a 5.64% CTR index improvement relative to the baselines and a 5.19% increase in daily order volume proportion.</p>
    <p><strong>Categories:</strong> Scene-wise Adaptive Network (SwAN), Neural Networks, Cold Start, Multi-scene Recommendations, Dynamic Scenarios, Recommendation Systems, Mobile E-commerce, Catering Services, Location-based Services, Evaluation Metrics, Beyond Accuracy, Real-world Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1063/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Learned Ranking Function: From Short-term Behavior Predictions to Long-term User Satisfaction (2024)</h3>
    <p><strong>Authors:</strong> Yi Wu, Jennifer She, Daryl Chang, Lukasz Heldt, Zhe Zhao, Li Wei</p>
    <p>We present the Learned Ranking Function (LRF), a system that takes short-term user-item behavior predictions as input and outputs a slate of recommendations directly optimizing for long-term user satisfaction. Most previous work is based on optimizing hyper-parameters of a heuristic function. We propose to model the problem directly as a slate optimization problem with the objective of maximizing long-term user satisfaction. We also develop a new constraint optimization method that is able to improve the stability for multi-objective optimization. We evaluate our approach with live experiments and describe its deployment on YouTube.</p>
    <p><strong>Categories:</strong> Learned Ranking Function, Recommendation Systems, Slate Optimization, Long-term User Satisfaction, Multi-objective Optimization, Stability, Real-world Applications, A/B Testing, YouTube, Media/Video, Direct Ranking Methods, Cold Start (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1090/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>MODEM: Decoupling User Behavior for Shared-Account Video Recommendations on Large Screen Devices (2024)</h3>
    <p><strong>Authors:</strong> Jiang Li, Lantao Hu, Zhen Zhang, Xiang Feng, Yongqi Liu, Muyang Li</p>
    <p>In scenarios involving sequence recommendations on large screen devices, such as tablets or TVs, the equipment is often shared among multiple users. This sharing leads to a mixture of behaviors from different users, posing significant challenges to recommendation systems, especially when clear supervisory signals for distinguishing among users are absent. Current solutions tend to either operate in an unsupervised manner or rely on constructed supervisory signals that are not entirely reliable. Moreover, the peculiarities of short video recommendations in this context have not been thoroughly explored in existing research. In response to these challenges, this paper introduces Multi-User Contrastive Decoupling Model (MODEM), a novel short video recommendation model specifically designed for large screen devices. MODEM leverages an attention mechanism, grounded in session segmentation, to disentangle the intertwined user behavior histories. It also discriminates between the impacts of long and short viewing behaviors on short video recommendations by cross-analyzing sequences of both. Furthermore, we have developed a contrastive learning method to oversee the decoupling of user behaviors effectively. Our evaluations demonstrate noticeable improvements through both offline assessments within public datasets and online A/B testing within Kuaishou’s short video recommendation environment on large screen devices. Specifically, our online A/B tests resulted in a 0.55% increase in watch time. These results underscore MODEM’s efficacy in enhancing recommendation quality in shared account contexts.</p>
    <p><strong>Categories:</strong> Video Recommendations, Large Screen Devices, Shared Accounts, User Behavior Analysis, Attention Mechanism, Contrastive Learning, Session Segmentation, A/B Testing, Real-World Application, Beyond Accuracy, Cold Start (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1098/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Distillation Matters: Empowering Sequential  Recommenders to Match the Performance of Large Language Models (2024)</h3>
    <p><strong>Authors:</strong> Jiawei Chen, Yi Wan, Heng Tang, Bohao Wang, Feng Liu, Pengbo Wang, Jun Wang, Yu Cui</p>
    <p>Owing to their powerful semantic reasoning capabilities, Large Language Models (LLMs) have been effectively utilized as recommenders, achieving impressive performance. However, the high inference latency of LLMs significantly restricts their practical deployment. To address this issue, this work investigates knowledge distillation from cumbersome LLM-based recommendation models to lightweight conventional sequential models. It encounters three challenges: 1) the teacher’s knowledge may not always be reliable; 2) the capacity gap between the teacher and student makes it difficult for the student to assimilate the teacher’s knowledge; 3) divergence in semantic space poses a challenge to distill the knowledge from embeddings. To tackle these challenges, this work proposes a novel distillation strategy, DLLM2Rec, specifically tailored for knowledge distillation from LLM-based recommendation models to conventional sequential models. DLLM2Rec comprises: 1) Importance-aware ranking distillation, which filters reliable and student-friendly knowledge by weighting instances according to teacher confidence and student-teacher consistency; 2)  Collaborative embedding distillation integrates knowledge from teacher embeddings with collaborative signals mined from the data. Extensive experiments demonstrate the effectiveness of the proposed DLLM2Rec, boosting three typical sequential models with an average improvement of 47.97%, even enabling them to surpass LLM-based recommenders in some cases.</p>
    <p><strong>Categories:</strong> Knowledge Distillation, Sequential Recommenders, Large Language Models (LLMs), Recommendation Systems, Performance Improvement, Efficiency Optimization, Cold Start, Algorithmic Innovation, Collaborative Filtering, Scalability and Efficiency (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1028/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>A Pre-trained Zero-shot Sequential Recommendation Framework via Popularity Dynamics (2024)</h3>
    <p><strong>Authors:</strong> Hari Sundaram, Junting Wang, Praneet Rathi</p>
    <p>This paper proposes a novel pre-trained framework for zero-shot cross-domain  sequential recommendation without auxiliary information. While using auxiliary information (e.g., item descriptions) seems promising for cross-domain transfer, a cross-domain adaptation of sequential recommenders can be challenging when the target domain differs from the source domain—item descriptions are in different languages; metadata modalities (e.g., audio, image, and text) differ across source and target domains. If we can learn universal item representations independent of the domain type (e.g., groceries, movies), we can achieve zero-shot cross-domain transfer without auxiliary information. Our critical insight is that user interaction sequences highlight shifting user preferences via the popularity dynamics of interacted items. We present a pre-trained sequential recommendation framework: PrepRec, which utilizes a novel popularity dynamics-aware transformer architecture. Through extensive experiments on five real-world datasets, we show that PrepRec, without any auxiliary information, can zero-shot adapt to new application domains and achieve competitive performance compared to state-of-the-art sequential recommender models. In addition, we show that PrepRec complements existing sequential recommenders. With a simple post-hoc interpolation, PrepRec improves the performance of existing sequential recommenders on average by 11.8% in Recall@10 and 22% in NDCG@10. We provide an anonymized implementation of PrepRec at \url{https://anonymous.4open.science/r/PrepRec–128E/}.</p>
    <p><strong>Categories:</strong> Zero-shot Recommendation, Cross-domain Recommendation, Sequential Recommendation, Transformer Architecture, Popularity Dynamics, User Interaction Sequences, Pre-trained Models, Real-world Applications, Cold Start, Recommendation Frameworks, User Preferences, Evaluation Metrics (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1022/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Cross-Domain Latent Factors Sharing via Implicit Matrix Factorization (2024)</h3>
    <p><strong>Authors:</strong> Alexey Vasilev, Anton Vakhrushev, Evgeny Frolov, Abdulaziz Samra, Alexander Grigorevskiy</p>
    <p>Data sparsity has been one of the long-standing problems for recommender systems. One of the solutions to mitigate this issue is to exploit knowledge available in other source domains. However, many cross-domain recommender systems introduce a complex architecture that makes them less scalable in practice. On the other hand, matrix factorization methods are still considered to be strong baselines for single-domain recommendations. In this paper, we introduce the CDIMF, a model that extends the standard implicit matrix factorization with ALS to cross-domain scenarios. We apply the Alternating Direction Method of Multipliers to learn shared latent factors for overlapped users while factorizing the interaction matrix. In a dual-domain setting, experiments on industrial datasets demonstrate a competing performance of CDIMF for both cold-start and warm-start. The proposed model can outperform most other recent cross-domain and single-domain models.</p>
    <p><strong>Categories:</strong> Matrix Factorization, Cross-Domain, Data Sparsity, Scalability, Cold Start, Recommender Systems, Implicit Feedback, Alternating Least Squares (ALS), Latent Factors, Evaluation Metrics Beyond Accuracy (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1029/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>