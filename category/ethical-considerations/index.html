<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">Ethical Considerations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/explainability/">Explainability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reinforcement-learning/">Reinforcement Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Explainability in Music Recommender System (2024)</h3>
    <p><strong>Authors:</strong> Shahrzad Shashaani</p>
    <p>Recommendation systems play a crucial role in our daily lives, influencing many of our significant and minor decisions. These systems also have become integral to the music industry, guiding users to discover new content based on their tastes. However, the lack of transparency in these systems often leaves users questioning the rationale behind recommendations. To address this issue, adding transparency and explainability to recommender systems is a promising solution. Enhancing the explainability of these systems can significantly improve user trust and satisfaction. This research focuses on exploring transparency and explainability in the context of recommendation systems, focusing on the music domain. This research can help to understand the gaps in explainability in music recommender systems to create more engaging and trustworthy music recommendations.</p>
    <p><strong>Categories:</strong> Music Recommendations, Explainability, Transparency, User Trust, Recommendation Systems, Human-Computer Interaction, Algorithmic Transparency, Ethical Considerations, Case Study, Music Industry Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1132/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Less is More: Towards Sustainability-Aware Persuasive Explanations in Recommender Systems (2024)</h3>
    <p><strong>Authors:</strong> Seda Polat Erdeniz, Viet-Man Le, Sebastian Lubos, Alexander Felfernig, Merfat El Mansi, Thi Ngoc Trang Tran</p>
    <p>Recommender systems play an important role in supporting the achievement of the United Nations sustainable development goals (SDGs). In recommender systems, explanations can support different goals, such as increasing a user’s trust in a recommendation, persuading a user to purchase specific items, or increasing the understanding of the reasons behind a recommendation. In this paper, we discuss the concept of “sustainability-aware persuasive explanations” which we regard as a major concept to support the achievement of the mentioned SDGs. Such explanations are orthogonal to most existing explanation approaches since they focus on a “less is more” principle, which per se is not included in existing e-commerce platforms. Based on a user study in three item domains, we analyze the potential impacts of sustainability-aware persuasive explanations. The study results are promising regarding user acceptance and the potential impacts of such explanations.</p>
    <p><strong>Categories:</strong> Sustainability, Recommender Systems, User Trust, Persuasion, Explanation Methods, User Studies, Ethical Considerations, Sustainable Development Goals (SDGs), Human-Computer Interaction, Impact Analysis (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1202/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Recommender Systems and Algorithmic Hate (2022)</h3>
    <p><strong>Authors:</strong> Robin Burke, Lucia Jayne, Jessie Smith</p>
    <p>Despite increasing reliance on personalization in digital platforms, many algorithms that curate content or information for users have been met with resistance. When users feel dissatisfied or harmed by recommendations, this can lead users to hate, or feel negatively towards these personalized systems. Algorithmic hate detrimentally impacts both users and the system, and can result in various forms of algorithmic harm, or in extreme cases can lead to public protests against “the algorithm” in question. In this work, we summarize some of the most common causes of algorithmic hate and their negative consequences through various case studies of personalized recommender systems. We explore promising future directions for the RecSys research community that could help alleviate algorithmic hate and improve the relationship between recommender systems and their users.</p>
    <p><strong>Categories:</strong> Algorithm Design, User Experience (UX), Ethics in AI/ML, Trust Issues, Algorithmic Harm, Case Studies, Public Perception, User Trust, Ethical Considerations, Personalization Issues (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/798/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Estimating and Penalizing Preference Shifts in Recommender Systems (2021)</h3>
    <p><strong>Authors:</strong> Micah Carroll, Stuart Russell, Anca Dragan, Dylan Hadfield-Menell</p>
    <p>Recommender systems trained via long-horizon optimization (e.g., reinforcement learning) will have incentives to actively manipulate user preferences through the recommended content. While some work has argued for making systems myopic to avoid this issue, even such systems can induce systematic undesirable preference shifts. Thus, rather than artificially stifling the capabilities of the system, in this work we explore how we can make capable systems that explicitly avoid undesirable shifts. We advocate for (1) estimating the preference shifts that would be induced by recommender system policies, and (2) explicitly characterizing what unwanted shifts are and assessing before deployment whether such policies will produce them – ideally even actively optimizing to avoid them. These steps involve two challenging ingredients: (1) requires the ability to anticipate how hypothetical policies would influence user preferences if deployed; instead, (2) requires metrics to assess whether such influences are manipulative or otherwise unwanted. We study how to do (1) from historical user interaction data by building a user predictive model that implicitly contains their preference dynamics; to address (2), we introduce the notion of a “safe policy”, which defines a trust region within which behavior is believed to be safe. We show that recommender systems that optimize for staying in the trust region avoid manipulative behaviors (e.g., changing preferences in ways that make users more predictable), while still generating engagement.</p>
    <p><strong>Categories:</strong> Recommender Systems, Reinforcement Learning, Preference Dynamics, Ethical Considerations, User Modeling, Trustworthy AI, Safe Policy, Fairness, Optimization, User-Centric Design, Real-World Applications, Improvement of Recommender Systems, Predictive Modeling (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/690/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>“Serving Each User”: Supporting Different Eating Goals Through a Multi-List Recommender Interface (2021)</h3>
    <p><strong>Authors:</strong> Alain Starke, Christoph Trattner, Edis Asotic</p>
    <p>Food recommender systems optimize towards a user’s current preferences. However, appetites may vary, in the sense that users might seek healthy recipes today and look for unhealthy meals tomorrow. In this paper, we propose a novel approach in the food domain to diversify recommendations across different lists to ‘serve’ different users goals, compiled in a multi-list food recommender interface. We evaluated our interface in a 2 (single list vs multiple lists) x 2 (without or with explanations) between-subject user study (N = 366), linking choice behavior and evaluation aspects through the user experience framework. Our multi-list interface was evaluated more favorably than a single-list interface, in terms of diversity and choice satisfaction. Moreover, it triggered changes in food choices, even though these choices were less healthy than those made in the single-list interface.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Food/Nutrition, Personalization, Multi-List Recommendations, User Study/Evaluation Methods, A/B Testing, Diversity in Recommendations, Choice Satisfaction, Behavior Change (Diet/Health), Real World Applications, Explanation Techniques, Ethical Considerations (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/659/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Designing for the Better by Taking Users into Account: A Qualitative Evaluation of User Control Mechanisms in (News) Recommender Systems (2019)</h3>
    <p><strong>Authors:</strong> Jaron Harambam, Mykola Makhortykh, Dimitrios Bountouridis, Joris van Hoboken</p>
    <p>Recommender systems (RS) are on the rise in many domains. While they offer great promises, they also raise concerns: lack of transparency, reduction of diversity, little to no user control. In this paper, we align with the normative turn in computer science which scrutinizes the ethical and societal implications of RS. We focus and elaborate on the concept of user control because that mitigates multiple problems at once. Taking the news industry as our domain, we conducted four focus groups, or moderated think-aloud sessions, with Dutch news readers (N=21) to systematically study how people evaluate different control mechanisms (at the input, process, and output phase) in a News Recommender Prototype (NRP). While these mechanisms are sometimes met with distrust about the actual control they offer, we found that an intelligible user profile (including reading history and flexible preferences settings), coupled with possibilities to influence the recommendation algorithms is highly valued, especially when these control mechanisms can be operated in relation to achieving personal goals. By bringing (future) users’ perspectives to the fore, this paper contributes to a  richer understanding of why and how to design for user control in recommender systems.</p>
    <p><strong>Categories:</strong> Recommender Systems, News Domain, User Control Mechanisms, Qualitative Evaluation, Ethical Considerations, Societal Implications, User Study, Input Control Mechanisms, Process Control Mechanisms, Output Control Mechanisms, Transparency, Personalization, News Recommender Systems, User-Centered Design, Usability (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/438/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>How Algorithmic Confounding in Recommendation Systems Increases Homogeneity and Decreases Utility (2018)</h3>
    <p><strong>Authors:</strong> Allison Chaney, Brandon Stewart, Barbara Engelhardt</p>
    <p>Recommendation systems are ubiquitous and impact many domains; they have the potential to influence product consumption, individuals’ perceptions of the world, and life-altering decisions. These systems are often part of a feedback loop: models are evaluated or trained with observed data that are confounded because users are already exposed to algorithmic recommendations. In this paper, we use simulations to show that using confounded data may amplify homogenization of user behavior without increasing utility.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Diversity of Recommendations, Feedback Loops, Algorithmic Bias, Confounded Data, Beyond Accuracy, User Behavior, Ethical Considerations (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/344/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Behaviorism is Not Enough (2016)</h3>
    <p><strong>Authors:</strong> Martijn C Willemsen, Michael D Ekstrand</p>
    <p>Behaviorism is the currently-dominant paradigm for building and evaluating recommender systems. Both the operation and the evaluation of recommender system applications are most often driven by analyzing the behavior of users. In this paper, we argue that listening to what users say — about the items and recom-mendations they like, the control they wish to exert on the output, and the ways in which they perceive the system — and not just observing what they do will enable important developments in the future of recommender systems. We provide both philosophi-cal and pragmatic motivations for this idea, describe the various points in the recommendation and evaluation processes where explicit user input may be considered, and discuss benefits that may result from considered incorporation of user preferences at each of these points. In particular, we envision recommender applications that aim to support users’ better selves: helping them live the life that they desire to lead. For example, recommender-assisted behavior change requires algorithms to predict not what users choose or do now, inferable from behavioral data, but what they should choose or do in the future to become healthier, fitter, more sustainable, or culturally aware. We hope that our work will spur useful discussion and many new ideas for recommenders that empower their users.</p>
    <p><strong>Categories:</strong> Recommender Systems, Explicit Feedback, Evaluation Methods, User-Centered Design, Behavior Change, Empowerment, Ethical Considerations, Interdisciplinary Approaches (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/213/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Algorithms Aside: Recommendation As The Lens Of Life (2016)</h3>
    <p><strong>Authors:</strong> Marko Tkalcic, Paolo Cremonesi, Anna Zacchi, Ayse Göker, Daniel Kohlsdorf, Davide Malagoli, Jean-Yves Le Moine, Martha Larson, Francesco Ricci, Domonkos Tikk, Andreas Lommatzsch, Thuy Ngoc Nguyen, Tamas Motajcsek, Jasminko Novak, Kristaps Dobrajs, Franca Garzotto, Omar Alonso, Andrew Demetriou, Frank Hopfgartner, Mario Scriminaci</p>
    <p>In this position paper, we take the experimental approach of putting algorithms aside, and reflect on what recommenders would be for people if they were not tied to technology. By looking at some of the shortcomings that current recommenders have fallen into and discussing their limitations from a human point of view, we ask the question: if freed from all limitations, what should, and what could, RecSys be? We then turn to the idea that life itself is the best recommender system, and that people themselves are the query. By looking at how life brings people in contact with options that suit their needs or match their preferences, we hope to shed further light on what current RecSys could be doing better. Finally, we look at the forms that RecSys could take in the future. By formulating our vision beyond the reach of usual considerations and current limitations, including business models, algorithms, data sets, and evaluation methodologies, we attempt to arrive at fresh conclusions that may inspire the next steps taken by the community of researchers working on RecSys.</p>
    <p><strong>Categories:</strong> Future Directions, Human-Centered Design, Ethical Considerations, Philosophy of Recommendations, Personalization Beyond Algorithms, Visionary Concepts, Societal Impact, Business Models in Recommendations, Reflections on Recommender Systems (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/217/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Recommending with an Agenda: Active Learning of Private Attributes using Matrix Factorization (2014)</h3>
    <p><strong>Authors:</strong> Smriti Bhagat, Nina Taft, Udi Weinsberg, Stratis Ioannidis</p>
    <p>Recommender systems leverage user demographic information, such as age, gender, etc., to personalize recommendations and better place their targeted ads. Oftentimes, users do not volunteer this information due to privacy concerns, or due to a lack of initiative in filling out their online profiles. We illustrate a new threat in which a recommender learns private attributes of users who do not voluntarily disclose them. We design both passive and active attacks that so- licit ratings for strategically selected items, and could thus be used by a recommender system to pursue this hidden agenda. Our methods are based on a novel usage of Bayesian matrix factorization in an active learning setting. Evaluations on multiple datasets illustrate that such attacks are indeed feasible and use significantly fewer rated items than static inference methods. Importantly, they succeed without sacrificing the quality of recommendations to users.</p>
    <p><strong>Categories:</strong> Matrix Factorization, Privacy, Active Learning, Bayesian Methods, Recommendation Systems, Attack Methods, User Demographics, Evaluation, Ethical Considerations (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/26/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>