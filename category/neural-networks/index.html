<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">Neural Networks</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/offline-evaluation/">Offline Evaluation</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/performance-evaluation/">Performance Evaluation</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Comparative Analysis of Pretrained Audio Representations in Music Recommender Systems (2024)</h3>
    <p><strong>Authors:</strong> Yan-Martin Tamm, Anna Aljanaki</p>
    <p>Over the years, Music Information Retrieval (MIR) has proposed various foundation models pretrained on large amounts of music data. Transfer learning showcases proven effectiveness of foundation models with a broad spectrum of downstream tasks, including auto-tagging and genre classification. However, MIR papers generally do not explore the efficiency of foundation models for Music Recommender Systems (MRS). In addition, the Recommender Systems (RS) community tends to favour traditional end-to-end neural network learning over these models. Our research addresses this gap and evaluates the applicability of six pretrained foundation models (MusicFM, Music2Vec, MERT, EncodecMAE, Jukebox, and MusiCNN) in the context of MRS. We assess their performance using three recommendation models: K-nearest neighbours (KNN), shallow neural network, and BERT4Rec. Our findings suggest that these models exhibit significant performance variability between traditional MIR tasks and MRS, indicating that valuable aspects of musical information captured by foundation models may differ depending on the task. This study establishes a foundation for further exploration of pretrained foundation models to enhance music recommendation systems.</p>
    <p><strong>Categories:</strong> Pretrained Models, Music Recommender Systems, Transfer Learning, Evaluation Methods, Music Information Retrieval, Neural Networks, Recommendation Algorithms, Audio Representations (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1084/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Enhancing Cross-Domain Recommender Systems with LLMs: Evaluating Bias and Beyond-Accuracy Measures (2024)</h3>
    <p><strong>Authors:</strong> Thomas Elmar Kolb</p>
    <p>The research domain of recommender systems is rapidly evolving. Initially, optimization efforts focused primarily on accuracy. However, recent research has highlighted the importance of addressing bias and beyond-accuracy measures such as novelty, diversity, and serendipity. With the rise of multi-domain recommender systems, the need to re-examine bias and beyond-accuracy measures in cross-domain settings has become crucial. Traditional methods face challenges such as cold-start problems, which can potentially be mitigated by leveraging LLMs. This proposed work investigates how LLM-based recommendation methods can enhance cross-domain recommender systems, focusing on identifying, measuring, and mitigating bias while evaluating the impact of beyond-accuracy measures. We aim to provide new insights by comparing traditional and LLM-based systems within a real-world environment encompassing the domains of news, books, and various lifestyle areas. Our research seeks to address the outlined gaps and develop effective evaluation strategies for the unique challenges posed by LLMs in cross-domain recommender systems.</p>
    <p><strong>Categories:</strong> Cross-Domain Recommender Systems, Large Language Models (LLMs), Bias, Novelty, Diversity, Serendipity, Beyond Accuracy, Cold Start Problem, News Domain, Books Domain, Lifestyle Domain, Real-World Applications, Traditional Recommenders, Neural Networks, Multi-Domain Evaluation, Enhancing Recommender Systems (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1137/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Scene-wise Adaptive Network for Dynamic Cold-start Scenes Optimization in CTR Prediction (2024)</h3>
    <p><strong>Authors:</strong> Chao Tang, Wenhao Li, Jie Zhou, Shixiong Zhao, Chuan Luo, Kun Zhang</p>
    <p>In the realm of modern mobile E-commerce, providing users with nearby commercial service recommendations through location-based online services has become increasingly vital. While machine learning approaches have shown promise in multi-scene recommendation, existing methodologies often struggle to address cold-start problems in unprecedented scenes: the increasing diversity of commercial choices, along with the short online lifespan of scenes, give rise to the complexity of effective recommendations in online and dynamic scenes. In this work, we propose Scene-wise Adaptive Network (SwAN), a novel approach that emphasizes high-performance cold-start online recommendations for new scenes. Our approach introduces several crucial capabilities, including scene similarity learning, user-specific scene transition cognition, scene-specific information construction for the new scene, and enhancing the diverged logical information between scenes. We demonstrate SwAN’s potential to optimize dynamic multi-scene recommendation problems by effectively online handling cold-start recommendations for any newly arrived scenes. More encouragingly, SwAN has been successfully deployed in Company M’s online catering recommendation service, which serves millions of customers per day, and SwAN has achieved a 5.64% CTR index improvement relative to the baselines and a 5.19% increase in daily order volume proportion.</p>
    <p><strong>Categories:</strong> Scene-wise Adaptive Network (SwAN), Neural Networks, Cold Start, Multi-scene Recommendations, Dynamic Scenarios, Recommendation Systems, Mobile E-commerce, Catering Services, Location-based Services, Evaluation Metrics, Beyond Accuracy, Real-world Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1063/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Investigating the effects of incremental training on neural ranking models (2023)</h3>
    <p><strong>Authors:</strong> Wenzhe Shi, Gilberto Titericz, Kazuki Onodera, Gabriel de Souza Pereira Moreira, Praveen Dhinwa, Chris Deotte, Even Oldridge, Vishal Agrawal, Chris Green, Benedikt Schifferer</p>
    <p>Recommender systems are an essential component of online systems, providing users with a personalized experience. Some recommendation scenarios such as social networks or news are very dynamic, with new items added continuously and the interest of users changing over time due to breaking news or popular events. Incremental training is a popular technique to keep recommender models up-to-date in those dynamic platforms. In this paper, we provide an empirical analysis of a large industry dataset from the Sharechat app MOJ, a social media platform for short videos, to answer relevant questions like – how often should I retrain the model? – do different models, features and dataset sizes benefit from incremental training? – Do all users and items benefit the same from incremental training?</p>
    <p><strong>Categories:</strong> Neural Networks, Incremental Training, Recommendation Systems, Model Evaluation, Dynamic Data, Social Media, Online Learning, Industry Dataset, User Behavior Analysis, Short Videos (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1001/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Multiple Connectivity Views for Session-based Recommendation (2023)</h3>
    <p><strong>Authors:</strong> Zheng Miao, Yujing Wang, Yaming Yang, Jieyu Zhang, Yunhai Tong</p>
    <p>Session-based recommendation (SBR), which makes the next-item recommendation based on previous anonymous actions, has drawn increasing attention. The last decade has seen multiple deep learning-based modeling choices applied on SBR successfully, e.g., recurrent neural networks (RNNs), convolutional neural networks (CNNs), graph neural networks (GNNs), and each modeling choice has its intrinsic superiority and limitation. We argue that these modeling choices differentiate from each other by (1) the way they capture the interactions between items within a session and (2) the operators they adopt for composing the neural network, e.g., convolutional operator or self-attention operator. In this work, we dive deep into the former as it is relatively unique to the SBR scenario, while the latter is shared by general neural network modeling techniques. We first introduce the concept of connectivity view to describe the different item interaction patterns at the input level. Then, we develop the Multiple Connectivity Views for Session-based Recommendation (MCV-SBR), a unified framework that incorporates different modeling choices in a single model through the lens of connectivity view. In addition, MCV-SBR allows us to effectively and efficiently explore the search space of the combinations of connectivity views by the Tree-structured Parzen Estimator Approach (TPE) algorithm. Finally, on three widely used SBR datasets, we verify the superiority of MCV-SBR by comparing the searched models with state-of-the-art baselines. We also conduct a series of studies to demonstrate the efficacy and practicability of the proposed connectivity view search algorithm, as well as other components in MCV-SBR.</p>
    <p><strong>Categories:</strong> Session-based Recommendation, Neural Networks, Connectivity Views, Modeling Choices in SBR, Tree-structured Parzen Estimator (TPE), Deep Learning, Unsupervised Learning, Evaluation Studies, Recommendation Algorithms (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/938/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Track Mix Generation on Music Streaming Services using Transformers (2023)</h3>
    <p><strong>Authors:</strong> Thomas Bouabça, Walid Bendada, Benjamin Chapus, Thibault Cador, Théo Bontempelli, Mathieu Morlon, Guillaume Salha-Galvan</p>
    <p>This paper introduces Track Mix, a personalized playlist generation system released in 2022 on the music streaming service Deezer. Track Mix automatically generates “mix” playlists inspired by initial music tracks, allowing users to discover music similar to their favorite content. To generate these mixes, we consider a Transformer model trained on millions of track sequences from user playlists. In light of the growing popularity of Transformers in recent years, we analyze the advantages, drawbacks, and technical challenges of using such a model for mix generation on the service, compared to a more traditional collaborative filtering approach. Since its release, Track Mix has been generating playlists for millions of users daily, enhancing their music discovery experience on Deezer.</p>
    <p><strong>Categories:</strong> Transformer Models, Music Streaming, Recommendation Systems, Deep Learning, Personalization, Language Modeling, Collaborative Filtering, Real-World Applications, User Engagement, Playlist Generation, Neural Networks, Music Domain, Scalability, Model Evaluation (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1016/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Integrating the ACT-R Framework with Collaborative Filtering for Explainable Sequential Music Recommendation (2023)</h3>
    <p><strong>Authors:</strong> Christian Wallmann, Markus Schedl, Elisabeth Lex, Dominik Kowald, Marta Moscati, Markus Reiter-Haas</p>
    <p>Music listening sessions often consist of sequences including repeating tracks. Modeling such relistening behavior with models of human memory has been proven effective in predicting the next track of a session. However, these models intrinsically lack the capability of recommending novel tracks that the target user has not listened to in the past. Collaborative filtering strategies, on the contrary, provide novel recommendations by leveraging past collective behaviors but are often limited in their ability to provide explanations. To narrow this gap, we propose four hybrid algorithms that integrate collaborative filtering with the cognitive architecture ACT-R. We compare their performance in terms of accuracy, novelty, diversity, and popularity bias, to baselines of different types, including pure ACT-R, kNN-based, and neural-networks-based approaches. We show that the proposed algorithms are able to achieve the best performances in terms of novelty and diversity, and simultaneously achieve a higher accuracy of recommendation with respect to pure ACT-R models. Furthermore, we illustrate how the proposed models can provide explainable recommendations.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Music, ACT-R Framework, Collaborative Filtering, Explainability, Accuracy, Novelty, Diversity, Popularity Bias, kNN-Based Algorithms, Neural Networks, Sequential Recommendations, Real-World Applications, Hybrid Methods, Cold Start (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/919/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>From Research to Production: Towards Scalable and Sustainable Neural Recommendation Models on Commodity CPU Hardware (2023)</h3>
    <p><strong>Authors:</strong> Anshumali Shrivastava, Nicholas Meisburger, Tharun Medini, Siddharth Jain, David Torres Ramos, Vihan Lakshman, Joshua Engels, Pratik Pranav, Yashwanth Adunukota, Shubh Gupta, Benito Geordie</p>
    <p>In the last decade, large-scale deep learning has fundamentally transformed industrial recommendation systems. However, this revolutionary technology remains prohibitively expensive due to the need for costly and scarce specialized hardware, such as GPUs, to train and serve models. In this talk, we share our multi-year journey at ThirdAI in developing efficient neural recommendation models that can be trained and deployed on commodity CPU machines without the need for costly accelerators like GPUs. In particular, we discuss the limitations of the current GPU-based ecosystem in machine learning, why recommendation systems are amenable to the strengths of CPU devices, and present results from our efforts to translate years of academic research into a deployable system that fundamentally shifts the economics of training and operating large-scale machine learning models.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Neural Networks, Production Systems, Scalability, Hardware Optimization, Cost-Effectiveness, Efficiency Optimization, Research to Practice (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/999/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>A Model-Agnostic Framework for Recommendation via Interest-aware Item Embeddings (2023)</h3>
    <p><strong>Authors:</strong> Yu Xiong, Amit Kumar Jaiswal</p>
    <p>Item representation holds significant importance in recommendation systems, which encompasses domains such as news, retail, and videos. Retrieval and ranking models utilise item representation to capture the user-item relationship based on user behaviours. While existing representation learning methods primarily focus on optimising item-based mechanisms, such as attention and sequential modelling. However, these methods lack a modelling mechanism to directly reflect user interests within the learned item representations. Consequently, these methods may be less effective in capturing user interests indirectly. To address this challenge, we propose a novel Interest-aware Capsule network (IaCN) recommendation model, a model-agnostic framework that directly learns interest-oriented item representations. IaCN serves as an auxiliary task, enabling the joint learning of both item-based and interest-based representations. This framework adopts existing recommendation models without requiring substantial redesign. We evaluate the proposed approach on benchmark datasets, exploring various scenarios involving different deep neural networks, behaviour sequence lengths, and joint learning ratios of interest-oriented item representations. Experimental results demonstrate significant performance enhancements across diverse recommendation models, validating the effectiveness of our approach.</p>
    <p><strong>Categories:</strong> Model-Agnostic Methods, Neural Networks, News Recommendations, Retail Recommendations, Video Recommendations, Item Embeddings, User Interest Modeling, Recommendation Frameworks, Deep Learning Models, Behavior Modeling (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/949/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Power Loss Function in Neural Networks for Predicting Click-Through Rate (2023)</h3>
    <p><strong>Authors:</strong> Ergun Biçici</p>
    <p>Loss functions guide machine learning models towards concentrating on the error most important to improve upon.  We introduce power loss functions for neural networks and apply them on imbalanced click-through rate datasets. Power loss functions decrease the loss for confident predictions and increase the loss for error-prone predictions. They improve both AUC and F1 and produce better calibrated results.  We obtain improvements in the results on four different classifiers and on two different datasets. We obtain significant improvements in AUC that reach $0.44\%$ for DeepFM on the Avazu dataset.</p>
    <p><strong>Categories:</strong> Neural Networks, Click-Through Rate, Deep Learning, Loss Functions, Imbalanced Data, Recommendation Systems, AUC, F1 Score, Evaluation Metrics, Digital Advertising (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/963/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Overcoming Recommendation Limitations with Neuro-Symbolic Integration (2023)</h3>
    <p><strong>Authors:</strong> Tommaso Carraro</p>
    <p>Despite being studied for over twenty years, Recommender Systems (RSs) still suffer from important issues that limit their applicability in real-world scenarios. Data sparsity, cold start, and explainability are some of the most impacting problems. Intuitively, these historical limitations can be mitigated by injecting prior knowledge into recommendation models. Neuro-Symbolic (NeSy) approaches are suitable candidates for achieving this goal. Specifically, they aim to integrate learning (e.g., neural networks) with symbolic reasoning (e.g., logical reasoning). Generally, the integration lets a neural model interact with a logical knowledge base, enabling reasoning capabilities. In particular, NeSy approaches have been shown to deal well with poor training data, and their symbolic component could enhance model transparency. This gives insights that NeSy systems could potentially mitigate the aforementioned RSs limitations. However, the application of such systems to RSs is still in its early stages, and most of the proposed architectures do not really exploit the advantages of a NeSy approach. To this end, we conducted preliminary experiments with a Logic Tensor Network (LTN), a novel NeSy framework. We used the LTN to train a vanilla Matrix Factorization model using a First-Order Logic knowledge base as an objective. In particular, we encoded facts to enable the regularization of the latent factors using content information, obtaining promising results. In this paper, we review existing NeSy recommenders, argue about their limitations, show our preliminary results with the LTN, and propose interesting future works in this novel research area. In particular, we show how the LTN can be intuitively used to regularize models, perform cross-domain recommendation, ensemble learning, and explainable recommendation, reduce popularity bias, and easily define the loss function of a model.</p>
    <p><strong>Categories:</strong> Neuro-Symbolic Integration, Recommender Systems, Data Sparsity, Cold Start Problem, Explainability, Neural Networks, Symbolic Reasoning, Logic Tensor Networks (LTN), Matrix Factorization, Content-Based Recommendations, Model Transparency, Regularization, Cross-Domain Recommendation, Ensemble Learning, Popularity Bias (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/986/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>You Say Factorization Machine, I Say Neural Network – It’s All in the Activation (2022)</h3>
    <p><strong>Authors:</strong> Yedid Hoshen, Chen Almagor</p>
    <p>In recent years, many methods for machine learning on tabular data were introduced that use either factorization machines, neural networks or both. This created a great variety of methods making it non-obvious which method should be used in practice. We begin by extending the previously established theoretical connection between polynomial neural networks and factorization machines (FM) to recently introduced FM techniques. This allows us to propose a single neural-network-based framework that can switch between the deep learning and FM paradigms by a simple change of an activation function. We further show that an activation function exists which can adaptively learn to select the optimal paradigm. Another key element in our framework is its ability to learn high-dimensional embeddings by low-rank factorization. Our framework can handle numeric and categorical data as well as multiclass outputs. Extensive empirical experiments verify our analytical claims. Source code is available at https://github.com/ChenAlmagor/FiFa</p>
    <p><strong>Categories:</strong> Factorization Machines, Neural Networks, Machine Learning, Hybrid Methods, Activation Functions, Empirical Evaluation, Model Flexibility, Tabular Data, Algorithm Design. (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/782/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Defending Substitution-based Profile Pollution Attacks on Sequential Recommenders (2022)</h3>
    <p><strong>Authors:</strong> Huimin Zeng, Dong Wang, Lanyu Shang, Zhenrui Yue, Ziyi Kou</p>
    <p>While sequential recommender systems achieve significant improvements on capturing user dynamics, we argue that sequential recommenders are vulnerable against substitution-based profile pollution attacks. To demonstrate our hypothesis, we propose a substitution-based adversarial attack algorithm, which modifies the input sequence by selecting certain vulnerable elements and substituting them with adversarial items. In both untargeted and targeted attack scenarios, we observe significant performance deterioration using the proposed profile pollution algorithm. Motivated by such observations, we design an efficient adversarial defense method called Dirichlet neighborhood sampling. Specifically, we sample item embeddings from a convex hull constructed by multi-hop neighbors to replace the original items in input sequences. During sampling, a Dirichlet distribution is used to approximate the probability distribution in the neighborhood such that the recommender learns to combat local perturbations. Additionally, we design an adversarial training method tailored for sequential recommender systems. In particular, we represent selected items with one-hot encodings and perform gradient ascent on the encodings to search for the worst case linear combination of item embeddings in training. As such, the embedding function learns robust item representations and the trained recommender is resistant to test-time adversarial examples. Extensive experiments show the effectiveness of both our attack and defense methods, which consistently outperform baselines by a significant margin across model architectures and datasets.</p>
    <p><strong>Categories:</strong> Adversarial Attacks, Security in Recommender Systems, Sequential Recommenders, Machine Learning, Neural Networks, Embedding Functions, Defense Mechanisms, Recommendation Systems (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/757/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Modeling User Repeat Consumption Behavior for Online Novel Recommendation (2022)</h3>
    <p><strong>Authors:</strong> Cunxiang Yin, Leeven Luo, Yuncong Li, Yancheng He, Guoqiang Xu, Jing Cai, Sheng-hua Zhong</p>
    <p>Given a user’s historical interaction sequence, online novel recommendation suggests the next novel the user may be interested in. Online novel recommendation is important but underexplored. In this paper, we concentrate on recommending online novels to new users of an online novel reading platform, whose first visits to the platform occurred in the last seven days. We have two observations about online novel recommendation for new users. First, repeat novel consumption of new users is a common phenomenon. Second, interactions between users and novels are informative. To accurately predict whether a user will reconsume a novel, it is crucial to characterize each interaction at a fine-grained level. Based on these two observations, we propose a neural network for online novel recommendation, called NovelNet. NovelNet can recommend the next novel from both the user’s consumed novels and new novels simultaneously. Specifically, an interaction encoder is used to obtain accurate interaction representation considering fine-grained attributes of interaction, and a pointer network with a pointwise loss is incorporated into NovelNet to recommend previously-consumed novels. Moreover, an online novel recommendation dataset is built from a well-known online novel reading platform and is released for public use as a benchmark. Experimental results on the dataset demonstrate the effectiveness of NovelNet 1.</p>
    <p><strong>Categories:</strong> Repeat Consumption, User Behavior, Online Platforms, Neural Networks, Books, Cold Start, Recommendation Accuracy, Personalization, Interaction Encoding, Experimental Analysis, Dataset Construction, Benchmarking, Web Systems (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/776/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>TorchRec: a PyTorch domain library for recommendation systems (2022)</h3>
    <p><strong>Authors:</strong> Dennis Van Der Staay, Colin Taylor, Xing Liu, Rahul Kindi, Anirudh Sudarshan, Shahin Sefati, Will Feng, Dmytro Ivchenko</p>
    <p>Recommendation Systems (RecSys) comprise a large footprint of production-deployed AI today. The neural network-based recommender systems differ from deep learning models in other domains in using high-cardinality categorical sparse features that require large embedding tables to be trained. In this talk we introduce TorchRec, a PyTorch domain library for Recommendation Systems. This new library provides common sparsity and parallelism primitives, enabling researchers to build state-of-the-art personalization models and deploy them in production. In this talk we cover the building blocks of the TorchRec library including modeling primitives such as embedding bags and jagged tensors, optimized recommender system kernels powered by FBGEMM, a flexible sharder that supports a veriety of strategies for partitioning embedding tables, a planner that automatically generates optimized and performant sharding plans, support for GPU inference and common modeling modules for building recommender system models. TorchRec library is currently used to train large-scale recommender models at Meta. We will present how TorchRec helped Meta’s recommender system platform to transition from CPU asynchronous training to accelerator-based full-sync training.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Neural Networks, PyTorch, Sparse Features, High Cardinality, Personalization Models, Embedding Bags, Jagged Tensors, FBGEMM, GPU Inference, Accelerator-Based Training, Production Deployment, Sparsity Management, Model Optimization, Sharding Strategies, Sharding Plans, Meta Platforms, Large-Scale Models, Production-Grade, Model Efficiency (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/828/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Rethinking Personalized Ranking at Pinterest: An End-to-End Approach (2022)</h3>
    <p><strong>Authors:</strong> Jiajing Xu</p>
    <p>In this work, we present our journey to revolutionize the personalized recommendation engine through end-to-end learning from raw user actions. We encode user’s long-term interest in PinnerFormer, a user embedding optimized for long-term future actions via a new dense all-action loss, and capture user’s short-term intention by directly learning from the real-time action sequences. We conducted both offline and online experiments to validate the performance of the new model architecture, and also address the challenge of serving such a complex model using mixed CPU/GPU setup in production. The proposed system has been deployed in production at Pinterest and has delivered significant online gains across organic and Ads applications.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Personalized Ranking, End-to-End Learning, Neural Networks, Transformer-Based Models, User Embeddings, Long-Term Interest, Short-Term Intention, Offline Experiments, Online Experiments, A/B Testing, Real-World Applications, Deployment in Production, Scalability, Performance Optimization (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/823/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>