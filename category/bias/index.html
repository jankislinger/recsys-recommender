<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/cold-start/">Cold Start</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/scalability/">Scalability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/performance-evaluation/">Performance Evaluation</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Enhancing Cross-Domain Recommender Systems with LLMs: Evaluating Bias and Beyond-Accuracy Measures (2024)</h3>
    <p><strong>Authors:</strong> Thomas Elmar Kolb</p>
    <p>The research domain of recommender systems is rapidly evolving. Initially, optimization efforts focused primarily on accuracy. However, recent research has highlighted the importance of addressing bias and beyond-accuracy measures such as novelty, diversity, and serendipity. With the rise of multi-domain recommender systems, the need to re-examine bias and beyond-accuracy measures in cross-domain settings has become crucial. Traditional methods face challenges such as cold-start problems, which can potentially be mitigated by leveraging LLMs. This proposed work investigates how LLM-based recommendation methods can enhance cross-domain recommender systems, focusing on identifying, measuring, and mitigating bias while evaluating the impact of beyond-accuracy measures. We aim to provide new insights by comparing traditional and LLM-based systems within a real-world environment encompassing the domains of news, books, and various lifestyle areas. Our research seeks to address the outlined gaps and develop effective evaluation strategies for the unique challenges posed by LLMs in cross-domain recommender systems.</p>
    <p><strong>Categories:</strong> Cross-Domain Recommender Systems, Large Language Models (LLMs), Bias, Novelty, Diversity, Serendipity, Beyond Accuracy, Cold Start Problem, News Domain, Books Domain, Lifestyle Domain, Real-World Applications, Traditional Recommenders, Neural Networks, Multi-Domain Evaluation, Enhancing Recommender Systems (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1137/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Oh, Behave! Country Representation Dynamics Created by Feedback Loops in Music Recommender Systems (2024)</h3>
    <p><strong>Authors:</strong> Dominik Kowald, Markus Schedl, Oleg Lesota, Jonas Geiger, Max Walder</p>
    <p>Recent work suggests that music recommender systems are prone to disproportionally frequent recommendations of music from countries more prominently represented in the training data, notably the US. However, it remains unclear to what extent feedback loops in music recommendation influence the dynamics of such imbalance. In this work, we investigate the dynamics of representation of local (i.e., country-specific) and US-produced music in user profiles and recommendations. To this end, we conduct a feedback loop simulation study using the standardized LFM-2b dataset. The results suggest that most of the investigated recommendation models decrease the proportion of music from local artists in their recommendations. Furthermore, we find that models preserving average proportions of US and local music do not necessarily provide country-calibrated recommendations. We also look into popularity calibration and, surprisingly, find that the most popularity-calibrated model in our study (ItemKNN) provides the least country-calibrated recommendations. In addition, users from less represented countries (e.g., Finland) are, in the long term, most affected by the under-representation of their local music in recommendations.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Music Recommender, Country Representation, User Behavior, Feedback Loops, Bias, Fairness, Algorithmic Bias, Empirical Study, Real World Applications, Cultural Dynamics, Calibration, Evaluation Metrics, Performance Analysis, User Impact (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1099/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Bias in Book Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Savvina Daniil</p>
    <p>Recommender systems are prevalent in many applications, but hide risks; issues like bias propagation have been on the focus of related studies in recent years. My own research revolves around tracking bias in the book recommendation domain. Specifically, I am interested in whether the incorporation of recommender systems in a library’s loaning system serves their social responsibility and purpose, with bias being the main point of concern. To this end, I engage with the topic in three ways; by mapping the area of ethics in book recommendation, by investigating and reflecting on challenges with studying bias in recommender systems in general, and by showcasing a set of social implication of statistical bias in the book recommendation domain in particular. In this doctoral symposium paper, I further elaborate on the problem at hand, the outline of my thesis, the progress I have made so far, as well as my plans for future work along with specific questions that have arisen from my research efforts.</p>
    <p><strong>Categories:</strong> Book Recommendation, Recommender Systems, Ethics, Bias, Statistical Bias, Social Implications, Library Systems, Social Responsibility, Algorithmic Fairness (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1127/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Group Fairness for Content Creators: the Role of Human and Algorithmic Biases under Popularity-based Recommendations (2023)</h3>
    <p><strong>Authors:</strong> Nicolo Pagan, Stefania Ionescu, Aniko Hannak</p>
    <p>The Creator Economy faces concerning levels of unfairness. Content creators (CCs) publicly accuse platforms of purposefully reducing the visibility of their content based on protected attributes, while platforms place the blame on viewer biases. Meanwhile, prior work warns about the “rich-get-richer”  effect perpetuated by existing popularity biases in recommender systems: Any initial advantage in visibility will likely be exacerbated over time. What remains unclear is how the biases based on protected attributes from platforms and viewers interact and contribute to the observed inequality in the context of popularity-biased recommender systems. The difficulty of the question lies in the complexity and opacity of the system. To overcome this challenge, we create a simple agent-based model (ABM) that unifies the platform systems which allocate the visibility of CCs (e.g., recommender systems, moderation) into a single popularity-based function, which we call the visibility allocation system (VAS). Through simulations, we find that although viewer homophilic biases do alone create inequalities, small levels of additional biases in VAS are more harmful. From the perspective of interventions, our results suggest that (a) attempts to reduce attribute-biases in moderation and recommendations should precede those reducing viewer homophilic tendencies, (b) decreasing the popularity-biases in VAS decreases but not eliminates inequalities, (c) boosting the visibility of protected CCs to overcome viewer homophily with respect to one metric is unlikely to produce fair outcomes with respect to all metrics, and (d) the process is also unfair for viewers and this unfairness could be overcome through the same interventions. More generally, this work demonstrates the potential of using ABMs to better understand the causes and effects of biases and interventions within complex sociotechnical systems.</p>
    <p><strong>Categories:</strong> Fairness, Bias, Recommendation Systems, Algorithmic Fairness, Creator Economy, Agent-Based Modeling (ABM), Sociotechnical Systems, Interventions in Recommendation Systems, Human Biases, Algorithmic Biases, Complexity of Systems, Group Fairness (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/915/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Sparse Feature Factorization for Recommender Systems with Knowledge Graphs (2021)</h3>
    <p><strong>Authors:</strong> Antonio Ferrara, Alberto Carlo Maria Mancino, Vito Walter Anelli, Tommaso Di Noia</p>
    <p>Deep Learning and factorization-based collaborative filtering recommendation models have undoubtedly dominated the scene of recommender systems in recent years. However, despite their outstanding performance, these methods require a training time proportional to the size of the embeddings and it further increases when also side information is considered for the computation of the recommendation list. In fact, in these cases we have that with a large number of high-quality features, the resulting models are more complex and difficult to train. This paper addresses this problem by presenting KGFlex: a sparse factorization approach that grants an even greater degree of expressiveness. To achieve this result, KGFlex analyzes the historical data to understand the dimensions the user decisions depend on (e.g., movie direction, musical genre, nationality of book writer). KGFlex represents each item feature as an embedding and it models user-item interactions as a factorized entropy-driven combination of the item attributes relevant to the user. KGFlex facilitates the training process by letting users update only those relevant features on which they base their decisions. In other words, the user-item prediction is mediated by the user’s personal view that considers only relevant features. An extensive experimental evaluation shows the approach’s effectiveness, considering the recommendation results’ accuracy, diversity, and induced bias. The public implementation of KGFlex is available at https://split.to/kgflex.</p>
    <p><strong>Categories:</strong> Sparse Feature Factorization, Recommender Systems, Knowledge Graphs, Machine Learning, Movies, Music, Embeddings, Relevant Features, Model Complexity, Accuracy, Diversity, Bias (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/661/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Analyzing Item Popularity Bias of Music Recommender Systems: Are Different Genders Equally Affected? (2021)</h3>
    <p><strong>Authors:</strong> Navid Rekabsaz, Elisabeth Lex, Dominik Kowald, Stefan Brandl, Markus Schedl, Alessandro Melchiorre, Oleg Lesota</p>
    <p>Several studies have identified discrepancies between the popularity of items in user profiles and the corresponding recommendation lists. Such behavior, which concerns a variety of recommendation algorithms, is referred to as popularity bias. Existing work predominantly adopts simple statistical measures, such as the difference of mean or median popularity, to quantify popularity bias. Moreover, it does so irrespective of user characteristics other than the inclination to popular content. In this work, in contrast, we propose to investigate popularity differences (between the user profile and recommendation list) in terms of median, a variety of statistical moments, as well as similarity measures that consider the entire popularity distributions (Kullback-Leibler divergence and Kendall’s τ rank-order correlation). This results in a more detailed picture of the characteristics of popularity bias. Furthermore, we investigate whether such algorithmic popularity bias affects users of different genders in the same way. We focus on music recommendation and conduct experiments on the recently released standardized LFM-2b dataset, containing listening profiles of Last.fm users. We investigate the algorithmic popularity bias of seven common recommendation algorithms (five collaborative filtering and two baselines). Our experiments show that (1) the studied metrics provide novel insights into popularity bias in comparison with only using average differences, (2) algorithms less inclined towards popularity bias amplification do not necessarily perform worse in terms of utility (NDCG), (3) the majority of the investigated recommenders intensify the popularity bias of the female users.</p>
    <p><strong>Categories:</strong> Music, Recommendation Systems, Gender, User Characteristics, Fairness, Bias, Diversity of Recommendations, Real World Applications, Beyond Accuracy, Collaborative Filtering, Algorithm Analysis, User Demographics (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/682/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>I want to break free! Recommending friends from outside the echo chamber (2021)</h3>
    <p><strong>Authors:</strong> Daniela Godoy, Antonela Tommasel, Juan Manuel Rodriguez</p>
    <p>Recommender systems serve as mediators of information consumption and propagation. In this role, these systems have been recently criticized for introducing biases and promoting the creation of echo chambers and filter bubbles, thus lowering the diversity of both content and potential new social relations users are exposed to. Some of these issues are a consequence of the fundamental concepts on which recommender systems are based on. Assumptions like the homophily principle might lead users to content that they already like or friends they already know, which can be naïve in the era of ideological uniformity and fake news. A significant challenge in this context is how to effectively learn the dynamic representations of users based on the content they share and their echo chamber or community interactions to recommend potentially relevant and diverse friends from outside the network of influence of the users’ echo chamber. To address this, we devise FRediECH (a Friend RecommenDer for breakIng Echo CHambers), an echo chamber-aware friend recommendation approach that learns users and echo chamber representations from the shared content and past users’ and communities’ interactions. Comprehensive evaluations over Twitter data showed that our approach achieved better performance (in terms of relevance and novelty) than state-of-the-art alternatives, validating its effectiveness.</p>
    <p><strong>Categories:</strong> Echo Chamber, Filter Bubbles, Bias, Diversity of Recommendations, Social Network Analysis, Homophily Principle, Content-Based Filtering, Collaborative Filtering, Community Detection, Real-World Application (Twitter Data), Cross-Network Recommendations, Novelty, Algorithm Design, General Recommendation Systems (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/646/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Measuring the Concentration Reinforcement Bias of Recommender Systems (2015)</h3>
    <p><strong>Authors:</strong> Alexander Tuzhilin, Peter Mountanos, Panagiotis Adamopoulos</p>
    <p>In this paper, we propose new metrics to accurately measure the concentration reinforcement of recommender systems and the enhancement of the “long tail”. We also conduct a comparative analysis of various RS algorithms illustrating the usefulness of the proposed metrics.</p>
    <p><strong>Categories:</strong> Bias, Fairness, Concentration Reinforcement Bias, Long Tail Recommendations, Evaluation Metrics, Algorithm Comparison, Recommender Systems, Empirical Study, Performance Analysis (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/152/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>