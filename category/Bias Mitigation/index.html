<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/Large%20Language%20Models%20(LLMs)/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/Transfer%20Learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/Explainability/">Explainability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/Sequential%20Recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">Bias Mitigation</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/AB%20Testing/">AB Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/AB%20Test/">AB Test</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/Fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/Reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/Data%20Sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/Reinforcement%20Learning/">Reinforcement Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/Evaluation%20Metrics/">Evaluation Metrics</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Putting Popularity Bias Mitigation to the Test: A User-Centric Evaluation in Music Recommenders (2024)</h3>
    <p><strong>Authors:</strong> Robin Ungruh, Karlijn Dinnissen, Maria Soledad Pera, Hanna Hauptmann, Anja Volk</p>
    <p>Popularity bias is a prominent phenomenon in recommender systems (RS), especially in the music domain. Although popularity bias mitigation techniques are known to enhance the fairness of RS while maintaining their high performance, there is a lack of understanding regarding users’ actual perception of the suggested music. To address this gap, we conducted a user study (n=40) exploring user satisfaction and perception of personalized music recommendations generated by algorithms that explicitly mitigate popularity bias. Specifically, we investigate item-centered and user-centered bias mitigation techniques, aiming to ensure fairness for artists or users, respectively. Results show that neither mitigation technique harms the users’ satisfaction with the recommendation lists despite promoting underrepresented items. However, the item-centered mitigation technique impacts user perception; by promoting less popular items, it reduces users’ familiarity with the items. Lower familiarity evokes discovery—the feeling that the recommendations enrich the user’s taste. We demonstrate that this  can ultimately lead to higher satisfaction, highlighting the potential of less-popular recommendations to improve the user experience.</p>
    <p><strong>Categories:</strong> Recommendation Algorithms, Music Recommendations, Popularity Bias, User-Centric Evaluation, Bias Mitigation, Recommender Systems Evaluation, Personalization, User Perception, Fairness in Recommendations, Discovery (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1056/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Bridging Viewpoints in News with Recommender Systems (2024)</h3>
    <p><strong>Authors:</strong> Jia Hua Jeng</p>
    <p>News Recommender systems (NRSs) aid in decision-making in news media. However, undesired effects can emerge. Among these are selective exposures that may contribute to polarization, potentially reinforcing existing attitudes through belief perseverance—discounting contrary evidence due to their opposing attitudinal strength. This can be unsafe for people, making it difficult to accept information objectively. A crucial issue in news recommender system research is how to mitigate these undesired effects by designing recommender interfaces and machine learning models that enable people to consider to be more open to different perspectives. Alongside accurate models, the user experience is an equally important measure. Indeed, the core statistics are based on users’ behaviors and experiences in this research project. Therefore, this research agenda aims to steer the choices of readers’ based on altering their attitudes. The core methods plan to concentrate on the interface design and ML model building involving manipulations of cues, users’ behaviors prediction, NRSs algorithm and changing the nudges. In sum, the project aims to provide insight in the extent to which news recommender systems can be effective in mitigating polarized opinions.</p>
    <p><strong>Categories:</strong> Machine Learning, News Recommender Systems, Bias Mitigation, Personalization, User Experience, Interface Design, Behavior Prediction, Nudge Theory, Selective Exposure, Diversity of Recommendations, Perspective Bridging, Accuracy in Recommendations, Attitude Alteration, Media, Public Opinion, Social Impact (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1135/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Fairness Matters: A look at LLM-generated group recommendations (2024)</h3>
    <p><strong>Authors:</strong> Antonela Tommasel</p>
    <p>Recommender systems play a crucial role in how users consume information, with group recommendation receiving considerable attention. Ensuring fairness in group recommender systems entails providing recommendations that are useful and relevant to all group members rather than solely reflecting the majority’s preferences, while also addressing fairness concerns related to sensitive attributes (e.g., gender). Recently, the advancements on Large Language Models (LLMs) have enabled the development of new kinds of recommender systems. However, LLMs can perpetuate social biases present in training data, posing risks of unfair outcomes and harmful impacts. We investigated LLMs impact on group recommendation fairness, establishing and instantiating a framework that encompasses group definition, sensitive attribute combinations, and evaluation methodology. Our findings revealed the interactions patterns between sensitive attributes and LLMs and how they affected recommendation. This study advances the understanding of fairness considerations in group recommendation systems, laying the groundwork for future research.</p>
    <p><strong>Categories:</strong> Recommender Systems, Fairness, Group Recommendations, Large Language Models (LLMs), Sensitive Attributes, Bias Mitigation, Natural Language Processing (NLP), Evaluation Methodology, Social Biases, Societal Impact (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1089/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>AIE: Auction Information Enhanced Framework for CTR Prediction in Online Advertising (2024)</h3>
    <p><strong>Authors:</strong> Chenxu Zhu, Muyu Zhang, Yang Yang, Huifeng Guo, Menghui Zhu, Bo Chen, Ruiming Tang, Zhenhua Dong, Xinyi Dai</p>
    <p>Click-Through Rate (CTR) prediction is a fundamental technique for online advertising recommendation and the complex online competitive auction process also brings many difficulties to CTR optimization. Recent studies have shown that introducing posterior auction information contributes to the performance of CTR prediction. However, existing work doesn’t fully capitalize on the benefits of auction information and overlooks the data bias brought by the auction, leading to biased and suboptimal results. To address these limitations, we propose Auction Information Enhanced Framework (AIE) for CTR prediction in online advertising, which delves into the problem of insufficient utilization of auction signals and first reveals the auction bias. Specifically, AIE introduces two pluggable modules, namely Adaptive Market-price AuxiliaryModule (AM2) and Bid Calibration Module (BCM), which work collaboratively to excavate the posterior auction signals better and enhance the performance of CTR prediction. Furthermore, the two proposed modules are lightweight, model-agnostic and friendly to inference latency. Extensive experiments are conducted on a public dataset and an industrial dataset to demonstrate the effectiveness and compatibility of AIE. Besides, a one-month online A/B test in a large-scale advertising platform shows that AIE improves the base model by 5.76% and 2.44% in terms of eCPM and CTR, respectively.</p>
    <p><strong>Categories:</strong> Algorithm Design, Online Advertising, Recommendation Systems, Auction Mechanisms, CTR Prediction, Model Performance, Bias Mitigation, A/B Test, Evaluation Metrics, Scalability, Real-World Applications, Beyond Accuracy (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1021/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>It’s Not You, It’s Me: The Impact of Choice Models and Ranking Strategies on Gender Imbalance in Music Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Michael Ekstrand, Andres Ferraro, Christine Bauer</p>
    <p>As recommender systems are prone to various biases, mitigation approaches are needed to ensure that recommendations are fair to various stakeholders. One particular concern in music recommendation is artist gender fairness. Recent work has shown that the gender imbalance in the sector translates to the output of music recommender systems, creating a feedback loop that can reinforce gender biases over time. In this work, we examine whether algorithmic strategies or user behavior are a greater contributor to ongoing improvement (or loss) in fairness as models are repeatedly re-trained on new user feedback data. We simulate this repeated process to investigate the effects of ranking strategies and user choice models on gender fairness metrics. We find re-ranking strategies have a greater effect than user choice models on recommendation fairness over time</p>
    <p><strong>Categories:</strong> Re-ranking, Recommendation Algorithms, Music, Gender Imbalance, Bias Mitigation, Fairness Metrics, Algorithmic Bias, Feedback Loops, Evaluation of Algorithms, Model Retraining (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1094/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Not All Videos Become Outdated: Short-Video Recommendation by Learning to Deconfound Release Interval Bias (2024)</h3>
    <p><strong>Authors:</strong> Aixin Sun, Guoxiu He, Lulu Dong</p>
    <p>Short-video recommender systems often exhibit a biased preference to recently released videos. However, not all videos become outdated; certain classic videos can still attract user’s attention. Such bias along temporal dimension can be further aggravated by the matching model between users and videos, because the model learns from preexisting interactions. From real data, we observe that different videos have varying sensitivities to recency in attracting users’ attention. Our analysis, based on a causal graph modeling short-video recommendation, suggests that the release interval serves as a confounder, establishing a backdoor path between users and videos. To address this confounding effect, we propose a model-agnostic causal architecture called Learning to Deconfound the Release Interval Bias (LDRI). LDRI enables jointly learning of the matching model and the video recency sensitivity perceptron. In the inference stage, we apply a backdoor adjustment, effectively blocking the backdoor path by intervening on each video. Extensive experiments on two benchmarks demonstrate that LDRI consistently outperforms backbone models and exhibits superior performance against state-of-the-art models. Additional comprehensive analyses confirm the deconfounding capability of LDRI. Our code is available online: https://anonymous.4open.science/r/LDRI/.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Short-Video Recommendations, Bias Mitigation, Temporal Dynamics, Model Agnostic, Causal Inference, User Modeling, Evaluation Methods, Matching Models, Confounding Variables, Recency Sensitivity (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1053/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>GenUI(ne) CRS: UI Elements and Retrieval-Augmented Generation in Conversational Recommender Systems with LLMs (2024)</h3>
    <p><strong>Authors:</strong> Lien Michiels, Annelien Smets, Ulysse Maes</p>
    <p>Previous research has used Large Language Models (LLMs) to develop personalized Conversational Recommender Systems (CRS) with text-based user interfaces (UIs). However, the potential of LLMs to generate interactive graphical elements that enhance user experience remains largely unexplored. To address this gap, we introduce “GenUI(ne) CRS,” a novel framework designed to leverage LLMs for adaptive and interactive UIs. Our framework supports domain-specific graphical elements such as buttons and cards, in addition to text-based inputs. It also addresses the common LLM issue of outdated knowledge, known as the “knowledge cut-off,” by implementing Retrieval-Augmented Generation (RAG). To illustrate its potential, we developed a prototype movie CRS. This work demonstrates the feasibility of LLM-powered interactive UIs and paves the way for future CRS research, including user experience validation, transparent explanations, and addressing LLM biases.</p>
    <p><strong>Categories:</strong> Conversational Recommender Systems (CRS), Large Language Models (LLMs), UI/UX Design, Graphical User Interfaces (GUI), Retrieval-Augmented Generation (RAG), Movies, Human-Computer Interaction, User Experience, Transparency in Recommendations, Bias Mitigation (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1206/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Fairness explanation in recommender systems (2024)</h3>
    <p><strong>Authors:</strong> Luan Souza</p>
    <p>Fairness in recommendations is an emerging area in recommender systems, aiming to mitigate discriminations against individuals or/and groups of individuals in recommendations. These mitigation strategies rely on statistical bias detection, which is a non-trivial task that requires complex analysis and interventions to ensure fairness in these engines. Furthermore, fairness interventions in recommender systems involve a tradeoff between fairness and performance of the recommendation lists, impacting the user experience with less accurate lists. In this context, fairness interventions with explanations have been proposed recently, mitigating discrimination in recommendation lists and providing explainability about the recommendation process and the impact of the fairness interventions. However, in spite of the different approaches it is still not clear how these proposals compare with each other, even those that propose to mitigate the same kind of bias. In addition, the contribution of these different explainable algorithmic fairness approaches to users’ fairness perceptions was not explored until the moment. Looking at these gaps, our doctorate project aims to investigate how these explainable fairness proposals compare to each other and how they are perceived by the users, in order to identify which fairness interventions and explanation strategies are most promising to increase transparency and fairness perceptions of recommendation lists.</p>
    <p><strong>Categories:</strong> Fairness, Explainability, Recommender Systems, Bias Mitigation, Tradeoff Between Fairness and Performance, User Perception of Fairness, Transparency, Emerging Areas, Research Project Overview (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1142/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Interface Design to Mitigate Inflation in Recommender Systems (2023)</h3>
    <p><strong>Authors:</strong> Rana Shahout, Nikhil Garg, Sasha Stoikov, Yehonatan Peisakhovsky</p>
    <p>Recommendation systems rely on user-provided data to learn about item quality and provide personalized recommendations. An implicit assumption when aggregating ratings into item quality is that ratings are strong indicators of item quality. In this work, we analyze this assumption using data collected from a music discovery application. Our study focuses on two factors that cause rating inflation: heterogeneous user rating behavior and the dynamics of personalized recommendations. We show that user rating behavior is significantly variable, leading to item quality estimates that reflect the users who rated an item more than the item quality itself. Additionally, items that are more likely to be shown via personalized recommendations can experience a substantial increase in their exposure and potential bias toward them. To mitigate these effects, we conducted a randomized controlled trial where the rating interface was modified. This resulted in a substantial improvement in user rating behavior and a reduction in item quality inflation. These findings highlight the importance of carefully considering the assumptions underlying recommendation systems and designing interfaces that encourage accurate rating behavior.</p>
    <p><strong>Categories:</strong> Recommendation Systems, User Behavior, Rating Inflation, Interface Design, Randomized Controlled Trials, Personalization Strategies, Music Discovery, Bias Mitigation, Data Quality, Evaluation of Recommendations (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/927/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation (2023)</h3>
    <p><strong>Authors:</strong> Jizhi Zhang, Yang Zhang, Keqin Bao, Wenjie Wang, Fuli Feng, Xiangnan He</p>
    <p>The resounding triumph of the Large Language Models (LLMs) has ushered in a novel LLM for recommendation (LLM4rec) paradigm. Notwithstanding, the capacity of LLM4rec to provide equitable recommendations remains uncharted due to the potential presence of societal prejudices in LLMs. In order to avert the plausible hazard of employing LLM4rec, we scrutinize the fairness of LLM4rec with respect to the users’ sensitive attributes. Owing to the disparity between LLM4rec and the conventional recommendation paradigm, there are challenges in utilizing the conventional recommendation fairness benchmark directly. To explore the fairness of recommendations under the LLM4rec, we propose a new benchmark Fairness in Large language models for Recommendation (FairLR), which consists of carefully designed metrics and a dataset that considers eight sensitive attributes in two recommendation scenarios: music and movie. We utilize our FairLR benchmark to examine ChatGPT and expose that it still demonstrates bias towards certain sensitive attributes while making recommendations. Our code and dataset can be found at https://anonymous.4open.science/r/FairLR-751D/.</p>
    <p><strong>Categories:</strong> Fairness in Recommendation Systems, Large Language Models (LLMs), Traditional vs. Modern Recommendation Systems, Domain-Specific Recommendations, Evaluation Metrics for Recommendation Systems, Methodological Challenges, Bias Mitigation, Ethical Considerations in AI, System Fairness (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/945/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>When Fairness meets Bias: a Debiased Framework for Fairness aware Top-N Recommendation (2023)</h3>
    <p><strong>Authors:</strong> Zhi Gong, Jingsen Zhang, Jiakai Tang, Xu Chen, Shiqi Shen, Zhipeng Wang</p>
    <p>Fairness in the recommendation domain has recently attracted increasing attention due to the more and more concerns on the algorithm discrimination and ethics. While recent years have witnessed many promising fairness aware recommender models, an important problem has been largely ignored, that is, the fairness can be biased due to the user personalized selection tendencies or the non-uniform item exposure probabilities. To study this problem, in this paper, we formally define a novel task named as unbiased fairness aware Top-N recommendation. For solving this task, we firstly define an ideal loss function based on all the user-item pairs. Considering that, in real-world datasets, only a small number of user-item interactions can be observed, we then approximate the above ideal loss with a more tractable objective based on the inverse propensity score (IPS). Since the recommendation datasets can be noisy and quite sparse, which brings difficulties for accurately estimating the IPS, we propose to optimize the objective in an IPS range instead of a specific point, which improve the model fault tolerance capability. In order to make our model more applicable to the commonly studied Top-N recommendation, we soften the ranking metrics such as Precision, Hit-Ratio and NDCG to derive an fully differentiable framework. We conduct extensive experiments to demonstrate the effectiveness of our model based on four real-world datasets.</p>
    <p><strong>Categories:</strong> Bias Mitigation, Fairness, Recommendation Systems, Top-N Recommendation, Inverse Propensity Score (IPS), User Preferences, Bias-Variance Tradeoff, Social Aspects of Recommendation, Evaluation Metrics, Framework Proposal, Novel Methodology (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/892/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Evaluating The Effects of Calibrated Popularity Bias Mitigation: A Field Study (2023)</h3>
    <p><strong>Authors:</strong> Astrid Tessem, Anastasiia Klimashevskaia, Dietmar Jannach, Lars Skjærven, Christoph Trattner, Mehdi Elahi</p>
    <p>Despite their proven various benefits, Recommender Systems can cause or amplify certain undesired effects. In this paper, we focus on Popularity Bias, i.e., the tendency of a recommender system to utilize the effect of recommending popular items to the user. Prior research has studied the negative impact of this type of bias on individuals and society as a whole and proposed various approaches to mitigate this in various domains. However, almost all works adopted offline methodologies to evaluate the effectiveness of the proposed approaches. Unfortunately, such offline simulations can potentially be rather simplified and unable to capture the full picture. To contribute to this line of research and given a particular lack of knowledge about how debiasing approaches work not only offline, but online as well, we present in this paper the results of user study on a national broadcaster movie streaming platform in [country]1, i.e., [platform], following the A/B testing methodology. We deployed an effective mitigation approach for popularity bias, called Calibrated Popularity (CP), and monitored its performance in comparison to the platform’s existing collaborative filtering recommendation approach as a baseline over a period of almost four months. The results obtained from a large user base interacting in real-time with the recommendations indicate that the evaluated debiasing approach can be effective in addressing popularity bias while still maintaining the level of user interest and engagement</p>
    <p><strong>Categories:</strong> Recommender Systems, Popularity Bias, Field Study, A/B Testing, Collaborative Filtering, Real-Time Interaction, User Interest/Engagement, Streaming Platforms, Bias Mitigation, Evaluation Methodology, Real-World Applications, User Behavior Analysis (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/953/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Adversary or Friend? An adversarial Approach to Improving Recommender Systems (2022)</h3>
    <p><strong>Authors:</strong> Pannaga Shivaswamy, Dario Garcia Garcia</p>
    <p>Typical recommender systems models are trained to have good average performance across all users or items. In practice, this results in model performance that is good for some users but sub-optimal for many users. In this work, we consider adversarially trained machine learning models and extend them to recommender systems problems. The adversarial models are trained with no additional demographic or other information than already available to the learning algorithm. We show that adversarially reweighted learning models give more emphasis to dense areas of the feature-space that incur high loss during training. We show that a straightforward adversarial model adapted to recommender systems can fail to perform well and that a carefully designed adversarial model can perform much better. The proposed models are trained using a standard gradient descent/ascent approach that can be easily adapted to many recommender problems. We compare our results with an inverse propensity weighting based baseline that also works well in practice. We delve deep into the underlying experimental results and show that, for the users who are under-served by the baseline model, the adversarial models can achieve significantly better results.</p>
    <p><strong>Categories:</strong> Adversarial Training, Recommender Systems, Machine Learning, Recommendation Algorithms, User-Specific Recommendations, Improving Recommendation Accuracy, Fairness in AI, Bias Mitigation, Beyond Accuracy, Algorithmic Fairness, Optimization Techniques, Fairness in Recommendations, Bias in AI, Algorithmic Bias (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/748/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Countering Popularity Bias by Regularizing Score Differences (2022)</h3>
    <p><strong>Authors:</strong> Sung Min Cho, Wondo Rhee, Bongwon Suh</p>
    <p>Recommendation system often suffers from popularity bias. Often the training data inherently exhibits long-tail distribution in item popularity (data bias). Moreover, the recommendation systems could give unfairly higher recommendation scores to popular items even among items a user equally liked, resulting in over-recommendation of popular items (model bias). In this study we propose a novel method to reduce the model bias while maintaining accuracy by directly regularizing the recommendation scores to be equal across items a user preferred. Akin to contrastive learning, we extend the widely used pairwise loss (BPR loss) which maximizes the score differences between preferred and unpreferred items, with a regularization term that minimizes the score differences within preferred and unpreferred items, respectively, thereby achieving both high debias and high accuracy performance with no additional training. To test the effectiveness of the proposed method, we design an experiment using a synthetic dataset which induces model bias with baseline training; we showed applying the proposed method resulted in drastic reduction of model bias while maintaining accuracy. Comprehensive comparison with earlier debias methods showed the proposed method had advantages in terms of computational validity and efficiency. Further empirical experiments utilizing four benchmark datasets and four recommendation models indicated the proposed method showed general improvements over performances of earlier debias methods. We hope that our method could help users enjoy diverse recommendations promoting serendipitous findings. Code available at https://github.com/stillpsy/popbias.</p>
    <p><strong>Categories:</strong> Popularity Bias, Recommendation Systems, Regularization, Pairwise Loss, Loss Functions, Evaluation Metrics, Fairness, Diversity of Recommendations, Model Accuracy, Bias Mitigation, Benchmark Datasets (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/753/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Mitigating Confounding Bias in Recommendation via Information Bottleneck (2021)</h3>
    <p><strong>Authors:</strong> Hong Zhu, Weike Pan, Zhong Ming, Zhenhua Dong, Dugang Liu, Xiuqiang He, Pengxiang Cheng</p>
    <p>How to effectively mitigate the bias of feedback in recommender systems is an important research topic. In this paper, we first describe the generation process of the biased and unbiased feedback in recommender systems via two respective causal diagrams, where the difference between them can be regarded as the source of bias. We then define this difference as a confounding bias, which can be regarded as a collection of some specific biases that have previously been studied. For the case with biased feedback alone, we derive the conditions that need to be satisfied to obtain a debiased representation from the causal diagrams. Based on information theory, we propose a novel method called debiased information bottleneck (DIB) to optimize these conditions and then find a tractable solution for it. In particular, the proposed method constrains the model to learn a biased embedding vector with independent biased and unbiased components in the training phase, and uses only the unbiased component in the test phase to deliver more accurate recommendations. Finally, we conduct extensive experiments on a public dataset and a real product dataset to verify the effectiveness of the proposed method and discuss its properties.</p>
    <p><strong>Categories:</strong> Bias Mitigation, Causality, Information Bottleneck, Debiased Representation, Causal Models, Recommendation Algorithms, Real-World Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/650/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Elsevier, Fairness in Recommendations (2021)</h3>
    <p><strong>Authors:</strong> Daniel Kershaw</p>
    <p>At Elsevier we aim to help scientists further their research. On the one hand, by offering a platform for publishing ground-breaking research, and on the other by helping researchers discover relevant content to assist their work. In our team, Editorial Data Science, we support the former goal by providing tools to help editors in the decisions that they make, from finding reviewers to recommending transfers for manuscripts to more appropriate journals. To improve the workflows for publishing research and help editors in finding relevant reviewers we developed a reviewer recommender. The reviewer recommender will, based on a submitted manuscript, recommend reviewers that optimise the fit between the manuscript, the journal, and the reviewer themselves. To achieve this, the reviewer recommender is built on top of the three principles of quality data, expert knowledge, and experimentation. We leverage data from across Elsevier, ranging from Scopus profiles which includes publication histories and academic impact data, to data from our publishing platform which includes historic invites and subsequent actions around submitted manuscripts. All this data is used to develop and evaluate appropriate models for recommending reviewers. There are many factors to consider when deciding if someone is relevant to review a submitted manuscript. To name a few, the topic of interest of the reviewer, level of expertise of the reviewer, fit of the reviewer to the journal. But we are ultimately looking for the fit between the three stake holders: the manuscript, the journal, and the reviewer. However, it is not as simple as looking for a fit purely on topic or relevance. We can find evidence for that in the growth of research into recommender systems for multi-sided market [ 7] places where all sides within the systems must be considered. In our case, in addition to topical fit, we must consider fairness in recommendations, where minority groups get equal and fair treatment. For this reason, we consider two ’additional’ constraints when developing recommender systems at Elsevier, fairness in the recommendations and respecting the reviewers time and privacy. Fairness in Recommendations is important in recommending people particular within academic fields where there are known biases against minority groups. This bias is present in data which we use. [REF] showed that women and minorities over their academic and research careers are disadvantaged compared to members of the majority groups, either through less research output due to more teaching [ 6] or taking time out to raise families which resulted in lower number of research output [8]. Though bias is not only limited to quantities of research output [ 4 ] showed that women receive fewer citations even when controlling to quantity of research output and gender. This effect of receiving proportionally fewer citation is also amplified [ 5] identified that women are disproportionally ranked lower on the author list of a paper. These observations are not limited to gender but also race and age. Using these features derived from bibliometric analysis without due care within a model could lead to adverse impact against minority groups, resulting in the reviewers from these groups being recommended less and thus less invites to review manuscripts, whilst making sure any interventions does not create additional harms. Respecting user privacy and time. We know through user research that there is also the need to respect the researcher whom we are asking to review these manuscripts. Either because they are too busy to accept another review, or that they feel the manuscript is out of their scope and they are invited to review irrelevant papers. In this talk we show how at Elsevier we build fairness and understanding of stake holders into our recommenders, particularly the reviewer recommender. We experimented with a variety of technologies, ranging from simple search to sophisticated ML models. They all follow the common IR approach of candidate selection followed by re-ranking. All approaches use roughly the same features representing the reviewer (topics they have published in), the journal (topics the journal publishes and metrics about the reviewers they have selected before), the manuscript (key concepts within the manuscript). Through iterating over different models and techniques, we understand the importance of different features, along with their impact on model performance. But more specifically which features have inherent bias built into them, which feature are most predictive , and which fairness aware methods have been used to mitigate these biases [1, 2]. We will then discuss the methods and challenges of including bias metrics in the evaluation and development of the reviewer recommender. In this we focus on how we evaluate fairness from historical data along with group-wise fairness metrics such as demographic parity and equal opportunity. We then discuss the challenges and successes of monitoring these metrics in production [3]. This combination of feature understanding and fairness evaluation at every stage in the model development means we have a deep understanding of how the model works, who it impacts, and how. Resulting in a fair model which keeps the reviewers at the heart of the review process. However, we must not forget, this work sits within the wider academic review system, where bias is introduced on many levels through hiring, promotions and research engagement. Even though a reviewer recommender can not correct the underlying bias, it can contribute to a fairer and more equal outcome.</p>
    <p><strong>Categories:</strong> Fairness in Recommendations, Academic Publishing, Reviewer Recommenders, Machine Learning Models, Information Retrieval, Bias Mitigation, Gender Bias, Minority Groups, Group-wise Fairness Metrics, Demographic Parity, Equal Opportunity, Systemic Bias, Feature Analysis, Model Transparency, Reviewer Privacy, Time Management (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/720/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>