<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">Algorithm Comparison</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/bias-mitigation/">Bias Mitigation</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/e-commerce/">E-commerce</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Integrating Offline Reinforcement Learning with Transformers for Sequential Recommendation (2023)</h3>
    <p><strong>Authors:</strong> Xumei Xi, Yang Wu, Liwen Ouyang, Yuke Zhao, Quan Liu</p>
    <p>We consider the problem of sequential recommendation, where the current recommendation is made based on past interactions. This recommendation task requires efficient processing of the sequential data and aims to provide recommendations that maximize the long-term reward. To this end, we train a farsighted recommender by using an offline RL algorithm with the policy network in our model architecture that has been initialized from a pre-trained transformer model. The pre-trained model leverages the superb ability of the transformer to process sequential information. Compared to prior works that rely on online interaction via simulation, we focus on implementing a fully offline RL framework that is able to converge in a fast and stable way. Through extensive experiments on public datasets, we show that our method is robust across various recommendation regimes, including e-commerce and movie suggestions. Compared to state-of-the-art supervised learning algorithms, our algorithm yields recommendations of higher quality, demonstrating the clear advantage of combining RL and transformers.</p>
    <p><strong>Categories:</strong> Reinforcement Learning, Transformers, Sequential Recommendation, Offline Reinforcement Learning, Sequential Data Processing, Transfer Learning, E-commerce, Movies, Recommendation System, Long-term Reward Maximization, Algorithm Comparison, Efficiency (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/955/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Towards Sustainability-aware Recommender Systems: Analyzing the Trade-off Between Algorithms Performance and Carbon Footprint (2023)</h3>
    <p><strong>Authors:</strong> Giovanni Semeraro, Giuseppe Spillo, Allegra De Filippo, Cataldo Musto, Michela Milano</p>
    <p>In this paper, we present a comparative analysis of the trade-off between the performance of state-of-the-art recommendation algorithms and their sustainability. In particular, we compared 18 popular recommendation algorithms in terms of both standard metrics (i.e., accuracy and diversity of the recommendations) as well as in terms of energy consumption and carbon footprint on three different datasets. In order to obtain a fair comparison, all the algorithms were run based on the implementations available in a popular recommendation library, i.e., RecBole, and used the same experimental settings. The outcomes of the experiments clearly showed that the choice of the optimal recommendation algorithm requires a thorough analysis, since more sophisticated algorithms often led to tiny improvements at the cost of an exponential increase of carbon emissions. Through this paper, we aim to shed light on the problem of carbon footprint and energy consumption of recommender systems, and we make the first step towards the development of sustainability-aware recommendation algorithms.</p>
    <p><strong>Categories:</strong> Sustainability-aware Recommender Systems, Recommendation Algorithms, Algorithm Comparison, Carbon Footprint, Energy Consumption, Accuracy Metrics, Diversity of Recommendations, Sustainability, Real World Applications, Performance Metrics, Green Computing, Energy-efficient Algorithms, Recommendation Systems, Recommender Systems, Sustainability-aware RS (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/912/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Turning Dross Into Gold Loss: is BERT4Rec really better than SASRec? (2023)</h3>
    <p><strong>Authors:</strong> Alexey Vasilev, Anton Klenitskiy</p>
    <p>Recently sequential recommendations and next-item prediction task has become increasingly popular in the field of recommender systems. Currently, two state-of-the-art baselines are Transformer-based models SASRec and BERT4Rec. Over the past few years, there have been quite a few publications comparing these two algorithms and proposing new state-of-the-art models. In most of the publications, BERT4Rec achieves better performance than SASRec. But BERT4Rec uses cross-entropy over softmax for all items, while SASRec uses negative sampling and calculates binary cross-entropy loss for one positive and one negative item. In our work, we show that if both models are trained with the same loss, which is used by BERT4Rec, then SASRec will significantly outperform BERT4Rec both in terms of quality and training speed. In addition, we show that SASRec could be effectively trained with negative sampling and still outperform BERT4Rec, but the number of negative examples should be much larger than one.</p>
    <p><strong>Categories:</strong> Sequential Recommendations, Transformer-Based Models, BERT4Rec, SASRec, Algorithm Comparison, Loss Functions, Evaluation Metrics, State-of-the-Art Models, Training Efficiency, Model Performance, Recommender Systems, Negative Sampling, Cross-Entropy Loss, Binary Cross-Entropy Loss (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/966/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Everyone’s a Winner! On Hyperparameter Tuning of Recommendation Models (2023)</h3>
    <p><strong>Authors:</strong> Dietmar Jannach, Faisal Shehzad</p>
    <p>The performance of a recommender system algorithm in terms of common offline accuracy measures often strongly depends on the chosen hyperparameters. Therefore, when comparing algorithms in offline experiments, we can obtain reliable insights regarding the effectiveness of a newly proposed algorithm only if we compare it to a number of state-of-the-art baselines that are carefully tuned for each of the considered datasets. While this fundamental principle of any area of applied machine learning is undisputed, we find that the tuning process for the baselines in the current literature is barely documented in much of today’s published research. Ultimately, in case the baselines are actually not carefully tuned, progress may remain unclear. In this paper, we showcase how every method in such an unsound comparison can be reported to be outperforming the state-of-the-art. Finally, we iterate appropriate research practices to avoid unreliable algorithm comparisons in the future.</p>
    <p><strong>Categories:</strong> Algorithm Comparison, Hyperparameter Tuning, Reproducibility, Research Methodology, Model Evaluation, Experimental Design, Best Practices, Recommendation Systems, Research Practices (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/942/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Revisiting the Performance of iALS on Item Recommendation Benchmarks (2022)</h3>
    <p><strong>Authors:</strong> Yehuda Koren, Walid Krichene, Steffen Rendle, Li Zhang</p>
    <p>Matrix factorization learned by implicit alternating least squares (iALS) is a popular baseline in recommender system research publications. iALS is known to be one of the most computationally efficient and scalable collaborative filtering methods. However, recent studies suggest that its prediction quality is not competitive with the current state of the art, in particular autoencoders and other item-based collaborative filtering methods. In this work, we revisit four well-studied benchmarks where iALS was reported to perform poorly and show that with proper tuning, iALS is highly competitive and outperforms any method on at least half of the comparisons. We hope that these high quality results together with iALS’s known scalability spark new interest in applying and further improving this decade old technique.</p>
    <p><strong>Categories:</strong> Matrix Factorization, Collaborative Filtering, Item Recommendation, Recommender Systems, Performance Evaluation, Computational Efficiency, Benchmark Testing, Scalability, Algorithm Comparison, Research Methodology (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/780/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Identifying New Podcasts with High General Appeal Using a Pure Exploration Infinitely-Armed Bandit Strategy (2022)</h3>
    <p><strong>Authors:</strong> Hugues Bouchard, Javed Aslam, Kevin Jamieson, Maryam Aziz, Alice Y. Wang, Jesse Anderton</p>
    <p>Podcasting is an increasingly popular medium for entertainment and discourse around the world, with tens of thousands of new podcasts released on a monthly basis. We consider the problem of identifying from these newly-released podcasts those with the largest potential audiences so they can be considered for personalized recommendation to users. We first study and then discard a supervised approach due to the inadequacy of either content or consumption features for this task, and instead propose a novel non-contextual bandit algorithm in the fixed-budget infinitely-armed pure-exploration setting. We demonstrate that our algorithm is well-suited to the best-arm identification task for a broad class of arm reservoir distributions, out-competing a large number of state-of-the-art algorithms. We then apply the algorithm to identifying podcasts with broad appeal in a simulated study, and show that it efficiently sorts podcasts into groups by increasing appeal while avoiding the popularity bias inherent in supervised approaches.</p>
    <p><strong>Categories:</strong> Multi-Armed Bandits, Pure Exploration, Podcasts, Entertainment, Recommendation Systems, Cold Start, Resource Allocation, Best Arm Identification, Simulation Studies, Algorithm Comparison, Mitigating Bias, Beyond Accuracy Evaluation (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/766/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Online Evaluation Methods for the Causal Effect of Recommendations (2021)</h3>
    <p><strong>Authors:</strong> Masahiro Sato</p>
    <p>Evaluating the causal effect of recommendations is an important objective because the causal effect on user interactions can directly leads to an increase in sales and user engagement. To select an optimal recommendation model, it is common to conduct A/B testing to compare model performance. However, A/B testing of causal effects requires a large number of users, making such experiments costly and risky. We therefore propose the first interleaving methods that can efficiently compare recommendation models in terms of causal effects. In contrast to conventional interleaving methods, we measure the outcomes of both items on an interleaved list and items not on the interleaved list, since the causal effect is the difference between outcomes with and without recommendations. To ensure that the evaluations are unbiased, we either select items with equal probability or weight the outcomes using inverse propensity scores. We then verify the unbiasedness and efficiency of online evaluation methods through simulated online experiments. The results indicate that our proposed methods are unbiased and that they have superior efficiency to A/B testing.</p>
    <p><strong>Categories:</strong> Causal Effect Evaluation, Recommendation Systems, Online Experiments, Interleaving Methods, A/B Testing, Causal Inference, Algorithm Comparison, User Engagement, Efficiency Optimization, Unbiased Evaluation (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/648/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Neural Collaborative Filtering vs. Matrix Factorization Revisited (2020)</h3>
    <p><strong>Authors:</strong> John Anderson, Steffen Rendle, Walid Krichene, Li Zhang</p>
    <p>Embedding based models have been the state of the art in collaborative filtering for over a decade. Traditionally, the dot product or higher order equivalents have been used to combine two or more embeddings, e.g., most notably in matrix factorization. In recent years, it was suggested to replace the dot product with a learned similarity e.g. using a multilayer perceptron (MLP). This approach is often referred to as neural collaborative filtering (NCF). In this work, we revisit the experiments of the NCF paper that popularized learned similarities using MLPs. First, we show that with a proper hyperparameter selection, a simple dot product substantially outperforms the proposed learned similarities. Second, while a MLP can in theory approximate any function, we show that it is non-trivial to learn a dot product with an MLP. Finally, we discuss practical issues that arise when applying MLP based similarities and show that MLPs are too costly to use for item recommendation in production environments while dot products allow to apply very efficient retrieval algorithms. We conclude that MLPs should be used with care as embedding combiner and that dot products might be a better default choice.</p>
    <p><strong>Categories:</strong> Neural Collaborative Filtering, Matrix Factorization, Algorithm Comparison, Hyperparameters, Recommendation Systems, Evaluation Metrics, Production Systems, Efficiency, Cost Analysis, Traditional Methods vs. Neural Methods, Practical Recommendations (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/590/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Performance Comparison of Neural and Non-Neural Approaches to Session-based Recommendation (2019)</h3>
    <p><strong>Authors:</strong> Malte Ludewig, Sara Latifi, Noemi Mauro, Dietmar Jannach</p>
    <p>The benefits of neural approaches are undisputed in many application areas. However, today’s research practice in applied machine learning‚ where researchers often use a variety of baselines, datasets, and evaluation procedures, can make it difficult to understand how much progress is actually achieved through novel technical approaches. In this work, we focus on the fast-developing area of session-based recommendation and aim to contribute to a better understanding of what represents the state-of-the-art. To that purpose, we have conducted an extensive set of experiments, using a variety of datasets, in which we benchmarked four neural approaches that were published in the last three years against each other and against a set of simpler baseline techniques, e.g., based on nearest neighbors. The evaluation of the algorithms under the exact same conditions revealed that the benefits of applying today’s neural approaches to session-based recommendations are still limited. In the majority of the cases, and in particular when precision and recall are used, it turned out that simple techniques in most cases outperform recent neural approaches. Our findings therefore point to certain major limitations of today’s research practice. By sharing our evaluation framework publicly, we hope that some of these limitations can be overcome in the future. i>Presentation: Tuesday Poster Lunch</i</p>
    <p><strong>Categories:</strong> Neural Networks, Recommendation Systems, Session-Based Recommendations, Algorithm Comparison, Performance Evaluation, Machine Learning, Evaluation Framework, Research Practices (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/490/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>A Comparison of Calibrated and Intent-Aware Recommendations (2019)</h3>
    <p><strong>Authors:</strong> Derek Bridge, Mesut Kaya</p>
    <p>Calibrated and intent-aware recommendation are recent approaches to recommendation that have apparent similarities. Both try, to a certain extent, to cover the user’s interests, as revealed by her user profile. In this paper, we compare them in detail. On two datasets, we show the extent to which intent-aware recommendations are calibrated and the extent to which calibrated recommendations are diverse. We consider two ways of defining a user’s interests, one based on item features, the other based on subprofiles of the user’s profile. We find that defining interests in terms of subprofiles results in highest precision and the best relevance/diversity trade-off. Along the way, we define a new version of calibrated recommendation and three new evaluation metrics.</p>
    <p><strong>Categories:</strong> Calibrated Recommendations, Intent-Aware Recommendations, Recommendation Comparison, Evaluation Metrics, Diversity of Recommendations, Relevance vs Diversity Trade-off, Precision in Recommendations, User Interests, User Profiling, Algorithm Comparison (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/429/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>User-Centric Evaluation of Session-Based Recommendations for an Automated Radio Station (2019)</h3>
    <p><strong>Authors:</strong> Dietmar Jannach, Malte Ludewig</p>
    <p>The creation of an automated and virtually endless playlist given a start item is a common feature of modern media streaming services. When no past information about the user’s preferences is available, the creation of such playlists can be done using session-based recommendation techniques. In this case, the recommendations only depend on the start item and the user’s interactions in the current listening session, such as ‘liking’ or skipping an item. In recent years, various novel session-based techniques were proposed, often based on deep learning. The evaluation of such approaches is in most cases solely based on offline experimentation and abstract accuracy measures. However, such evaluations cannot inform us about the quality as perceived by users. To close this research gap, we have conducted a user study (N=250), where the participants interacted with an automated online radio station. Each treatment group received recommendations that were generated by one of five different algorithms. Our results show that comparably simple techniques led to quality perceptions that are similar or even better than when a complex deep learning mechanism or Spotify’s recommendations are used. The simple mechanisms, however, often tend to recommend comparably popular tracks, which can lead to lower discovery effects. i>Presentation: Wednesday Poster Lunch</i</p>
    <p><strong>Categories:</strong> Session-Based Recommendations, User-Centric Evaluation, Recommendation Systems, Media Streaming, Automated Radio, Deep Learning, Algorithm Comparison, A/B Testing, User Study, Real-World Applications, Perceived Quality, Discovery Effects, Beyond Accuracy (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/499/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Exploring Author Gender in Book Rating and Recommendation (2018)</h3>
    <p><strong>Authors:</strong> Hoda Mehrpouyan, Mohammed Imran R. Kazi, Michael D. Ekstrand, Mucun Tian, Daniel Kluver</p>
    <p>Collaborative filtering algorithms find useful patterns in rating and consumption data and exploit these patterns to guide users to good items. Many of the patterns in rating datasets reflect important real-world differences between the various users and items in the data; other patterns may be irrelevant or possibly undesirable for social or ethical reasons, particularly if they reflect undesired discrimination, such as discrimination in publishing or purchasing against authors who are women or ethnic minorities. In this work, we examine the response of collaborative filtering recommender algorithms to the distribution of their input data with respect to a dimension of social concern, namely content creator gender. Using publicly-available book ratings data, we measure the distribution of the genders of the authors of books in user rating profiles and recommendation lists produced from this data. We find that common collaborative filtering algorithms differ in the gender distribution of their recommendation lists, and in the relationship of that output distribution to user profile distribution.</p>
    <p><strong>Categories:</strong> Collaborative Filtering, Book Domain, Gender Bias, Fairness, Authorship, Recommendation Evaluation, Diversity of Recommendations, Algorithm Comparison, Real World Data Analysis, Ethical Considerations in AI, Input-Output Relationships, Discrimination (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/336/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Item2vec: Neural Item Embedding for Collaborative Filtering (2016)</h3>
    <p><strong>Authors:</strong> Oren Barkan, Noam Koenigstein</p>
    <p>Many Collaborative Filtering (CF) algorithms are item-based in the sense that they analyze item-item relations in order to produce item similarities. Recently, several works in the field of Natural Language Processing (NLP) suggested to learn a latent representation of words using neural embedding algorithms. Among them, the Skip-gram with Negative Sampling (SGNS), also known as Word2vec, was shown to provide state-of-the-art results on various linguistics tasks. In this paper, we show that item-based CF can be cast in the same framework of neural word embedding. Inspired by SGNS, we describe a method we name Item2vec for item-based CF that produces embedding for items in a latent space. The method is capable of inferring item-item relations even when user information is not available. We present experimental results that demonstrate the effectiveness of the Item2vec method and show it is competitive with SVD.</p>
    <p><strong>Categories:</strong> Collaborative Filtering, Neural Networks, Item Embedding, Natural Language Processing Techniques, Matrix Factorization, Recommendation Systems, Algorithm Comparison, Item Similarity, Neural Embeddings, Cross-Domain Methods, Evaluation Metrics (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/247/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Pairwise Preferences Based Matrix Factorization and Nearest Neighbor Recommendation Techniques (2016)</h3>
    <p><strong>Authors:</strong> Marko Tkalcic, Francesco Ricci, Saikishore Kalloori</p>
    <p>Many recommendation techniques rely on the knowledge of preferences data in the form of ratings for items. In this paper, we focus on pairwise preferences as an alternative way for acquiring user preferences and building recommendations. In our scenario, users provide pairwise preference scores for a set of item pairs, indicating how much one item in each pair is preferred to the other. We propose a matrix factorization (MF) and a nearest neighbor (NN) prediction techniques for pairwise preference scores. Our MF solution maps users and items pairs to a joint latent features vector space, while the proposed NN algorithm leverages specific user-to-user similarity functions well suited for comparing users preferences of that type. We compare our approaches to state of the art solutions and show that our solutions produce more accurate pairwise preferences and ranking predictions.</p>
    <p><strong>Categories:</strong> Matrix Factorization, Nearest Neighbor Algorithms, Recommendation Systems, Pairwise Preferences, User Preference Modeling, Latent Features, Collaborative Filtering, Algorithm Comparison (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/210/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Evaluating Tag Recommender Algorithms in Real-World Folksonomies: A Comparative Study (2015)</h3>
    <p><strong>Authors:</strong> Elisabeth Lex, Dominik Kowald</p>
    <p>To date, the evaluation of tag recommender algorithms has mostly been conducted in limited ways, including -core pruned datasets, a small set of compared algorithms and solely based on recommender accuracy. In this study, we use an open-source evaluation framework to compare a rich set of state-of-the-art algorithms in six unfiltered, open datasets via various metrics, measuring not only accuracy but also the diversity, novelty and computational costs of the approaches. We therefore provide a transparent and reproducible tag recommender evaluation in real-world folksonomies. Our results suggest that the efficacy of an algorithm highly depends on the given needs and thus, they should be of interest to both researchers and developers in the field of tag-based recommender systems.</p>
    <p><strong>Categories:</strong> Tag Recommender Algorithms, Folksonomy, Evaluation Metrics, Algorithm Comparison, Open Source Tools, Real-World Applications, Diversity of Recommendations, Beyond Accuracy, Computational Efficiency, Personalization, Reproducibility (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/116/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Measuring the Concentration Reinforcement Bias of Recommender Systems (2015)</h3>
    <p><strong>Authors:</strong> Alexander Tuzhilin, Peter Mountanos, Panagiotis Adamopoulos</p>
    <p>In this paper, we propose new metrics to accurately measure the concentration reinforcement of recommender systems and the enhancement of the “long tail”. We also conduct a comparative analysis of various RS algorithms illustrating the usefulness of the proposed metrics.</p>
    <p><strong>Categories:</strong> Bias, Fairness, Concentration Reinforcement Bias, Long Tail Recommendations, Evaluation Metrics, Algorithm Comparison, Recommender Systems, Empirical Study, Performance Analysis (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/152/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>