<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">Explainable AI (XAI)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/evaluation-metrics/">Evaluation Metrics</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/e-commerce/">E-commerce</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Are We Explaining the Same Recommenders? Incorporating Recommender Performance for Evaluating Explainers (2024)</h3>
    <p><strong>Authors:</strong> Amir Reza Mohammadi, Michael Müller, Eva Zangerle, Andreas Peintner</p>
    <p>Explainability in recommender systems is both crucial and challenging. Among the state-of-the-art explanation strategies, counterfactual explanation provides intuitive and easily understandable insights into model predictions by illustrating how a small change in the input can lead to a different outcome. Recently, this approach has garnered significant attention, with various studies employing different metrics to evaluate the performance of these explanation methods. In this paper, we investigate the metrics used for evaluating counterfactual explainers for recommender systems. Through extensive experiments, we demonstrate that the performance of recommenders has a direct effect on counterfactual explainers and ignoring it results in inconsistencies in the evaluation results of explainer methods. Our findings highlight an additional challenge in evaluating counterfactual explainer methods and underscore the need to report the recommender performance or consider it in evaluation metrics.</p>
    <p><strong>Categories:</strong> Explainable AI (XAI), Recommender Systems, Counterfactual Explanations, Evaluation of Explainability Methods, Evaluation Metrics, Evaluating Recommenders, Performance Impact on Evaluation, Consistency in Evaluation, Recommendation Explainers (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1187/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Explainable Graph Neural Network Recommenders; Challenges and Opportunities (2023)</h3>
    <p><strong>Authors:</strong> Amir Reza Mohammadi</p>
    <p>Graph Neural Networks (GNNs) have demonstrated significant potential in recommendation tasks by effectively capturing intricate connections among users, items, and their associated features. Given the escalating demand for interpretability, current research endeavors in the domain of GNNs for Recommender Systems (RecSys) necessitate the development of explainer methodologies to elucidate the decision-making process underlying GNN-based recommendations. In this work, we aim to present our research focused on techniques to extend beyond the existing approaches for addressing interpretability in GNN-based RecSys.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Graph Neural Networks (GNNs), Explainable AI (XAI), Interpretability, Research Review/Analysis, Methodological Extensions, User-Item Interaction, Trust in AI/Recommenders, Machine Learning, Challenges &amp; Opportunities, Beyond Accuracy (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/980/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Developing a Human-Centered Framework for Transparency in Fairness-Aware Recommender Systems (2022)</h3>
    <p><strong>Authors:</strong> Jessie J. Smith</p>
    <p>Though recommender systems fundamentally rely on human input and feedback, human-centered research in the RecSys discipline is lacking. When recommender systems aim to treat users more fairly, misinterpreting user objectives could lead to unintentional harm, whether or not fairness is part of the aim. When users seek to understand recommender systems better, a lack of transparency could act as an obstacle for their trust and adoption of the platform. Human-centered machine learning seeks to design systems that understand their users, while simultaneously designing systems that the users can understand. In this work, I propose to explore the intersection of transparency and user-system understanding through three phases of research that will result in a Human-Centered Framework for Transparency in Fairness-Aware Recommender Systems.</p>
    <p><strong>Categories:</strong> Recommender Systems, Fairness-Aware Recommendation, Transparency in Recommendations, Human-Centered Design, User-System Interaction, Trust in Recommendations, Ethical AI, Explainable AI (XAI), User Trust (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/815/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>An Interpretable Neural Network Model for Bundle Recommendations (2022)</h3>
    <p><strong>Authors:</strong> Edward C. Malthouse, Xinyi Li</p>
    <p>A users’ preference for a bundle – a set of items that can be purchased together – can be expressed by the utility of this bundle to the user. The multi-attribute utility theory motivate us to characterize the utility of a bundle using its attributes to improve the personalized bundle recommendation systems. This extended abstract for the Doctoral Symposium describes my PhD project for studying the utility of a bundle using its attributes. The steps taken and some preliminary results are presented, with an outline of the future plans.</p>
    <p><strong>Categories:</strong> Bundle Recommendations, Neural Networks in Recommendations, Interpretable Models, Explainable AI (XAI), Personalization, Beyond Accuracy, Preliminary Results, Theoretical Frameworks, User Preference Modeling, Doctoral Research (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/804/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>An Interpretable Recommendation Model for Gerontological Care (2021)</h3>
    <p><strong>Authors:</strong> Paula Castro, Andre Paulino de Lima, Maria Pimentel, Brunela Orlandi, Laurentino Augusto Dantas, Marcelo Garcia Manzato</p>
    <p>Recommender systems have been successfully applied to diverse areas, but their use in the healthcare domain is still rare. One challenge of applying recommender systems to this domain is related to legal concerns about the consequences of provided recommendations. In this work, we advance an expert-in-the-loop, explanation-first approach to tackle this challenge in a specific healthcare niche: gerontological care. A key aspect of the proposed approach is that both recommendations and explanations reflect the structured questionnaire employed by the practitioner to identify patient needs. Another key aspect is that a clinical dataset of patient assessments and respective assigned interventions is used to estimate effects of alternative interventions during the recommendation process. To evaluate the feasibility of this modelling approach, an explanation style was designed with help of practitioners, and a recommendation model was devised and evaluated against a clinical dataset, which was collected by a partner research group working on gerontological primary care. When compared to other traditional recommendation models, the attained precision was competitive across several evaluation conditions. The results suggest that the proposed approach is feasible and may point new ways of adapting recommender systems to play an assistive role in health care.</p>
    <p><strong>Categories:</strong> Interpretable Models, Healthcare, Gerontology, Regulatory Concerns, Expert Collaboration, Patient Needs, Clinical Data, User-Centered Design, Model Evaluation, Feasibility Studies, Method Comparison, Explainable AI (XAI) (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/680/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Providing Explainable Race-Time Predictions and Training Plan Recommendations to Marathon Runners (2020)</h3>
    <p><strong>Authors:</strong> Barry Smyth, Aonghus Lawlor, Brian Caulfield, Ciara Feely</p>
    <p>Millions of people participate in marathon events every year, typically devoting at least 12-16 weeks to building their endurance and fitness so that they can safely complete these gruelling 42.2km races. Most runners follow a training plan that is tailored to their expected finish-time (e.g. sub-4 hours or 4-5 hours), and these plans will prescribe a complex mixture of training sessions to help them achieve these times. However, such plans cannot adapt to the individual needs (fitness levels, changing goals, personal preferences) of runners, providing only broad training guidance rather than more personalised support. The development of wearable sensors and mobile fitness applications facilitates the collection of a large amount of training data from runners. In this paper, we propose a recommender system that utilizes such training data to deliver more personalised training advice to runners, using ideas from case-based reasoning to reuse and adapt the training habits of similar runners. Explainability plays a significant role in this type of system, and we also describe how the predictions and recommendation advice can be presented to runners. An initial off-line evaluation is presented based on a large-scale, real-world dataset.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Personalized Recommendations, Sports Applications, Wearable Technology, Case-Based Reasoning, Explainable AI (XAI), Training Plans, Marathon Running, Real-World Applications, Longitudinal Analysis, Adaptive Systems, Goal-Oriented Recommendations (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/586/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>DualDiv: Diversifying Items and Explanation Styles in Explainable Hybrid Recommendation (2019)</h3>
    <p><strong>Authors:</strong> Masataka Goto, Kosetsu Tsukuda</p>
    <p>In recommender systems, item diversification and explainable recommendations improve users’ satisfaction. Unlike traditional explainable recommendations that display a single explanation for each item, explainable hybrid recommendations display multiple explanations for each item and are, therefore, more beneficial for users. When multiple explanations are displayed, one problem is that similar sets of explanation styles (ESs) such as user-based, item-based, and popularity-based may be displayed for similar items. Although item diversification has been studied well, the question of how to diversify the ESs remains underexplored. In this paper, we propose a method for diversifying ESs and a framework, called DualDiv, that recommends items by diversifying both the items and the ESs. Our experimental results show that DualDiv can increase the diversity of the items and the ESs without largely reducing the recommendation accuracy. i>Presentation: Monday Poster Lunch</i</p>
    <p><strong>Categories:</strong> Recommendation Systems, Explainable AI (XAI), Hybrid Recommendations, Diversification, Explanation Styles (ESs), User Satisfaction, Algorithm Design, Evaluation Metrics, Real-World Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/472/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>IRF: Interactive Recommendation through Dialogue (2019)</h3>
    <p><strong>Authors:</strong> Oznur Alkan, Massimiliano Mattetti, Elizabeth M. Daly, Inge Vejsbjerg, Adi Botea</p>
    <p>Recent research focuses beyond recommendation accuracy, towards human factors that influence the acceptance of recommendations, such as user satisfaction, trust, transparency and sense of control. We present a generic interactive recommender framework that can add interaction functionalities to non-interactive recommender systems. We take advantage of dialogue systems to interact with the user and we design a middleware layer to provide the interaction functions, such as providing explanations for the recommendations, managing users' preferences learnt from dialogue, preference elicitation and refining recommendations based on learnt preferences.</p>
    <p><strong>Categories:</strong> Interactive Recommendations, Preference Elicitation, User Preference Learning, Explainable AI (XAI), Beyond Accuracy, Trust and Transparency, Human-Computer Interaction, Dialogue Systems Integration, Middleware Design, Framework Design (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/506/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>ExpLOD: A Framework for Explaining Recommendations based on the Linked Open Data Cloud (2016)</h3>
    <p><strong>Authors:</strong> Fedelucio Narducci, Pasquale Lops, Marco De Gemmis, Giovanni Semeraro, Cataldo Musto</p>
    <p>In this paper we present ExpLOD, a framework which exploits the information available in the Linked Open Data (LOD) cloud to generate a natural language explanation of the suggestions produced by a recommendation algorithm. The methodology is based on building a graph in which the items liked by a user are connected to the items recommended through the properties available in the LOD cloud. Next, given this graph, we implemented some techniques to rank those properties and we used the most relevant ones to feed a module for generating explanations in natural language. In the experimental evaluation we performed a user study with 308 subjects aiming to investigate to what extent our explanation framework can lead to more transparent, trustful and engaging recommendations. The preliminary results provided us with encouraging findings, since our algorithm performed better than both a non-personalized explanation baseline and a popularity-based one.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Explanation Generation, Linked Open Data (LOD), Natural Language Processing (NLP), User Study, Trust in Recommendations, Explainable AI (XAI), Transparency in Recommendations, Graph-Based Methods, Evaluation Techniques, Framework Design (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/204/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>