<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">Explainability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reinforcement-learning/">Reinforcement Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/evaluation-methods/">Evaluation Methods</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Fairness explanation in recommender systems (2024)</h3>
    <p><strong>Authors:</strong> Luan Souza</p>
    <p>Fairness in recommendations is an emerging area in recommender systems, aiming to mitigate discriminations against individuals or/and groups of individuals in recommendations. These mitigation strategies rely on statistical bias detection, which is a non-trivial task that requires complex analysis and interventions to ensure fairness in these engines. Furthermore, fairness interventions in recommender systems involve a tradeoff between fairness and performance of the recommendation lists, impacting the user experience with less accurate lists. In this context, fairness interventions with explanations have been proposed recently, mitigating discrimination in recommendation lists and providing explainability about the recommendation process and the impact of the fairness interventions. However, in spite of the different approaches it is still not clear how these proposals compare with each other, even those that propose to mitigate the same kind of bias. In addition, the contribution of these different explainable algorithmic fairness approaches to users’ fairness perceptions was not explored until the moment. Looking at these gaps, our doctorate project aims to investigate how these explainable fairness proposals compare to each other and how they are perceived by the users, in order to identify which fairness interventions and explanation strategies are most promising to increase transparency and fairness perceptions of recommendation lists.</p>
    <p><strong>Categories:</strong> Fairness, Explainability, Recommender Systems, Bias Mitigation, Tradeoff Between Fairness and Performance, User Perception of Fairness, Transparency, Emerging Areas, Research Project Overview (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1142/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Explainability in Music Recommender System (2024)</h3>
    <p><strong>Authors:</strong> Shahrzad Shashaani</p>
    <p>Recommendation systems play a crucial role in our daily lives, influencing many of our significant and minor decisions. These systems also have become integral to the music industry, guiding users to discover new content based on their tastes. However, the lack of transparency in these systems often leaves users questioning the rationale behind recommendations. To address this issue, adding transparency and explainability to recommender systems is a promising solution. Enhancing the explainability of these systems can significantly improve user trust and satisfaction. This research focuses on exploring transparency and explainability in the context of recommendation systems, focusing on the music domain. This research can help to understand the gaps in explainability in music recommender systems to create more engaging and trustworthy music recommendations.</p>
    <p><strong>Categories:</strong> Music Recommendations, Explainability, Transparency, User Trust, Recommendation Systems, Human-Computer Interaction, Algorithmic Transparency, Ethical Considerations, Case Study, Music Industry Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1132/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>CEERS: Counterfactual Evaluations of Explanations in Recommender Systems (2024)</h3>
    <p><strong>Authors:</strong> Mikhail Baklanov</p>
    <p>The growing emphasis on explainability in ethical AI, driven by regulations like GDPR, underscores the need for robust explanations of Recommender Systems (RS). Key to the development and research progress of such methods are reproducible, quantifiable evaluation metrics. Traditional human-involved evaluation methods are not reproducible, subjective, costly, and fail to capture the counterfactual nuances of AI explanations. Hence, there is a pressing need for objective and scalable metrics to accurately measure the correctness of explanation methods for recommender systems. Inspired by similar approaches in computer vision, this research aims to propose a counterfactual approach to evaluate explanation accuracy in RS. While counterfactual evaluation methods have been established in other domains, they are underexplored in RS. Our goal is to introduce quantifiable metrics that objectively assess the correctness of local explanations. This approach enhances evaluation reliability and scalability, integrating various recommenders, explanation algorithms, and datasets. Our goal is to provide a comprehensive mechanism combining model fidelity with explanation correctness, advancing transparency and trustworthiness in AI-driven recommender systems.</p>
    <p><strong>Categories:</strong> Explainability, Recommender Systems, Evaluation Metrics, Ethical AI, Counterfactual Analysis, Transparency, Trustworthy AI, Model Fidelity, Scalability (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1134/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Explainable Multi-Stakeholder Job Recommender Systems (2024)</h3>
    <p><strong>Authors:</strong> Roan Schellingerhout</p>
    <p>Public opinion on recommender systems has become increasingly wary in recent years. In line with this trend, lawmakers have also started to become more critical of such systems, resulting in the introduction of new laws focusing on aspects such as privacy, fairness, and explainability for recommender systems and AI at large. These concepts are especially crucial in high-risk domains such as recruitment. In recruitment specifically, decisions carry substantial weight, as the outcomes can significantly impact individuals’ careers and companies’ success. Additionally, there is a need for a multi-stakeholder approach, as these systems are used by job seekers, recruiters, and companies simultaneously, each with its own requirements and expectations. In this paper, I summarize my current research on the topic of explainable, multi-stakeholder job recommender systems and set out a number of future research directions.</p>
    <p><strong>Categories:</strong> Transparency, Legal Frameworks, Domain: Recruitment, Explainability, Multi-Stakeholder Systems, Job Recommendations, Fairness and Bias, Future Directions, Human Resources (HR) (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1133/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>A Tool for Explainable Pension Fund Recommendations using Large Language Models (2024)</h3>
    <p><strong>Authors:</strong> Edleno Silva de Moura, Eduardo Alves da Silva, Leandro Balby Marinho, Altigran Soares da Silva</p>
    <p>In this demo, we present a prototype tool designed to help financial advisors recommend private pension funds to investors based on their preferences, offering personalized investment suggestions. The tool leverages Large Language Models (LLMs), which enhance explainability by providing clear and understandable rationales for recommendations and effectively handles both sequential and cold-start scenarios. We outline the design, implementation, and results of a user-based evaluation using real-world data. The evaluation shows a high recommendation acceptance rate among financial advisors, highlighting the tool’s potential to improve decision-making in financial advisory services.</p>
    <p><strong>Categories:</strong> Explainability, Large Language Models, Financial Services, Wealth Management, Personalization, Cold Start, Pension Fund Recommendations, Decision-Making, User Evaluation, Real-World Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1205/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Integrating the ACT-R Framework with Collaborative Filtering for Explainable Sequential Music Recommendation (2023)</h3>
    <p><strong>Authors:</strong> Christian Wallmann, Markus Schedl, Elisabeth Lex, Dominik Kowald, Marta Moscati, Markus Reiter-Haas</p>
    <p>Music listening sessions often consist of sequences including repeating tracks. Modeling such relistening behavior with models of human memory has been proven effective in predicting the next track of a session. However, these models intrinsically lack the capability of recommending novel tracks that the target user has not listened to in the past. Collaborative filtering strategies, on the contrary, provide novel recommendations by leveraging past collective behaviors but are often limited in their ability to provide explanations. To narrow this gap, we propose four hybrid algorithms that integrate collaborative filtering with the cognitive architecture ACT-R. We compare their performance in terms of accuracy, novelty, diversity, and popularity bias, to baselines of different types, including pure ACT-R, kNN-based, and neural-networks-based approaches. We show that the proposed algorithms are able to achieve the best performances in terms of novelty and diversity, and simultaneously achieve a higher accuracy of recommendation with respect to pure ACT-R models. Furthermore, we illustrate how the proposed models can provide explainable recommendations.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Music, ACT-R Framework, Collaborative Filtering, Explainability, Accuracy, Novelty, Diversity, Popularity Bias, kNN-Based Algorithms, Neural Networks, Sequential Recommendations, Real-World Applications, Hybrid Methods, Cold Start (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/919/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Stability of Explainable Recommendation (2023)</h3>
    <p><strong>Authors:</strong> Sairamvinay Vijayaraghavan, Prasant Mohapatra</p>
    <p>Explainable Recommendation has been gaining attention over the last few years in industry and academia. Explanations provided along with recommendations for each user in a recommender system framework have many uses: particularly reasoning why a suggestion is provided and how well an item aligns with a user’s personalized preferences. Hence, explanations can play a huge role in influencing users to purchase products. However, the reliability of the explanations under varying scenarios has not been strictly verified in an empirical perspective. Unreliable explanations can bear strong consequences such as attackers leveraging explanations for manipulating and tempting users to purchase target items: that the attackers would want to promote. In this paper, we study the vulnerability of existent feature-oriented explainable recommenders, particularly analyzing their performance under different levels of external noises added into model parameters. We conducted experiments by analyzing three important state-of-the-art explainable recommenders when trained on two widely used e-commerce based recommendation datasets of different scales. We observe that all the explainable models are vulnerable to increased noise levels. Experimental results verify our hypothesis that the ability to explain recommendations does decrease along with increasing noise levels and particularly adversarial noise does contribute to a much stronger decrease. Our study presents an empirical verification on the topic of robust explanations in recommender systems which can be extended to different types of explainable recommenders in RS.</p>
    <p><strong>Categories:</strong> Explainability, Security, Adversarial Attacks, Robustness, Recommender Systems, Empirical Evaluation, E-commerce, Model Stability, Trustworthiness, Algorithmic Approaches, System Design, Noise Impact, Methodology (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/934/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Overcoming Recommendation Limitations with Neuro-Symbolic Integration (2023)</h3>
    <p><strong>Authors:</strong> Tommaso Carraro</p>
    <p>Despite being studied for over twenty years, Recommender Systems (RSs) still suffer from important issues that limit their applicability in real-world scenarios. Data sparsity, cold start, and explainability are some of the most impacting problems. Intuitively, these historical limitations can be mitigated by injecting prior knowledge into recommendation models. Neuro-Symbolic (NeSy) approaches are suitable candidates for achieving this goal. Specifically, they aim to integrate learning (e.g., neural networks) with symbolic reasoning (e.g., logical reasoning). Generally, the integration lets a neural model interact with a logical knowledge base, enabling reasoning capabilities. In particular, NeSy approaches have been shown to deal well with poor training data, and their symbolic component could enhance model transparency. This gives insights that NeSy systems could potentially mitigate the aforementioned RSs limitations. However, the application of such systems to RSs is still in its early stages, and most of the proposed architectures do not really exploit the advantages of a NeSy approach. To this end, we conducted preliminary experiments with a Logic Tensor Network (LTN), a novel NeSy framework. We used the LTN to train a vanilla Matrix Factorization model using a First-Order Logic knowledge base as an objective. In particular, we encoded facts to enable the regularization of the latent factors using content information, obtaining promising results. In this paper, we review existing NeSy recommenders, argue about their limitations, show our preliminary results with the LTN, and propose interesting future works in this novel research area. In particular, we show how the LTN can be intuitively used to regularize models, perform cross-domain recommendation, ensemble learning, and explainable recommendation, reduce popularity bias, and easily define the loss function of a model.</p>
    <p><strong>Categories:</strong> Neuro-Symbolic Integration, Recommender Systems, Data Sparsity, Cold Start Problem, Explainability, Neural Networks, Symbolic Reasoning, Logic Tensor Networks (LTN), Matrix Factorization, Content-Based Recommendations, Model Transparency, Regularization, Cross-Domain Recommendation, Ensemble Learning, Popularity Bias (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/986/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Demystifying Recommender Systems: A Multi-faceted Examination of Explanation Generation, Impact, and Perception (2023)</h3>
    <p><strong>Authors:</strong> Giacomo Balloccu</p>
    <p>Recommender systems have become an integral component of the digital landscape, impacting a multitude of services and industries ranging from e-commerce to entertainment and beyond. By offering personalised suggestions, these systems challenge a fundamental problem in our modern information society named information overload. As users face a deluge of choices, recommender systems help sift through this immense sea of possibilities, delivering a personalised subset of options that align with user preferences and historical behaviour. However, despite their considerable utility, recommender systems often operate as “black boxes,” obscuring the rationale behind recommendations. This opacity can engender mistrust and undermine user engagement, thus attenuating the overall effectiveness of the system. Researchers have emphasized the importance of explanations in recommender systems, highlighting how explanations can enhance system transparency, foster user trust, and improve decision-making processes, thereby enriching user experiences and yielding potential business benefits. Yet, a significant gap persists in the current state of human-understandable explanations research. While recommender systems have grown increasingly complex, our capacity to generate clear, concise, and relevant explanations that reflect this complexity remains limited. Crafting explanations that are both understandable and reflective of sophisticated algorithmic decision-making processes poses a significant challenge, especially in a manner that meets the user’s cognitive and contextual needs.</p>
    <p><strong>Categories:</strong> Explainability, User Trust, Information Overload, Transparency, Artificial Intelligence, Human-Computer Interaction, Personalization, Relevance, Algorithmic Transparency, Cross-Domain Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/975/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Knowledge-Aware Recommender Systems based on Multi-Modal Information Sources (2023)</h3>
    <p><strong>Authors:</strong> Giuseppe Spillo</p>
    <p>The last few years saw a growing interest in Knowledge-Aware Recommender Systems (KARSs), given their capability in encoding and exploiting several data sources, both structured (such as <i>knowledge graphs</i>) and unstructured (such as plain text); indeed, several pieces of research show the competitiveness of these models. Nowadays, a lot of models at the state-of-the-art in KARSs use deep learning, enabling them to exploit large amounts of information, including knowledge graphs (KGs), user reviews, plain text, and multimedia content (pictures, audio, videos). In my Ph.D. I will explore and study techniques for designing KARSs leveraging embeddings deriving from multi-modal information sources; the models I will design will aim at providing fair, accurate, and explainable recommendations.</p>
    <p><strong>Categories:</strong> Knowledge Graphs, Deep Learning, Recommender Systems, Structured Data, Unstructured Data, Multi-Modal Information Sources, Embeddings, Fairness, Explainability, Academic Research, Real-World Applications, Multi-Source Information (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/985/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Large Language Models are Competitive Near Cold-start Recommenders for Language- and Item-based Preferences (2023)</h3>
    <p><strong>Authors:</strong> Lucas Dixon, Krisztian Balog, Filip Radlinski, Scott Sanner, Ben Wedin</p>
    <p>Traditional recommender systems leverage users’ item preference history to recommend novel content that users may like.  However, dialog interfaces that allow users to express language-based preferences offer a fundamentally different modality for preference input.  Inspired by recent successes of prompting paradigms for large language models (LLMs), we study their use for making recommendations from both item-based and language-based preferences in comparison to state-of-the-art item-based collaborative filtering (CF) methods.  To support this investigation, we collect a new dataset consisting of both item-based and language-based preferences elicited from users along with their ratings on a variety of (biased) recommended items and (unbiased) random items.  Among numerous experimental results, we find that LLMs provide competitive recommendation performance for pure language-based preferences (no item preferences) in the near cold-start case in comparison to item-based CF methods, despite having no supervised training for this specific task (zero-shot) or only a few labels (few-shot).  This is particularly promising as language-based preference representations are more explainable and scrutable than item-based or vector-based representations.</p>
    <p><strong>Categories:</strong> Recommender Systems, Cold Start, Language-Based Preferences, Item-Based Preferences, Large Language Models, Zero-Shot Learning, Few-Shot Learning, Dataset Collection, Evaluation Methodology, Collaborative Filtering, Algorithm Comparisons, Explainability (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/922/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>ProtoMF: Prototype-based Matrix Factorization for Effective and Explainable Recommendations (2022)</h3>
    <p><strong>Authors:</strong> Christian Ganhör, Alessandro B. Melchiorre, Navid Rekabsaz, Markus Schedl</p>
    <p>Recent studies show the benefits of reformulating common machine learning models through the concept of prototypes – representatives of the underlying data, used to calculate the prediction score as a linear combination of similarities of a data point to prototypes. Such prototype-based formulation of a model, in addition to preserving (sometimes enhancing) the performance, enables explainability of the model’s decisions, as the prediction can be linearly broken down into the contributions of distinct definable prototypes. Following this direction, we extend the idea of prototypes to the recommender system domain by introducing ProtoMF, a novel collaborative filtering algorithm. ProtoMF learns sets of user/item prototypes that represent the general consumption characteristics of users/items in the underlying dataset. Using these prototypes, ProtoMF then represents users and items as vectors of similarities to the corresponding prototypes. These user/item representations are ultimately leveraged to make recommendations that are both effective in terms of accuracy metrics, and explainable through the interpretation of prototypes’ contributions to the affinity scores. We conduct experiments on three datasets to assess both the effectiveness and the explainability of ProtoMF. Addressing the former, we show that ProtoMF exhibits higher Hit Ratio and NDCG compared to other relevant collaborative filtering approaches. As for the latter, we qualitatively show how ProtoMF can provide explainable recommendations and how its explanation capabilities can expose the existence of statistical biases in the learned representations, which we exemplify for the case of gender bias.</p>
    <p><strong>Categories:</strong> Matrix Factorization, Explainability, Collaborative Filtering, Recommendation Systems, Evaluation Metrics, Fairness, Explainable Recommendations, Prototype-based Models, Collaborative Filtering Techniques, Transparency (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/773/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Scalable Linear Shallow Autoencoder for Collaborative Filtering (2022)</h3>
    <p><strong>Authors:</strong> Vojtěch Vančura, Petr Kasalicky, Pavel Kordík, Rodrigo Alves</p>
    <p>Recently, the RS research community has witnessed a surge in popularity for shallow autoencoder-based CF methods. Due to its straightforward implementation and high accuracy on item retrieval metrics, EASE is potentially the most prominent of these models. Despite its accuracy and simplicity, EASE cannot be employed in some real-world recommender system applications due to its inability to scale to huge interaction matrices. In this paper, we proposed ELSA, a scalable shallow autoencoder method for implicit feedback recommenders. ELSA is a scalable autoencoder in which the hidden layer is factorizable into a low-rank plus sparse structure, thereby drastically lowering memory consumption and computation time. We conducted a comprehensive offline experimental section that combined synthetic and several real-world datasets. We also validated our strategy in an online setting by comparing ELSA to baselines in a live recommender system using an A/B test. Experiments demonstrate that ELSA is scalable and has competitive performance. Finally, we demonstrate the explainability of ELSA by illustrating the recovered latent space.</p>
    <p><strong>Categories:</strong> Scalable Linear Shallow Autoencoder, Collaborative Filtering, Recommender Systems, Matrix Factorization, Implicit Feedback, Autoencoder, Offline Evaluation, Online Evaluation, Scalability, Explainability (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/796/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Generic Automated Lead Ranking in Dynamics CRM (2021)</h3>
    <p><strong>Authors:</strong> Royi Ronen, Edan Hauon, Gopal Kasturi, AJ Ezzour, Oron Nir, Hilik Berezin, Rotem Preizler, Sayalee Bhanavase</p>
    <p>We developed a generic framework which enables Customer Relationship Management (CRM) organizations to deploy an automated ranking system for leads (commonly known as ‘lead scoring’). Leads are records that represent non-customers who might become customers. Lead ranking is a fundamental CRM problem with many flavors. Ranking serves as a prioritization management tool for CRM organizations, with many characteristics similar to those of recommender systems.<br>We present the system with its most recent developments, emphasizing challenges that go beyond the core of the learning algorithm, and that have played an instrumental role in maturing the system into a trustable feature, robust to different types of organizations and datasets. Particularly, we present features which enable Human in the Loop [1], a dominant concept in both configuration and result consumption. Another type of features demonstrates the addition of domain knowledge into the machine learning based process.<br>We present the concepts of feature selection, with and without human help, prediction explanations, insights on model inputs, data quality issues, training for UX consistency, and actionability for each individual prediction.</p>
    <p><strong>Categories:</strong> CRM, Lead Scoring, Recommender Systems, Machine Learning, Human-in-the-Loop, Domain Knowledge Integration, Feature Selection, Explainability, Generalizability, Data Quality, User Experience, Actionable Insights (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/702/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>EX3: Explainable Attribute-aware Item-set Recommendations (2021)</h3>
    <p><strong>Authors:</strong> Yikun Xian, Jun Ma, Jim Chan, Tong Zhao, George Karypis, Andrey Kan, Xin Luna Dong, Jin Li, Yongfeng Zhang, S. Muthukrishnan, Christos Faloutsos</p>
    <p>Existing recommender systems in the e-commerce domain primarily focus on generating a set of relevant items as recommendations; however, few existing systems utilize underlying item attributes as a key organizing principle in presenting recommendations to users. Mining important attributes of items from customer perspectives and presenting them along with item sets as recommendations can provide users more explainability and help them make better purchase decision. In this work, we generalize the attribute-aware item-set recommendation problem, and develop a new approach to generate sets of items (recommendations) with corresponding important attributes (explanations) that can best justify why the items are recommended to users. In particular, we propose a system that learns important attributes from historical user behavior to derive item set recommendations, so that an organized view of recommendations and their attribute-driven explanations can help users more easily understand how the recommendations relate to their preferences. Our approach is geared towards real world scenarios: we expect a solution to be scalable to billions of items, and be able to learn item and attribute relevance automatically from user behavior without human annotations. To this end, we propose a multi-step learning-based framework called Extract-Expect-Explain (EX3), which is able to adaptively select recommended items and important attributes for users. We experiment on a large-scale real-world benchmark and the results show that our model outperforms state-of-the-art baselines by an 11.35% increase on NDCG with adaptive explainability for item set recommendation.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Explainability, Attribute-aware Recommendations, E-commerce, Scalability, Learning without Annotations, User Behavior Analysis, Evaluation Metrics, Real-world Applications, Multi-step Frameworks (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/633/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Fast Multi-Step Critiquing for VAE-based Recommender Systems (2021)</h3>
    <p><strong>Authors:</strong> Boi Faltings, Diego Antognini</p>
    <p>Recent studies have shown that providing personalized explanations alongside recommendations increases trust and perceived quality. Furthermore, it gives users an opportunity to refine the recommendations by critiquing parts of the explanations. On one hand, current recommender systems model the recommendation, explanation, and critiquing objectives jointly, but this creates an inherent trade-off between their respective performance. On the other hand, although recent latent linear critiquing approaches are built upon an existing recommender system, they suffer from computational inefficiency at inference due to the objective optimized at each conversation’s turn. We address these deficiencies with M&Ms-VAE, a novel variational autoencoder for recommendation and explanation that is based on multimodal modeling assumptions. We train the model under a weak supervision scheme to simulate both fully and partially observed variables. Then, we leverage the generalization ability of a trained M&Ms-VAE model to embed the user preference and the critique separately. Our work’s most important innovation is our critiquing module, which is built upon and trained in a self-supervised manner with a simple ranking objective. Experiments on four real-world datasets demonstrate that among state-of-the-art models, our system is the first to dominate or match the performance in terms of recommendation, explanation, and multi-step critiquing. Moreover, M&Ms-VAE processes the critiques up to 25.6x faster than the best baselines. Finally, we show that our model infers coherent joint and cross generation, even under weak supervision, thanks to our multimodal-based modeling and training scheme.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Variational Autoencoder (VAE), Explainability, User Interaction, Multi-Modal Modeling, Weak Supervision, Self-Supervised Learning, Scalability, Efficiency, Inference Methods, AI/ML Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/638/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>