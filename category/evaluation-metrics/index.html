<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">Evaluation Metrics</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/cold-start/">Cold Start</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/scalability/">Scalability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/beyond-accuracy/">Beyond Accuracy</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/offline-evaluation/">Offline Evaluation</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/recommendation-quality/">Recommendation Quality</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Promoting Two-sided Fairness with Adaptive Weights for Providers and Customers in Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Wayne Xin Zhao, Ji-Rong Wen, Lanling Xu, Sheng Chen, Zihan Lin, Jinpeng Wang</p>
    <p>At present, most recommender systems involve two stakeholders, providers and customers. Apart from maximizing the recommendation accuracy, the fairness issue for both sides should also be considered. However, there is a trade-off for multi-objective optimization problems, where optimizing one objective (e.g. provider-side fairness) may degrade the performance of others (e.g. accuracy). Most of previous studies try to improve two-sided fairness with post-processing algorithms or fairness-aware loss constraints, which are highly dependent on the heuristic adjustments without respect to the optimization goal of accuracy. In contrast, we propose a novel training framework, adaptive weighting  towards two-sided fairness-aware recommendation (named Ada2Fair), which lies in the extension of the accuracy-focused objective to a controllable preference learning loss over the interaction data. Specifically, we adjust the optimization scale of an interaction sample with an adaptive weight generator, and estimate the two-sided fairness-aware weights within model training. During the training process, the recommender is trained with two-sided fairness-aware weights to boost the utility of niche providers and inactive customers in a unified way. Extensive experiments on three public datasets verify the effectiveness of Ada2Fair, which can achieve Pareto improvements in two-sided fairness-aware recommendation. Our code implementation is available at https://anonymous.4open.science/r/Ada2Fair.</p>
    <p><strong>Categories:</strong> Adaptive Weights, Recommender Systems, Fairness, Two-sided Fairness, Multi-objective Optimization, Recommendation Frameworks, Provider-Customer Dynamics, User-Centric Design, Real-world Applications, Adaptive Learning, Evaluation Metrics (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1112/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Co-optimize Content Generation and Consumption in a Large Scale Video Recommendation System (2024)</h3>
    <p><strong>Authors:</strong> Yuening Li, Mingyan Gao, Qingyun Liu, Sourabh Bansod, Shuchao Bi, Liang Liu, Yaping Zhang, Zhen Zhang</p>
    <p>Multi-task prediction models and value models are the de-facto standard ranking components in modern large-scale content recommendation systems. However, they are typically optimized to model users’ passive consumption behaviors, and rank content in a way to grow only consumption-centric values. In this talk, we discuss the key insight that it is possible to model sparse participatory content-generation actions as well and grow ecosystem value through a new ranking system. We made the following key technical contributions in this system: (1) introducing ranking for content generation based on a categorization of user participation actions of different sparsity, including proxy intent action or access point clicks. (2) improving sparse task prediction quality and stability by causal task relationship modeling, conditional loss modeling and ResNet based shared bottom network. (3) personalizing the value model to minimize conflicts between different values, through e.g. ranking inspiring content higher for users who actively generate content. (4) conducting systematic evaluation of proposed approach in a large short-form video UGC (User-Generated Content) platform.</p>
    <p><strong>Categories:</strong> Multi-Task Learning, Video Recommendation, User-Generated Content, Content Generation, Large-Scale Systems, Evaluation Metrics, Causal Modeling, Network Architecture, User Participation, Scalability, Content Creation, User-Centric Design (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1156/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Leveraging Monte Carlo Tree Search for Group Recommendation (2024)</h3>
    <p><strong>Authors:</strong> J. Andres Diaz-Pace, Antonela Tommasel</p>
    <p>Group recommenders aim to provide recommendations that satisfy the collective preferences of multiple users, a challenging task due to the diverse individual tastes and conflicting interests to be balanced. This is often accomplished by using aggregation techniques that select items on which the group can agree. Traditional aggregators struggle with these complexities, as items are chosen independently, leading to sub-optimal recommendations lacking diversity, novelty, or fairness. In this paper, we propose an aggregation technique that leverages Monte Carlo Tree Search (MCTS) to enhance group recommendations. MCTS is used to explore and evaluate candidate recommendation sequences to optimize overall group satisfaction. We also investigate the integration of MCTS with LLMs aiming at better understanding interactions between user preferences and recommendation sequences to inform the search. Experimental evaluations, although preliminary, showed that our proposal outperforms existing aggregation techniques in terms of relevance and beyond-accuracy aspects of recommendations. The LLM integration achieved positive results for recommendations’ relevance. Overall, this work highlights the potential of heuristic search techniques to tackle the complexities of group recommendations.</p>
    <p><strong>Categories:</strong> Group Recommenders, Monte Carlo Tree Search, Heuristic Search, General Recommendation, User Satisfaction, Group Recommendations, Beyond Accuracy, Diversity of Recommendations, LLM Integration, Evaluation Metrics, Relevance, Novel Approaches (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1200/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Informed Dataset Selection with ‘Algorithm Performance Spaces’ (2024)</h3>
    <p><strong>Authors:</strong> Lien Michiels, Lukas Wegmeth, Joeran Beel, Steffen Schulz</p>
    <p>When designing recommender-systems experiments, a key question that has been largely overlooked is the choice of datasets. In a brief survey of ACM RecSys papers, we found that authors typically justified their dataset choices by labelling them as public, benchmark, or ‘real-world’ without further explanation. We propose the Algorithm Performance Space (APS) as a novel method for informed dataset selection. The APS is an n-dimensional space where each dimension represents the performance of a different algorithm. Each dataset is depicted as an n-dimensional vector, with greater distances indicating higher diversity. In our experiment, we ran 29 algorithms on 95 datasets to construct an actual APS. Our findings show that many datasets, including most Amazon datasets, are clustered closely in the APS, i.e. they are not diverse. However, other datasets, such as MovieLens and Docear, are more dispersed. The APS also enables the grouping of datasets based on the solvability of the underlying problem. Datasets in the top right corner of the APS are considered ’solved problems’ because all algorithms perform well on them. Conversely, datasets in the bottom left corner lack well-performing algorithms, making them ideal candidates for new recommender-system research due to the challenges they present.</p>
    <p><strong>Categories:</strong> Algorithm Performance, Dataset Selection, Recommender Systems, Evaluation Metrics, Experimental Design, Real-World Applications, Research Methods (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1194/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Bayesian Optimization with LLM-Based Acquisition Functions for Natural Language Preference Elicitation (2024)</h3>
    <p><strong>Authors:</strong> Armin Toroghi, David Austin, Anton Korikov, Scott Sanner</p>
    <p>Designing preference elicitation (PE) methodologies that can quickly ascertain a user’s top item preferences in a cold-start setting is a key challenge for building effective and personalized conversational recommendation (ConvRec) systems. While large language models (LLMs) constitute a novel technology that enables fully natural language (NL) PE dialogues, we hypothesize that monolithic LLM NL-PE approaches lack the multi-turn, decision-theoretic reasoning required to effectively balance the NL exploration and exploitation of user preferences towards an arbitrary item set. In contrast, traditional Bayesian optimization PE methods define theoretically optimal PE strategies, but fail to use NL item descriptions or generate NL queries, unrealistically assuming users can express preferences with direct item ratings and comparisons. To overcome the limitations of both approaches, we formulate NL-PE in a Bayesian Optimization (BO) framework that seeks to actively elicit natural language feedback to reduce uncertainty over item utilities to identify the best recommendation. Key challenges in generalizing BO to deal with natural language feedback include determining (a) how to leverage LLMs to model the likelihood of NL preference feedback as a function of item utilities and (b) how to design an acquisition function that works for language-based BO that can elicit in the infinite space of language. We demonstrate our framework in a novel NL-PE algorithm, PEBOL, which uses Natural Language Inference (NLI) between user preference utterances and NL item descriptions to maintain preference beliefs and BO strategies such as Thompson Sampling (TS) and Upper Confidence Bound (UCB) to guide LLM query generation. We numerically evaluate our methods in controlled experiments, finding that PEBOL achieves up to 131% improvement in MAP@10 after 10 turns of cold start NL-PE dialogue compared to monolithic GPT-3.5, despite relying on a much smaller 400M parameter NLI model for preference inference.</p>
    <p><strong>Categories:</strong> Bayesian Optimization, Large Language Models (LLMs), Natural Language Processing (NLP), Conversational Recommendation Systems, Preference Elicitation, Cold Start Problem, Algorithm Selection, Multi-Armed Bandits, Thompson Sampling, Upper Confidence Bound, Natural Language Inference (NLI), Evaluation Metrics, Machine Learning (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1020/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Adaptive Fusion of Multi-View for Graph Contrastive Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Jie Zhou, Jianwei Yin, Meng Xi, Ying Li, Mengduo Yang, Xiaohua Pan, Yi Yuan, Yangyang Wu, Jinshan Zhang</p>
    <p>Recommendation is a key mechanism for modern users to access items of their interests from massive entities and information. Recently, graph contrastive learning (GCL) has demonstrated satisfactory results on recommendation, due to its ability to enhance representation by integrating graph neural networks (GNNs) with contrastive learning. However, those methods often generate contrastive views by performing random perturbation on edges or embeddings, which is likely to bring noise in representation learning. Besides, in all these methods, the degree of user preference on items is omitted during the representation learning process, which may cause incomplete user/item modeling. To address these limitations, we propose the Adaptive Fusion of Multi-View Graph Contrastive Recommendation (AMGCR) model. Specifically, to generate the informative and less noisy views for better contrastive learning, we design four view generators to learn the edge weights focusing on weight adjustment, feature transformation, neighbor aggregation, and attention mechanism, respectively. Then, we employ an adaptive multi-view fusion module to combine different views from both the view-shared and the view-specific levels. Moreover, to make the model capable of capturing preference information during the learning process, we further adopt a preference refinement strategy on the fused contrastive view. Experimental results on three real-world datasets demonstrate that AMGCR consistently outperforms the state-of-the-art methods, with average improvements of over 10% in terms of Recall and NDCG.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Graph Neural Networks (GNNs), Contrastive Learning, Multi-View Learning, Adaptive Methods, User Preferences, Evaluation Metrics (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1025/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Prompt Tuning for Item Cold-start Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Jingchi Wang, Gaode Chen, Qi Zhang, Jingjian Lin, Yuezihan Jiang, Kaigui Bian, Wenhan Zhang, Peng Jiang, Yinjie Jiang</p>
    <p>The item cold-start problem is crucial for online recommender systems, as the success of the cold-start phase determines whether items can transition into popular ones. Prompt learning, a powerful technique used in natural language processing (NLP) to address zero- or few-shot problems, has been adapted for recommender systems to tackle similar challenges. However, existing methods typically rely on content-based properties or text descriptions for prompting, which we argue may be suboptimal for cold-start recommendations due to 1) semantic gaps with recommender tasks, 2) model bias caused by warm-up items contribute most of positive feedback to the model, which is the core of the cold-start problem that hinder the recommender quality on cold-start items. We propose to leverage high-value positive feedback, termed pinnacle feedback as prompt information, to simultaneously resolve the above two problems. We experimentally prove that comparing to content description proposed in existing works, the positive feedback is more suitable to serve as prompt information by bridging the semantic gaps. Besides, we propose item-wise personalized prompt networks to encode pinnaclce feedback to relieve the model bias by the positive feedback dominance problem. Extensive experiments on four real-world datasets demonstrate the superiority of our model over state-of-the-art methods. Moreover, PROMO has been successfully deployed on a popular short-video sharing platform, a billion-user scale commercial short-video application, achieving remarkable performance gains across various commercial metrics within cold-start scenarios.</p>
    <p><strong>Categories:</strong> Cold Start, Prompt Learning, Natural Language Processing (NLP), Recommender Systems, Semantic Gaps, Model Bias, High-Value Feedback, Personalized Prompts, Real-World Applications, A/B Testing, Evaluation Metrics, Feedback-Based Methods (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1055/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Biased User History Synthesis for Personalized Long-Tail Item Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Elan Markowitz, Greg Ver Steeg, Abdulla Alshabanah, Keshav Balasubramanian, Murali Annavaram</p>
    <p>Recommendation systems connect users to items and create value chains in the internet economy. Recommendation systems learn from past user-item interaction histories. As such, items that have short interaction histories, either because they are new or not popular, are disproportionately under-recommended. This long-tail item problem can exacerbate model bias, and reinforce poor recommendation of tail items. In this paper, we propose a novel training algorithm, <i>biased user history synthesis</i>, to not only address this problem but also achieve better personalization in recommendation systems. As a result, we concurrently improve tail and head item recommendation performance. Our approach is built on a tail item biased User Interaction History (UIH) sampling strategy and a synthesis model that produces an augmented user representation from the sampled user history. We provide a theoretical justification for our approach using information theory and demonstrate through extensive experimentation, that our model outperforms state-of-the-art baselines on tail, head, and overall recommendation. the source code is available at https://github.com/lkp411/BiasedUserHistorySynthesis.</p>
    <p><strong>Categories:</strong> Algorithm, Personalization, Long-Tail Item Recommendation, Cold Start Problem, User Interaction History, Data Augmentation, Recommendation Systems, Evaluation Metrics, Theoretical Justification, Scalability and Performance (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1026/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>The Fault in Our Recommendations: On the Perils of Optimizing the Measurable (2024)</h3>
    <p><strong>Authors:</strong> Akshit Kumar, Yash Kanoria, Omar Besbes</p>
    <p>Recommendation systems play a pivotal role on digital platforms by curating content, products and services for users. These systems are widespread, and through customized recommendations, promise to match users with options they will like. To that end,  data on engagement is collected and used, with, e.g., measurements of clicks, but also  purchases or consumption times.   Most recommendation systems are ranking-based, where they rank and recommend items based on their predicted engagement.  However, the engagement signals  are often only a crude proxy for user utility, as data on the latter is rarely collected or available. This paper explores the following critical research question: By optimizing for measurable proxies, are recommendation systems at risk of significantly under-delivering on user utility? If that is indeed the case, how can one improve utility which is seldom measured? To study these questions, we introduce a model of repeated user consumption in which, at each interaction, users select between an outside option and the best option from a recommendation set. Our model accounts for user heterogeneity, with the majority preferring “popular” content, and a minority favoring “niche” content. The system initially lacks knowledge of individual user preferences but can learn these preferences through observations of users’ choices over time. Our theoretical and numerical analysis demonstrate that optimizing for engagement signals can lead to significant utility losses. Instead, we propose a utility-aware policy that initially recommends a mix of popular and niche content. We show that such a policy substantially improves  utility despite not measuring it. In fact, in the limit of a forward-looking platform with discount factor $\delta \to 1$, our utility-aware policy achieves the best of both worlds: near-optimal user utility and near-optimal engagement simultaneously. Our study elucidates  an important feature of recommendation systems; given the ability to suggest multiple items, one can perform significant exploration without incurring significant reductions in short term engagement. By recommending high-risk, high-reward items alongside popular items, systems can enhance discovery of high utility items without significantly affecting engagement.</p>
    <p><strong>Categories:</strong> Optimization, Evaluation Metrics, Recommendation Systems, User Behavior, Engagement, Machine Learning, Reinforcement Learning, Bandit Algorithms, Ranking Algorithms, Digital Platforms, Personalization, Cold Start, Exploration vs Exploitation, Long-term User Satisfaction, Scalability, Theoretical Modeling, Numerical Analysis, User Heterogeneity, Ethical Considerations in AI, Beyond Accuracy, High-Risk High-Reward Recommendations (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1062/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Multimodal Representation Learning for high-quality Recommendations in Cold-start and Beyond-Accuracy (2024)</h3>
    <p><strong>Authors:</strong> Marta Moscati</p>
    <p>Recommender systems (RS) traditionally leverage the large amount of user–item interaction data. This exposes RS to a lower recommendation quality in cold-start scenarios, as well as to a low recommendation quality in terms of beyond-accuracy evaluation metrics. State-of-the-art (SotA) models for cold-start scenarios rely on the use of side information on the items or the users, therefore relating recommendation to multimodal machine learning (ML). However, the most recent techniques from multimodal ML are often not applied to the domain of recommendation. Additionally, the evaluation of SotA multimodal RS often neglects beyond-accuracy aspects of recommendation. In this work, we outline research into designing novel multimodal RS based on SotA multimodal ML architectures for cold-start recommendation, and their evaluation and benchmark with preexisting multimodal RS in terms of accuracy and beyond-accuracy aspects of recommendation quality.</p>
    <p><strong>Categories:</strong> Multimodal Representation Learning, Cold Start, Beyond Accuracy, Recommender Systems, Machine Learning, Evaluation Metrics, Scalability, Architecture Design, Benchmarking, Side Information (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1144/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Information-Controllable Graph Contrastive Learning for Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Zixuan Yang, Zirui Guo, Yanhua Yu, Liang Pang, Tat-Seng Chua, Kangkang Lu, Yuling Wang</p>
    <p>In the evolving landscape of recommender systems, Graph Contrastive Learning (GCL) has become a prominent method for enhancing recommendation performance by alleviating the issue of data sparsity. However, existing GCL-based recommendations often overlook the control of shared information between the contrastive views. In this paper, we initially analyze and experimentally demonstrate these methods often lead to the issue of augmented representation collapse, where the representations between views become excessively similar, diminishing their distinctiveness. To address this issue, we propose the Information-Controllable Graph Contrastive Learning (IGCL) framework, a novel approach that focuses on optimizing the shared information between views to include as much relevant information for the recommendation task as possible while maintaining an appropriate level. In particular, we design the Collaborative Signals Enhanced Augmentation module to infuse the augmented representation with rich, task-relevant collaborative signals. Furthermore, the Information-Controllable Contrastive Learning module is designed to direct control over the magnitude of shared information between the contrastive views to avoid over-similarity. Extensive experiments on three public datasets demonstrate the effectiveness of IGCL, showcasing significant improvements in performance and the capability to alleviate augmented representation collapse.</p>
    <p><strong>Categories:</strong> Graph Contrastive Learning, Recommendation Systems, Collaborative Filtering, Machine Learning, Representation Learning, Feature Engineering, Information Theory, Evaluation Metrics, Collaborative Signals, Information Flow Control, Data Sparsity, Empirical Study (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1047/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Sliding Window Training – Utilizing Historical Recommender Systems Data for Foundation Models (2024)</h3>
    <p><strong>Authors:</strong> Ko-Jen Hsiao, Yesu Feng, Sudarshan Lamkhede, Swanand Joshi, Zhe Zhang</p>
    <p>Long-lived recommender systems (RecSys) often encounter lengthy user-item interaction histories that span many years. To effectively learn long term user preferences, Large RecSys foundation models (FM) need to encode this information in pretraining. Usually, this is done by either generating a long enough sequence length to take all history sequences as input at the cost of large model input dimension or by dropping some parts of the user history to accommodate model size and latency requirements on the production serving side. In this paper, we introduce a sliding window training technique to incorporate long user history sequences during training time without increasing the model input dimension. We show the quantitative \& qualitative improvements this technique brings to the RecSys FM in learning user long term preferences. We additionally show that the average quality of items in the catalog learnt in pretraining also improves.</p>
    <p><strong>Categories:</strong> Recommender Systems (RecSys), Foundation Models, Training Techniques, Sliding Window, Historical Data Utilization, Model Optimization, User Preferences Learning, Catalog Quality, Pretraining Methods, Scalability, Time Series Analysis, Model Efficiency, Evaluation Metrics, Real-World Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1176/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>The MovieLens Beliefs Dataset: Collecting Pre-Choice Data for Online Recommender Systems (2024)</h3>
    <p><strong>Authors:</strong> Guy Aridor, Duarte Goncalves, Daniel Kluver, Ruoyan Kong, Joseph Konstan</p>
    <p>An increasingly important aspect of designing recommender systems involves considering how recommendations will influence consumer choices. This paper addresses this issue by introducing a method for collecting user beliefs about un-experienced items – a critical predictor of choice behavior. We implemented this method on the MovieLens platform, resulting in a rich dataset that combines user ratings, beliefs, and observed recommendations. We document challenges to such data collection, including selection bias in response and limited coverage of the product space. This unique resource empowers researchers to delve deeper into user behavior and analyze user choices absent recommendations, measure the effectiveness of recommendations, and prototype algorithms that leverage user belief data, ultimately leading to more impactful recommender systems.</p>
    <p><strong>Categories:</strong> User Behavior, Pre-Choice Data, Recommender Systems, MovieLens, Dataset, User Feedback, Recommendation Algorithms, Research Methods, Movies, Data Collection Challenges, Recommender Systems Design, Evaluation Metrics, Algorithm Development, Belief Modeling, User Choices, Cold Start (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1106/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Do Recommender Systems Promote Local Music? A Reproducibility Study Using Music Streaming Data (2024)</h3>
    <p><strong>Authors:</strong> Manuel Moussallam, Kristina Matrosova, Thomas Louail, Lilian Marey, Guillaume Salha-Galvan, Olivier Bodini</p>
    <p>This paper examines the influence of recommender systems on local music representation, discussing prior findings from an empirical study on the LFM-2b public dataset. This prior study argued that different recommender systems exhibit algorithmic biases shifting music consumption either towards or against local content. However, LFM-2b users do not reflect the diverse audience of music streaming services. To assess the robustness of this study’s conclusions, we conduct a comparative analysis using proprietary listening data from a global music streaming service, which we publicly release alongside this paper. We observe significant differences in local music consumption patterns between our dataset and LFM-2b, suggesting that caution should be exercised when drawing conclusions on local music based solely on LFM-2b. Moreover, we show that the algorithmic biases exhibited in the original work vary in our dataset, and that several unexplored model parameters can significantly influence these biases and affect the study’s conclusion on both datasets. Finally, we discuss the complexity of accurately labeling local music, emphasizing the risk of misleading conclusions due to unreliable, biased, or incomplete labels.</p>
    <p><strong>Categories:</strong> Recommendation Systems Analysis, Music Streaming, Reproducibility Analysis, Algorithmic Bias, Dataset Comparison, Content Labeling, Evaluation Metrics, Real-World Applications, Industry Data, Model Parameters (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1121/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>FedLoCA: Low-Rank Coordinated Adaptation with Knowledge Decoupling for Federated Recommendations (2024)</h3>
    <p><strong>Authors:</strong> Yong Liao, Boyu Fan, Pengyuan Zhou, Siqing Zhang, Yuchen Ding, Wei Sun</p>
    <p>Privacy protection in recommendation systems is gaining increasing attention, for which federated learning has emerged as a promising solution. Current federated recommendation systems grapple with high communication overhead due to sharing dense global embeddings, and also poorly reflect user preferences due to data heterogeneity. To overcome these challenges, we propose a two-stage Federated Low-rank Coordinated Adaptation (FedLoCA) framework to decouple global and client-specific knowledge into low-rank embeddings, which significantly reduces communication overhead while enhancing the system’s ability to capture individual user preferences amidst data heterogeneity. Further, to tackle gradient estimation inaccuracies stemming from data sparsity in federated recommendation systems, we introduce an adversarial gradient projected descent approach in low-rank spaces, which significantly boosts model performance while maintaining robustness. Remarkably, FedLoCA also alleviates performance loss even under the stringent constraints of differential privacy. Extensive experiments on various real-world datasets demonstrate that FedLoCA significantly outperforms existing methods in both recommendation accuracy and communication efficiency.</p>
    <p><strong>Categories:</strong> Federated Learning, Privacy Protection, Recommendation Systems, Communication Efficiency, Low-Rank Embeddings, Personalization, Gradient Estimation, Optimization, Differential Privacy, Data Heterogeneity, Cold Start, Evaluation Metrics (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1037/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Revisiting LightGCN: Unexpected Inflexibility, Inconsistency, and A Remedy Towards Improved Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Geon Lee, Kijung Shin, Kyungho Kim</p>
    <p>Graph Neural Networks (GNNs) have emerged as effective tools in recommender systems. Among various GNN models, LightGCN is distinguished by its simplicity and outstanding performance. Its efficiency has led to widespread adoption across different domains, including social, bundle, and multimedia recommendations. In this paper, we thoroughly examine the mechanisms of LightGCN, focusing on its strategies for scaling embeddings, aggregating neighbors, and pooling embeddings across layers. Our analysis reveals that, contrary to expectations based on its design, LightGCN suffers from inflexibility and inconsistency when applied to real-world data. We introduce LightGCN++, an enhanced version of LightGCN designed to address the identified limitations. LightGCN++ incorporates flexible scaling of embedding norms and neighbor weighting, along with a tailored approach for pooling layer-wise embeddings to resolve the identified inconsistencies. Despite its remarkably simple remedy, extensive experimental results demonstrate that LightGCN++ significantly outperforms LightGCN, achieving an improvement of up to 17.81% in terms of NDCG@20. Furthermore, state-of-the-art models utilizing LightGCN as a backbone for item, bundle, multimedia, and knowledge-graph-based recommendations exhibit improved performance when equipped with LightGCN++.</p>
    <p><strong>Categories:</strong> Graph Neural Networks (GNNs), Recommendation Systems, LightGCN, Algorithm Design, Evaluation Metrics, Embedding Techniques, Social Recommendations, Bundle Recommendations, Multimedia Recommendations, Model Limitations, Algorithm Optimization, Recommendation Quality (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1110/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>