<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reinforcement-learning/">Reinforcement Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/evaluation-methods/">Evaluation Methods</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/cold-start/">Cold Start</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/evaluation-metrics/">Evaluation Metrics</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Bias in Book Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Savvina Daniil</p>
    <p>Recommender systems are prevalent in many applications, but hide risks; issues like bias propagation have been on the focus of related studies in recent years. My own research revolves around tracking bias in the book recommendation domain. Specifically, I am interested in whether the incorporation of recommender systems in a library’s loaning system serves their social responsibility and purpose, with bias being the main point of concern. To this end, I engage with the topic in three ways; by mapping the area of ethics in book recommendation, by investigating and reflecting on challenges with studying bias in recommender systems in general, and by showcasing a set of social implication of statistical bias in the book recommendation domain in particular. In this doctoral symposium paper, I further elaborate on the problem at hand, the outline of my thesis, the progress I have made so far, as well as my plans for future work along with specific questions that have arisen from my research efforts.</p>
    <p><strong>Categories:</strong> Book Recommendation, Recommender Systems, Ethics, Bias, Statistical Bias, Social Implications, Library Systems, Social Responsibility, Algorithmic Fairness (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1127/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Understanding Fairness in Recommender Systems: A Healthcare Perspective (2024)</h3>
    <p><strong>Authors:</strong> Veronica Kecki, Alan Said</p>
    <p>Fairness in AI-driven decision-making systems has become a critical concern, especially when these systems directly affect human lives. This paper explores the public’s comprehension of fairness in healthcare recommendations. We conducted a survey where participants selected from four fairness metrics – Demographic Parity, Equal Accuracy, Equalized Odds, and Positive Predictive Value – across different healthcare scenarios to assess their understanding of these concepts. Our findings reveal that fairness is a complex and often misunderstood concept, with a generally low level of public understanding regarding fairness metrics in recommender systems. This study highlights the need for enhanced information and education on algorithmic fairness to support informed decision-making in using these systems. Furthermore, the results suggest that a one-size-fits-all approach to fairness may be insufficient, pointing to the importance of context-sensitive designs in developing equitable AI systems.</p>
    <p><strong>Categories:</strong> Fairness, Recommender Systems, Healthcare, Ethics in AI, Public Understanding, Algorithmic Fairness, Education, Context-aware Systems, Healthcare Applications, User Survey (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1201/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Social Choice for Heterogeneous Fairness in Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Amy Voida, Martin Homola, Nicholas Mattei, Amanda Aird, Elena Štefancová, Robin Burke, Cassidy All</p>
    <p>Algorithmic fairness in recommender systems requires close attention to the needs of a diverse set of stakeholders that may have competing interests. Previous work in this area has often been limited by fixed, single-objective definitions of fairness, built into algorithms or optimization criteria that are applied to a single fairness dimension or, at most, applied identically across dimensions. These narrow conceptualizations limit the ability to adapt fairness-aware solutions to the wide range of stakeholder needs and fairness definitions that arise in practice. Our work approaches recommendation fairness from the standpoint of computational social choice, using a multi-agent framework. In this paper, we explore the properties of different social choice mechanisms and demonstrate the successful integration of multiple, heterogeneous fairness definitions across multiple data sets.</p>
    <p><strong>Categories:</strong> Algorithmic Fairness, Recommender Systems, Social Choice Theory, Multi-Agent Frameworks, Stakeholder Needs, Diverse Fairness Definitions, Multi-Criteria Decision Making, Computational Methods (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1197/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Group Fairness for Content Creators: the Role of Human and Algorithmic Biases under Popularity-based Recommendations (2023)</h3>
    <p><strong>Authors:</strong> Nicolo Pagan, Stefania Ionescu, Aniko Hannak</p>
    <p>The Creator Economy faces concerning levels of unfairness. Content creators (CCs) publicly accuse platforms of purposefully reducing the visibility of their content based on protected attributes, while platforms place the blame on viewer biases. Meanwhile, prior work warns about the “rich-get-richer”  effect perpetuated by existing popularity biases in recommender systems: Any initial advantage in visibility will likely be exacerbated over time. What remains unclear is how the biases based on protected attributes from platforms and viewers interact and contribute to the observed inequality in the context of popularity-biased recommender systems. The difficulty of the question lies in the complexity and opacity of the system. To overcome this challenge, we create a simple agent-based model (ABM) that unifies the platform systems which allocate the visibility of CCs (e.g., recommender systems, moderation) into a single popularity-based function, which we call the visibility allocation system (VAS). Through simulations, we find that although viewer homophilic biases do alone create inequalities, small levels of additional biases in VAS are more harmful. From the perspective of interventions, our results suggest that (a) attempts to reduce attribute-biases in moderation and recommendations should precede those reducing viewer homophilic tendencies, (b) decreasing the popularity-biases in VAS decreases but not eliminates inequalities, (c) boosting the visibility of protected CCs to overcome viewer homophily with respect to one metric is unlikely to produce fair outcomes with respect to all metrics, and (d) the process is also unfair for viewers and this unfairness could be overcome through the same interventions. More generally, this work demonstrates the potential of using ABMs to better understand the causes and effects of biases and interventions within complex sociotechnical systems.</p>
    <p><strong>Categories:</strong> Fairness, Bias, Recommendation Systems, Algorithmic Fairness, Creator Economy, Agent-Based Modeling (ABM), Sociotechnical Systems, Interventions in Recommendation Systems, Human Biases, Algorithmic Biases, Complexity of Systems, Group Fairness (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/915/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Providing Previously Unseen Users Fair Recommendations Using Variational Autoencoders (2023)</h3>
    <p><strong>Authors:</strong> Bjørnar Vassøy, Helge Langseth, Benjamin Kille</p>
    <p>An emerging definition of fairness in machine learning requires that models are oblivious to demographic user information, e.g., a user’s gender or age should not influence the model. Personalized recommender systems are particularly prone to violating this definition through their explicit user focus and user modelling. Explicit user modelling is also an aspect that makes many recommender systems incapable of providing hitherto unseen users with recommendations. We propose novel approaches for mitigating discrimination in Variational Autoencoder-based recommender systems by limiting the encoding of demographic information. The approaches are capable of, and evaluated on, providing entirely new users with fair recommendations.</p>
    <p><strong>Categories:</strong> Recommender Systems, Variational Autoencoders, Fairness in Machine Learning, Algorithmic Fairness, Discrimination Mitigation, Evaluation of Recommender Systems, Cold Start, Scalability, Demographic Attributes (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/926/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Adversary or Friend? An adversarial Approach to Improving Recommender Systems (2022)</h3>
    <p><strong>Authors:</strong> Pannaga Shivaswamy, Dario Garcia Garcia</p>
    <p>Typical recommender systems models are trained to have good average performance across all users or items. In practice, this results in model performance that is good for some users but sub-optimal for many users. In this work, we consider adversarially trained machine learning models and extend them to recommender systems problems. The adversarial models are trained with no additional demographic or other information than already available to the learning algorithm. We show that adversarially reweighted learning models give more emphasis to dense areas of the feature-space that incur high loss during training. We show that a straightforward adversarial model adapted to recommender systems can fail to perform well and that a carefully designed adversarial model can perform much better. The proposed models are trained using a standard gradient descent/ascent approach that can be easily adapted to many recommender problems. We compare our results with an inverse propensity weighting based baseline that also works well in practice. We delve deep into the underlying experimental results and show that, for the users who are under-served by the baseline model, the adversarial models can achieve significantly better results.</p>
    <p><strong>Categories:</strong> Adversarial Training, Recommender Systems, Machine Learning, Recommendation Algorithms, User-Specific Recommendations, Improving Recommendation Accuracy, Fairness in AI, Bias Mitigation, Beyond Accuracy, Algorithmic Fairness, Optimization Techniques, Fairness in Recommendations, Bias in AI, Algorithmic Bias (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/748/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Fair Ranking Metrics (2022)</h3>
    <p><strong>Authors:</strong> Amifa Raj</p>
    <p>Information access systems such as search engines and recommender systems often display results in a sorted ranked list based on their relevance. Fairness of these ranked list has received attention as an important evaluation criteria along with traditional metrics such as utility or accuracy. Fairness broadly involves both provider and consumer side fairness at both group and individual levels. Several fair ranking metrics have been proposed to measure group fairness for providers based on various “sensitive attributes”. These metrics differ in their fairness goal, assumptions, and implementations. Although there are several fair ranking metrics to measure group fairness, multiple open challenges still exist in this area to consider.<br>In my thesis, I work on the area of fair ranking metrics for provider-side group fairness. I am interested in understanding the fairness concepts and practical applications of these metrics to identify their strength and limitations to aid the researchers and practitioner by pointing out the gaps. Moreover, I will contribute to this research area by focusing on some of the limitations like considering different browsing models and bias in relevance information.</p>
    <p><strong>Categories:</strong> Algorithmic Fairness, Recommendation Systems, Group Fairness, Evaluation Criteria, Provider-Side Fairness, Sensitive Attributes, Research Methodology, Browsing Models, Bias in Relevance Information, Theory and Practice, Fairness Metrics (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/809/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Matching Theory-based Recommendation in Online Dating (2022)</h3>
    <p><strong>Authors:</strong> Riku Togashi, Yoji Tomita, Daisuke Moriwaki</p>
    <p>Online dating platforms provide people with the opportunity to find a partner. Recommender systems in online dating platforms suggest one side of users to the other side of users. This reciprocal recommendation problem arises in online dating [26 , 31 ], job recommendation [2] and peer learning process [27]. Reciprocal recommender systems (RRSs) must take into account two aspects which do not arise in standard recommender systems. One aspect is the mutual interests of users. Even if one user has a strong interest in a particular user, she/he may have no interest in him/her at all. Thus, RRSs should be based on the interests of both sides of users. The other important aspect is capacities of the users; it is impossible for a user to keep up with all the candidates. Popular users are often recommended very frequently. As a result, a few super stars receive a large proportion of likes, overwhelming the time that they can spend for screening. RRSs should be designed to alleviate such problems to increase user satisfaction. The two-sided matching problem is to “match” one side of people and the other side in a situation where each person has a different preference and a matching capacity. This field is called matching theory, and various algorithms have been developed since [ 11 , 29 ]. Matching theory is applied in the analyses of marriage markets [4, 8], school choice [1] and job matching for physicians [28]. In this talk, we discuss the potential interactions between RRSs and matching theory. We also present our ongoing project to deploy a matching theory-based recommender system (MTRS) in a real-world online dating platform. This talk covers other important directions regarding RRSs, including scalability, algorithmic fairness, bandit algorithm, and online experimentation.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Online Dating, Matching Theory, Reciprocal Recommendations, Mutual Interests, User Capacity Management, Real-World Applications, A/B Testing, User Survey, Scalability, Algorithmic Fairness, Bandit Algorithm, Two-Sided Markets, Multi-Armed Bandits, Deployment, User Satisfaction (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/835/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Learning to Match Job Candidates Using Multilingual Bi-Encoder BERT (2021)</h3>
    <p><strong>Authors:</strong> Dor Lavi</p>
    <p>Randstad is the global leader in the HR services industry. We support people and organizations in realizing their true potential by combining the power of today’s technology with our passion for people. In 2020, we helped more than two million candidates find a meaningful job with our 236,100 clients. Randstad is active in 38 markets around the world and has top-three positions in almost half of these. In 2020, Randstad had on average 34,680 corporate employees and generated revenue of € 20.7 billion. Each day, at Randstad, we employ industry-scale recommender systems to recommend thousands of candidates to our clients, and the other way around; vacancies to job seekers. Our “Talent Recommender” recommender system is based on a heterogeneous collection of input data: CVs, vacancy texts (job descriptions) and structured data (e.g., the location of a candidate or vacancy). The goal of the system is to recommend the best candidates (talents) to each open vacancy. CVs are user-generated PDF files. It goes without saying that parsing those files to plain text can be a challenge in itself and therefore out of scope for this talk. On the other hand, vacancies are usually structured formatted text. We should be aware that due to the difference in structure and preprocessing steps, that the input to the subsequent steps is inevitably noisy. Most NLP research in text similarity is based on the assumption that 2 pieces of information are the same but written differently [1]. Like two artists that paint the same landscape, but each with its own style. However, in our case the 2 documents complement one another like pieces in a puzzle, together they create the bigger picture, rather than 2 similar paintings. Some of our biggest challenges with the “Talent Recommender” stem from dealing with the diverse nature of our textual sources of data: vacancies and CVs. While both capture similar information, they are inherently different in many ways. First, the information in CVs and vacancies are similar, but there exists a vocabulary gap, where grammar and context differ. For example, where “I have 10 years of experience as an instructor” in a CV shares no word overlap with “We are looking for a talented tutor” in a vacancy, both cases express similar information regarding “experience in the field of education,” we need to overcome the synonyms gap “instructor” and “tutor.” In addition, the sentence structure is completely different, CVs are typically written in “storytelling mode” “I have. . . ,” while the vacancy is in “exploration mode” “we are looking. . . ” The second challenge is multilinguality. Since we are a multinational company that operates all across the globe, developing a model per language is not scalable in our case. We ultimately would like one maintainable model that supports as many languages as possible. Our last challenge is cross language similarity [ 4]. In some of the countries we operate, there is a high percentage of job seekers that are not native to that country. For example, many of the job descriptions in the Netherlands are in Dutch, however around 10% of the CVs are in English. Classic text models, like TF-IDF and Word2vec, capture information within one language, but hardly connect between languages. Simply put, even if trained on multiple languages each language will have its own cluster in space. So “logistics” in English and “logistiek” in Dutch are embedded in a completely different point in space, even though the meaning is the same. Furthermore, we know that the language of CV correlates with nationality and therefore can be a proxy discriminator. Due to the impact of these systems and the risks of unintended algorithmic bias and discrimination, HR is marked as a high risk domain in the recently published EC Artificial Intelligence Act [2]. To avoid discriminating against nationality we would like to recommend a candidate to the vacancy no matter which language the CV is written in. That is of course only if language is not a requirement for that vacancy. In this talk, we will show how we used our internal history of candidate placements to generate labeled CV-vacancy pairs dataset. Afterwards we fine-tune a multilingual BERT with bi encoder structure [3] over this dataset, by adding a cosine similarity log loss layer. We will explain how using the mentioned structure helps us overcome most of the challenges described above, and how it enables us to build a maintainable and scalable pipeline to match CVs and vacancies. In addition, we show how we gain a better semantic understanding, and learn to bridge the vocabulary gap. Finally, we highlight how multilingual transformers help us handle cross language barrier and might reduce discrimination.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Natural Language Processing (NLP), Machine Learning, Multilingual Models, Text Similarity, HR/Recruitment, Algorithmic Fairness, Bias Mitigation, Transformers/BERT, Information Retrieval, Cross-Language Learning, Real-World Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/733/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Personalized Fairness-aware Re-ranking for Microlending (2019)</h3>
    <p><strong>Authors:</strong> Robin Burke, Shengyu Zhang, Weiwen Liu, Nasim Sonboli, Jun Guo</p>
    <p>Microlending can lead to improved access to capital in impoverished countries. Recommender systems could be used in microlending to provide efficient and personalized service to lenders. However, increasing concerns about discrimination in machine learning hinder the application of recommender systems to the microfinance industry. Most previous recommender systems focus on pure personalization, with fairness issue largely ignored. A desirable fairness property in microlending is to give borrowers from different demographic groups a fair chance of being recommended, as stated by Kiva. To achieve this goal, we propose a Fairness-Aware Re-ranking (FAR) algorithm to balance ranking quality and borrower-side fairness. Furthermore, we take into consideration that lenders may differ in their receptivity to the diversification of recommended loans, and develop a Personalized Fairness-Aware Re-ranking (PFAR) algorithm. Experiments on a real-world dataset from Kiva.org show that our re-ranking algorithm can significantly promote fairness with little sacrifice in accuracy, and be attentive to individual lender preference on loan diversity. i>Presentation: Tuesday Poster Lunch</i</p>
    <p><strong>Categories:</strong> Fairness in Recommendations, Re-ranking, Microlending, Finance/Microfinance, Personalized Fairness-Aware Re-Ranking (PFAR), Social Impact, Bias Mitigation, Diversity of Recommendations, Beyond Accuracy, Evaluation Metrics, Real-World Applications, Algorithmic Fairness, Recommender Systems (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/491/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>