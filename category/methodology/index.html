<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">Methodology</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/explainability/">Explainability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/user-behavior/">User Behavior</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Data Augmentation using Reverse Prompt for Cost-Efficient Cold-Start Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Genki Kusano</p>
    <p>Recommendation systems that use auxiliary information such as product names and categories have been proposed to address the cold-start problem. However, these methods do not perform well when we only have insufficient warm-start training data. On the other hand, large language models (LLMs) can perform as effective cold-start recommendation systems even with limited warm-start data. However, they require numerous API calls for inferences, which leads to high operational costs in terms of time and money. This is a significant concern in industrial applications. In this paper, we introduce a new method, RevAug, which leverages LLMs as a data augmentation to enhance cost-efficient cold-start recommendation systems. To generate pseudo-samples, we have reversed the commonly used prompt for an LLM from “Would this user like this item?” to “What kind of items would this user like?”. Generated outputs by this reverse prompt are pseudo-auxiliary information utilized to enhance recommendation systems in the training phase. In numerical experiments with four real-world datasets, RevAug demonstrated superior performance in cold-start settings with limited warm-start data compared to existing methods. Moreover, RevAug significantly reduced API fees and processing time compared to an LLM-based recommendation method.</p>
    <p><strong>Categories:</strong> Cold Start, Data Augmentation, Large Language Models (LLMs), Recommendation Systems, Reverse Prompt, Cost Efficiency, Methodology, Performance in Cold-Start Settings, Real-World Applications. (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1081/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Stability of Explainable Recommendation (2023)</h3>
    <p><strong>Authors:</strong> Sairamvinay Vijayaraghavan, Prasant Mohapatra</p>
    <p>Explainable Recommendation has been gaining attention over the last few years in industry and academia. Explanations provided along with recommendations for each user in a recommender system framework have many uses: particularly reasoning why a suggestion is provided and how well an item aligns with a user’s personalized preferences. Hence, explanations can play a huge role in influencing users to purchase products. However, the reliability of the explanations under varying scenarios has not been strictly verified in an empirical perspective. Unreliable explanations can bear strong consequences such as attackers leveraging explanations for manipulating and tempting users to purchase target items: that the attackers would want to promote. In this paper, we study the vulnerability of existent feature-oriented explainable recommenders, particularly analyzing their performance under different levels of external noises added into model parameters. We conducted experiments by analyzing three important state-of-the-art explainable recommenders when trained on two widely used e-commerce based recommendation datasets of different scales. We observe that all the explainable models are vulnerable to increased noise levels. Experimental results verify our hypothesis that the ability to explain recommendations does decrease along with increasing noise levels and particularly adversarial noise does contribute to a much stronger decrease. Our study presents an empirical verification on the topic of robust explanations in recommender systems which can be extended to different types of explainable recommenders in RS.</p>
    <p><strong>Categories:</strong> Explainability, Security, Adversarial Attacks, Robustness, Recommender Systems, Empirical Evaluation, E-commerce, Model Stability, Trustworthiness, Algorithmic Approaches, System Design, Noise Impact, Methodology (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/934/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Challenges for Anonymous Session-Based Recommender Systems in Indoor Environments (2023)</h3>
    <p><strong>Authors:</strong> Alessio Ferrato</p>
    <p>Recommender Systems (RSs) have gained widespread popularity for providing personalized recommendations in manifold domains. However, considering the growing user privacy concerns, the development of recommender systems that prioritize data protection has become increasingly important. In indoor environments, RSs face unique challenges, and ongoing research is being conducted to address them. Anonymous Session-Based Recommender Systems (ASBRSs) can represent a possible solution to address these challenges while ensuring user privacy. This paper aims to bridge the gap between existing RS research and the demand for privacy-preserving recommender systems, especially in indoor settings, where significant research efforts are underway. Therefore, it proposes three research questions: How does user modeling based on implicit feedback impact on ASBRSs, considering different embedding extraction networks? How can short sessions be leveraged to start the recommendation process in ASBRSs? To what extent can ASBRSs generate fair recommendations? By investigating these questions, this study establishes the foundations for applying ASBRSs in indoor environments, safeguarding user privacy, and contributing to the ongoing research in this field.</p>
    <p><strong>Categories:</strong> Recommender Systems, Privacy Preservation, Session-Based Recommenders, Indoor Environments, Implicit Feedback, User Modeling, Short Sessions Handling, Real-World Applications, Data Protection, Methodology, Fairness in Recommendations, Review/Foundational Research (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/979/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Taxonomic Recommendations of Real Estate Properties with Textual Attribute Information (2022)</h3>
    <p><strong>Authors:</strong> Zachary Harrison, Anish Khazane</p>
    <p>In this extended abstract, we present an end to end approach for building a taxonomy of home attribute terms that enables hierarchical recommendations of real estate properties. We cover the methodology for building a real-estate taxonomy, metrics for measuring this structure’s quality, and then conclude with a production use-case of making recommendations from search keywords at different levels of topical similarity.</p>
    <p><strong>Categories:</strong> Taxonomy, Hierarchical Recommendations, Real Estate, Methodology, Evaluation Metrics, Production Use-Case (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/842/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Eigenvalue Perturbation for Item-based Recommender Systems (2021)</h3>
    <p><strong>Authors:</strong> Cesare Bernardis, Paolo Cremonesi</p>
    <p>Adding confidence estimates to predicted ratings has been shown to positively influence the quality of the recommendations provided by a recommender system. While confidence over single point predictions of ratings and preferences has been widely studied in literature, limited effort has been put in exploring the benefits provided by user-level confidence indices. In this work we exploit a recently introduced user-level confidence index, called eigenvalue confidence index, in order to provide maximum confidence recommendations for item-based recommender systems. We firstly derive a closed form solution to calculate the index, then we propose a new recommendation methodology for item-based models, called eigenvalue perturbation, founded on the strongly positive correlation between the index value and the accuracy of the recommendations. We show and discuss the accuracy results obtained with a comprehensive set of experiments over several datasets and using different item-based models, empirically proving that applying the new technique we are able to outperform the original recommendation models in most of the experimental configurations.</p>
    <p><strong>Categories:</strong> Item-based Recommender Systems, Eigenvalue Perturbation, User-level Confidence Indices, Recommendation Accuracy, Closed-form Solution, Methodology, Experimental Evaluation, Machine Learning, Algorithmic Techniques, User Modeling (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/691/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Using Conceptual Incongruity as a Basis for Making Recommendations (2020)</h3>
    <p><strong>Authors:</strong> Nisheeth Srivastava, Tushar Shandhilya</p>
    <p>We evaluate the possibility of using within-item measures of meta-data similarity to improve recommendation rankings along psychologically salient dimensions of incongruity and creativity. Our approach contrasts with recently developed methods at introducing diversity into recommendations which rely on across-item measurements of dissimilarity, while sharing several formal and algorithmic elements. We show that semantic distance based operationalizations of psychological constructs show substantial correlation with empirical data. We further show that incongruity predicts variability in satisfaction as measured by movie ratings in a large corpus. Empirical results from a two month-long user study demonstrate that incongruity-based recommendations attract considerably more interaction from users, and users expressed significantly greater satisfaction given these recommendations. Based on these observations, we propose that using incongruity to diversify recommendations may be useful in expanding recommendation repertoires along interesting psychological dimensions, complementing relevance-based search.</p>
    <p><strong>Categories:</strong> Incongruity, Creativity, Psychology, Recommendation Systems, Diversity, Methodology, Evaluation Metrics, Real-World Applications, User Interaction, Recommendation Techniques (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/585/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Are We Evaluating Rigorously? Benchmarking Recommendation for Reproducible Evaluation and Fair Comparison (2020)</h3>
    <p><strong>Authors:</strong> Zhu Sun, Jie Zhang, Di Yu, Xinghua Qu, Hui Fang, Jie Yang, Cong Geng</p>
    <p>With tremendous amount of recommendation algorithms proposed every year, one critical issue has attracted a considerable amount of attention: there are no effective benchmarks for evaluation, which leads to two major concerns, i.e., unreproducible evaluation and unfair comparison. This paper aims to conduct rigorous (i.e., reproducible and fair) evaluation for implicit-feedback based top-N recommendation algorithms. We first systematically review 85 recommendation papers published at eight top-tier conferences (e.g., RecSys, SIGIR) to summarize important evaluation factors, e.g., data splitting and parameter tuning strategies, etc. Through a holistic empirical study, the impacts of different factors on recommendation performance are then analyzed in-depth. Following that, we create benchmarks with standardized procedures and provide the performance of seven well-tuned state-of-the-arts across six metrics on six widely-used datasets as a reference for later study. Additionally, we release a user-friendly Python toolkit, which differs from existing ones in addressing the broad scope of rigorous evaluation for recommendation. Overall, our work sheds light on the issues in recommendation evaluation and lays the foundation for further investigation. Our code and datasets are available at GitHub (https://github.com/AmazingDD/daisyRec).</p>
    <p><strong>Categories:</strong> Recommender Systems, Methodology, Reproducibility in Science, Benchmarking, Evaluation Protocols, Implicit Feedback, Fairness in AI, Performance Metrics, Reproducible Research, Open Source Tools, Literature Review, RecSys, SIGIR, Research Methodology (<i>deepseek-r1:70b</i>)</p>
    <p><a href="/recsys-recommender/response/584/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Combining context features in sequence-aware recommender systems (2019)</h3>
    <p><strong>Authors:</strong> Pavel Levin, Sarai Mizrachi</p>
    <p>There are several important design choices that machine learning practitioners need to make when incorporating predictors into RNN-based contextual recommender systems. A great deal of currently reported findings about these decisions focus on the setting where predicted items take on values from the space of sequence items. This work provides an empirical evaluation of some straightforward approaches of dealing with such problems on a real-world large scale prediction problem from the travel domain, where predicted entities do not live in the space of sequence items.</p>
    <p><strong>Categories:</strong> Recommender Systems, Context Features, Neural Networks, Travel Domain, Real-World Applications, Methodology, Design Choices, Specific Challenges in Recommendation Systems, Empirical Evaluation, Sequence-Aware Recommenders, Recurrent Neural Networks (RNNs), Large-Scale Prediction. (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/512/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Predicting Online Performance of Job Recommender Systems With Offline Evaluation (2019)</h3>
    <p><strong>Authors:</strong> Adrien Mogenet, Tuan Anh Nguyen Pham, Masahiro Kazama, Jialin Kong</p>
    <p>Recommender systems can be used to recommend jobs. In this context, implicit and explicit feedback signals we can collect are rare events, making the task of evaluation more complex. Online evaluation (A-B testing) is usually the most reliable way to measure the results from our experiments, but it is a slow process. In contrast, the offline evaluation process is faster, but it is critical to make it reliable as it informs our decision to roll out new improvements in production. In this paper, we review the comparative offline and online performances of three recommendations models, we describe the evaluation metrics we use and analyze how the offline performance metrics correlate with online metrics to understand how an offline evaluation process can be leveraged to inform the decisions. i>Presentation: Wednesday Poster Lunch</i</p>
    <p><strong>Categories:</strong> Job Recommendations, Recommendation Models, Offline Evaluation, Online Evaluation, Evaluation Metrics, Model Comparison, Performance Prediction, Decision-Making, Methodology (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/487/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Explore, Exploit, and Explain: Personalizing Explainable Recommendations with Bandits (2018)</h3>
    <p><strong>Authors:</strong> Rishabh Mehrotra, Karl Higley, Benjamin Lacker, James McInerney, Hugues Bouchard, Alois Gruson, Samantha Hansen</p>
    <p>The multi-armed bandit is an important framework for balancing exploration with exploitation in recommendation. Exploitation recommends content (e.g., products, movies, music playlists) with the highest predicted user engagement and has traditionally been the focus of recommender systems. Exploration recommends content with uncertain predicted user engagement for the purpose of gathering more information. The importance of exploration has been recognized in recent years, particularly in settings with new users, new items, non-stationary preferences and attributes. In parallel, explaining recommendations (“recsplanations”) is crucial if users are to understand their recommendations. Existing work has looked at bandits and explanations independently. We provide the first method that combines both in a principled manner. In particular, our method is able to jointly (1) learn which explanations each user responds to; (2) learn the best content to recommend for each user; and (3) balance exploration with exploitation to deal with uncertainty. Experiments with historical log data and tests with live production traffic in a large-scale music recommendation service show a significant improvement in user engagement.</p>
    <p><strong>Categories:</strong> Multi-Armed Bandits, Personalization, Explainability, Recommendation Systems, Exploration vs Exploitation, Methodology, User Modeling, Music, Real-World Applications, A/B Testing, Recsplations, User Engagement (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/345/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Eigenvalue Analogy for Confidence Estimation in Item-based Recommender Systems (2018)</h3>
    <p><strong>Authors:</strong> Paolo Cremonesi, Maurizio Ferrari Dacrema</p>
    <p>Item-item collaborative filtering (CF) models are a well known and studied family of recommender systems, however current literature does not provide any theoretical explanation of the conditions under which item-based recommendations will succeed or fail. We investigate the existence of an ideal item-based CF method able to make perfect recommendations. This CF model is formalized as an eigenvalue problem, where estimated ratings are equivalent to the true (unknown) ratings multiplied by a user-specific eigenvalue of the similarity matrix. Preliminary experiments show that the magnitude of the eigenvalue is proportional to the accuracy of recommendations for that user and therefore it can provide reliable measure of confidence.</p>
    <p><strong>Categories:</strong> Eigenvalue Methods, Item-based Recommendation, Collaborative Filtering, Matrix Factorization, Confidence Estimation, Theoretical Analysis, Evaluation Metrics, Recommendation Quality, Methodology (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/418/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Understanding Latent Factors Using a GWAP (2018)</h3>
    <p><strong>Authors:</strong> Jürgen Ziegler, Johannes Kunkel, Benedikt Loepp</p>
    <p>Recommender systems relying on latent factor models often appear as black boxes to their users. Semantic descriptions for the factors might help to mitigate this problem. Achieving this automatically is, however, a non-straightforward task due to the models’ statistical nature. We present an output-agreement game that represents factors by means of sample items and motivates players to create such descriptions. A user study shows that the collected output actually reflects real-world characteristics of the factors.</p>
    <p><strong>Categories:</strong> Algorithm Family (Latent Factor Models), User Study, Games with a Purpose (GWAP), Real-World Applications, Transparency/Interpretability in Recommendations, Explainability, Human-Readable Descriptions, Recommendation Systems, Social Aspects, Crowdsourcing, Methodology (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/420/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Memory Priming and User Preferences (2016)</h3>
    <p><strong>Authors:</strong> Efthimios Bothos, Babis Magoutas, Evagelia Anagnostopoulou, Gregoris Mentzas</p>
    <p>In this paper we provide a preliminary analysis of the effects of priming on user preferences and we describe two experiments that show such effects in test environments. Our first results show that small stimuli which primes the process of item rating leads to varying average ratings.</p>
    <p><strong>Categories:</strong> Psychology/Cognitive Science, Experimental Design and Analysis, User Behavior, Recommendation Systems, Evaluation Methods, User Preferences, Memory Priming, Rating Systems, Methodology, Human Factors (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/246/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Nudging Grocery Shoppers to Make Healthier Choices (2015)</h3>
    <p><strong>Authors:</strong> Elizabeth Wayman, Sriganesh Madhvanath</p>
    <p>Despite the rampant increase in obesity rates and concomitant increases in rates of mortality from heart disease, cancer and diabetes, getting the general public to adopt a healthy diet has proven to be challenging for a variety of reasons. In this paper, we describe Foodle, a research project aimed at providing automated, personalized and goal-driven dietary guidance to users based on their grocery receipt data, by leveraging the availability of digital receipts for grocery store purchases. We discuss challenges faced, the current state of the project, and directions for future work.</p>
    <p><strong>Categories:</strong> Behavior Change, Personalization, Recommendation Systems, Data Sources (Grocery Receipts), Methodology, Grocery Stores, Health Promotion, User Behavior Analysis, Well-being (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/129/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Evaluating Recommender Behavior For New Users (2014)</h3>
    <p><strong>Authors:</strong> Daniel Kluver, Joseph Konstan</p>
    <p>The new user experience is one of the important problems in recommender systems. Past work on recommending for new users has focused on the process of gathering information from the user. Our work focuses on how different algorithms behave for new users. We describe a methodology that we use to compare representatives of three common families of algorithms along eleven different metrics. We find that for the first few ratings a baseline algorithm performs better than three common collaborative filtering algorithms. Once we have a few ratings, we find that Funk’s SVD algorithm has the best overall performance. We also find that ItemItem, a very commonly deployed algorithm, performs very poorly for new users. Our results can inform the design of interfaces and algorithms for new users.</p>
    <p><strong>Categories:</strong> Cold Start, Algorithm Behavior, Recommender Systems, Collaborative Filtering, Matrix Factorization, New Users, Methodology, Performance Metrics, Onboarding Experience, Interface Design (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/12/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>