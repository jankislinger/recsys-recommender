<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">Large Language Models</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/explainability/">Explainability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/real-world-applications/">Real-world Applications</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reinforcement-learning/">Reinforcement Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/cold-start/">Cold Start</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>A Tool for Explainable Pension Fund Recommendations using Large Language Models (2024)</h3>
    <p><strong>Authors:</strong> Edleno Silva de Moura, Eduardo Alves da Silva, Leandro Balby Marinho, Altigran Soares da Silva</p>
    <p>In this demo, we present a prototype tool designed to help financial advisors recommend private pension funds to investors based on their preferences, offering personalized investment suggestions. The tool leverages Large Language Models (LLMs), which enhance explainability by providing clear and understandable rationales for recommendations and effectively handles both sequential and cold-start scenarios. We outline the design, implementation, and results of a user-based evaluation using real-world data. The evaluation shows a high recommendation acceptance rate among financial advisors, highlighting the tool’s potential to improve decision-making in financial advisory services.</p>
    <p><strong>Categories:</strong> Explainability, Large Language Models, Financial Services, Wealth Management, Personalization, Cold Start, Pension Fund Recommendations, Decision-Making, User Evaluation, Real-World Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1205/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Scalable Cross-Entropy Loss for Sequential Recommendations with Large Item Catalogs (2024)</h3>
    <p><strong>Authors:</strong> Gleb Mezentsev, Danil Gusak, Ivan Oseledets, Evgeny Frolov</p>
    <p>Scalability issue plays a crucial role in productionizing modern recommender systems. Even lightweight architectures may suffer from high computational overload due to intermediate calculations, limiting their practicality in real-world applications. Specifically, applying full Cross-Entropy (CE) loss often yields state-of-the-art performance in terms of recommendations quality. Still, it suffers from excessive GPU memory utilization when dealing with large item catalogs. This paper introduces a novel Scalable Cross-Entropy (SCE) loss function in the sequential learning setup. It approximates the CE loss for datasets with large-size catalogs, enhancing both time efficiency and memory usage without compromising recommendations quality. Unlike traditional negative sampling methods, our approach utilizes a selective GPU-efficient computation strategy, focusing on the most informative elements of the catalog, particularly those most likely to be false positives. This is achieved by approximating the softmax distribution over a subset of the model outputs through the maximum inner product search. Experimental results on multiple datasets demonstrate the effectiveness of SCE in reducing peak memory usage by a factor of up to 100 compared to the alternatives, retaining or even exceeding their metrics values. The proposed approach also opens new perspectives for large-scale developments in different domains, such as large language models.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Loss Functions, Scalability, Sequential Recommendations, GPU Memory Optimization, Large Item Catalogs, Efficiency and Performance, False Positives Handling, Max Inner Product Search (MIPS), Large Language Models (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1059/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>AI-based Human-Centered Recommender Systems: Empirical Experiments and Research Infrastructure (2024)</h3>
    <p><strong>Authors:</strong> Ruixuan Sun</p>
    <p>This is a dissertation plan built around human-centered empirical experiments evaluating recommender systems (RecSys). We see this as an important research theme since many AI-based RecSys algorithmic studies lack real human assessment. Therefore, we do not know how they work in the wild that only human experiments can tell us. We split this extended abstract into two parts — 1) A series of individual studies focusing on open questions about different human values or recommendation algorithms. Our completed works include user control over content diversity, user appreciation on DL-RecSys algorithms, and human-LLMRec interaction study. We also propose three future works to understand news recommendation depolarization, personalized news podcast, and interactive user representation; 2) An experimentation infrastructure named POPROX. As a personalized news recommendation platform, it aims to support the longitudinal study needs from the general AI and RecSys research community.</p>
    <p><strong>Categories:</strong> AI-based Recommender Systems, Human-Centered Design, Empirical Experiments, Deep Learning Algorithms, Large Language Models, News Recommendations, Content Diversity, Personalization, Longitudinal Studies, Media, Research Infrastructure, User Interaction (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1129/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>SeCor: Aligning Semantic and Collaborative representations by Large Language Models for Next-Point-of-Interest Recommendations (2024)</h3>
    <p><strong>Authors:</strong> Ling Ding, Yang Xiang, Jianting Chen, Shirui Wang, Xiaoying Gao, Bohan Xie</p>
    <p>The widespread adoption of location-based applications has created a growing demand for point-of-interest (POI) recommendation, which aims to predict a user’s next POI based on their historical check-in data and current location. However, existing methods often struggle to capture the intricate relationships within check-in data. This is largely due to their limitations in representing temporal and spatial information and underutilizing rich semantic features. While large language models (LLMs) offer powerful semantic comprehension to solve them, they are limited by hallucination and the inability to incorporate global collaborative information. To address these issues, we propose a novel method SeCor, which treats POI recommendation as a multi-modal task and integrates semantic and collaborative representations to form an efficient hybrid encoding. SeCor first employs a basic collaborative filtering model to mine interaction features. These embeddings, as one modal information, are fed into LLM to align with semantic representation, leading to efficient hybrid embeddings. To mitigate the hallucination, SeCor recommends based on the hybrid embeddings rather than directly using the LLM’s output text. Extensive experiments on three public real-world datasets show that SeCor outperforms all baselines, achieving improved recommendation performance by effectively integrating collaborative and semantic information through LLMs.</p>
    <p><strong>Categories:</strong> Large Language Models, Hybrid Methods, Point-of-Interest Recommendation, Location-Based Services, Multi-Modal, Real World Application, Evaluation in Recommendations, Recommender Systems, Semantics, Collaborative Filtering (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1066/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>TLRec: A Transfer Learning Framework to Enhance Large Language Models for Sequential Recommendation Tasks (2024)</h3>
    <p><strong>Authors:</strong> Shuang Peng, Jiaye Lin, Zhong Zhang, Peilin Zhao</p>
    <p>Recently, Large Language Models (LLMs) have garnered significant attention in recommendation systems, improving recommendation performance through in-context learning or parameter-efficient fine-tuning. However, cross-domain generalization, i.e., model training in one scenario (source domain) but inference in another (target domain), is underexplored. In this paper, we present TLRec, a transfer learning framework aimed at enhancing LLMs for sequential recommendation tasks. TLRec specifically focuses on text inputs to mitigate the challenge of limited transferability across diverse domains, offering promising advantages over traditional recommendation models that heavily depend on unique identities (IDs) like user IDs and item IDs. Moreover, we leverage the source domain data to further enhance LLMs’ performance in the target domain. Initially, we employ powerful closed-source LLMs (e.g., GPT-4) and chain-of-thought techniques to construct instruction tuning data from the third-party scenario (source domain). Subsequently, we apply curriculum learning to fine-tune LLMs for effective knowledge injection and perform recommendations in the target domain. Experimental results demonstrate that TLRec achieves superior performance under the zero-shot and few-shot settings.</p>
    <p><strong>Categories:</strong> Transfer Learning, Large Language Models, Recommendation Systems, Cross-Domain Recommendations, Instruction Tuning, Curriculum Learning, Fine-Tuning, Domain Adaptation, Zero-Shot Learning, Few-Shot Learning, Text-Based Recommendations, Sequential Recommendations, Chain of Thought (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1203/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Analyzing User Preferences and Quality Improvement on Bing’s WebPage Recommendation Experience with Large Language Models (2024)</h3>
    <p><strong>Authors:</strong> Jialin Liu, Gang Luo, Hongzhi Li, Chuck Wang, Fan Wu, Amey Barapatre, Jaidev Shah</p>
    <p>Explore Further @ Bing (Web Recommendations) is a web-scale query independent webpage-to-webpage recommendation system with an index size of over 200 billion webpages. Due to the significant variability in webpage quality across the web and the reliance of our system on learning soleley user behavior (clicks), our production system was susceptible to serving clickbait and low-quality recommendations. Our team invested several months in developing and shipping several improvements that utilize LLM-generated recommendation quality labels to enhance our ranking stack to improve the nature of the recommendations we show to our users. Another key motivation behind our efforts was to go beyond merely surfacing relevant webpages, focusing instead on prioritizing more useful and authoritative content that delivers value to users based on their implied intent. We demonstrate how large language models (LLMs) offer a powerful tool for product teams to gain deeper insights into shifts in product experience and user behavior following significant improvements or changes to a production system. In this work, to enable our analysis, we also showcase the use of a small language model (SLM) to generate better-quality webpage text features and summaries at scale and describe our approach to mitigating position bias in user interaction logs.</p>
    <p><strong>Categories:</strong> Web Recommendations, Large Language Models, Quality Improvement, Clickbait Detection, Ranking Algorithms, User Behavior Analysis, Position Bias Mitigation, Content Quality, Webpage Summarization, Authoritative Content, Real-world Applications, Beyond Accuracy (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1152/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>ReLand: Integrating Large Language Models’ Insights into Industrial Recommenders via a Controllable Reasoning Pool (2024)</h3>
    <p><strong>Authors:</strong> Haoyu Chen, Changxin Tian, Jiawei Chen, Jun Zhou, Ziqi Liu, Li Yu, Chunjing Gan, Zhuo Zhang, Binbin Hu, Zhiqiang Zhang</p>
    <p>Recently, Large Language Models (LLMs) have shown significant potential in addressing the isolation issues faced by recommender systems. However, despite performance comparable to traditional recommenders, the current methods are cost-prohibitive for industrial applications. Consequently, existing LLM-based methods still need to catch up regarding effectiveness and efficiency. To tackle the above challenges, we present an LLM-enhanced recommendation framework named ReLand, which leverages Retrieval to effortlessly integrate Large language models’ insights into industrial recommenders. Specifically, ReLand employs LLMs to perform generative recommendations on sampled users (a.k.a., seed users), thereby constructing an LLM Reasoning Pool. Subsequently, we leverage retrieval to attach reliable recommendation rationales for the entire user base, ultimately effectively improving recommendation performance. Extensive offline and online experiments validate the effectiveness of ReLand. Since January 2024, ReLand has been deployed in the recommender system of Alipay, achieving statistically significant improvements of 3.19% in CTR and 1.08% in CVR.</p>
    <p><strong>Categories:</strong> Large Language Models, Recommender Systems, Industrial Applications, Retrieval-Based Methods, Generative Recommendations, Controllable Reasoning Pool, Cold Start, Interpretability of Recommendations, Diversity of Recommendations, Evaluation Metrics (CTR, CVR), Real-World Applications, A/B Testing (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1054/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Recommending Healthy and Sustainable Meals exploiting Food Retrieval and Large Language Models (2024)</h3>
    <p><strong>Authors:</strong> Michele Ciro Di Carlo, Giovanni Semeraro, Giovanni Tempesta, Cataldo Musto, Alessandro Petruzzelli</p>
    <p>Many people are constantly seeking to make healthy food choices, but increasingly, we are also considering the impact our dietary habits have on the environment. This creates a complex challenge: how can we ensure that we are eating nutritious foods that nourish our bodies while also minimizing the environmental footprint of our meals?To address this issue, this paper proposes a novel framework called Healthy And Sustainable eating (HeASe). Given the rising global concerns about nutrition and environmental sustainability, individuals need effective tools to help them navigate these issues. HeASe leverages the latest advancements in artificial intelligence to empower users with knowledge and self-awareness.The framework works in two steps. First, it uses a food retrieval strategy that takes into account macro-nutrient information to identify alternative meals for a chosen recipe. This ensures that the substitutions maintain a similar nutritional profile. Next, HeASe employs large language models to re-rank these potential replacements while considering factors beyond just nutrition, such as the recipe’s environmental impact and other user-defined preferences. The experimental phase of this research demonstrates the capabilities of LLM in identifying the more sustainable and healthy recipe within a set of candidate options. This highlights the potential of these models to guide users towards food choices that are both nutritious and environmentally responsible.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Healthy Eating, Sustainability, Food Retrieval, Large Language Models, Beyond Accuracy, Diversity of Recommendations, Real-world Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1115/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Retrieval-augmented Recommender System: Enhancing Recommender Systems with Large Language Models (2023)</h3>
    <p><strong>Authors:</strong> Dario Di Palma</p>
    <p>Recommender Systems (RSs) play a pivotal role in delivering personalized recommendations across various domains, from e-commerce to content streaming platforms. Recent advancements in natural language processing have introduced Large Language Models (LLMs) that exhibit remarkable capabilities in understanding and generating human-like text. RS are renowned for their effectiveness and proficiency within clearly defined domains; nevertheless, they are limited in adaptability and incapable of providing recommendations for unexplored data. Conversely, LLMs exhibit contextual awareness and strong adaptability to unseen data. Combining these technologies creates a potent tool for delivering contextual and relevant recommendations, even in cold scenarios characterized by high data sparsity. The proposal aims to explore the possibilities of integrating LLMs into RS, introducing a novel approach called Retrieval-augmented Recommender Systems, which combines the strengths of retrieval-based and generation-based models to enhance the ability of RSs to provide relevant suggestions.</p>
    <p><strong>Categories:</strong> Recommender Systems, Large Language Models, Retrieval-Based Models, Generation-Based Models, Cold Start, Contextual Awareness, System Design, Adaptability, Hybrid Models, Innovative Approaches, Exploiting Language Models (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/984/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>User-Centric Conversational Recommendation: Adapting the Need of User with Large Language Models (2023)</h3>
    <p><strong>Authors:</strong> Gangyi Zhang</p>
    <p>Conversational recommender systems (CRS) promise to provide a more natural user experience for exploring and discovering items of interest through ongoing conversation. However, effectively modeling user preferences during conversations and generating personalized recommendations in real time remain challenging problems. Users often express their needs in a vague and evolving manner, and CRS must adapt to capture the dynamics and uncertainty in user preferences to have productive interactions. This research develops user-centric methods for building conversational recommendation system that can understand complex and changing user needs. We propose a graph-based conversational recommendation framework that represents multi-turn conversations as reasoning over a user-item-attribute graph. Enhanced conversational path reasoning incorporates graph neural networks to improve representation learning in this framework. To address uncertainty and dynamics in user preferences, we present the vague preference multi-round conversational recommendation scenario and an adaptive vague preference policy learning solution that employs reinforcement learning to determine recommendation and preference elicitation strategies tailored to the user. Looking to the future, large language models offer promising opportunities to enhance various aspects of CRS, including user modeling, policy learning, response generation.  Overall, this research takes a user-centered perspective in designing conversational agents that can adapt to the inherent ambiguity involved in natural language dialogues with people.</p>
    <p><strong>Categories:</strong> Conversational Recommender Systems, Large Language Models, Graph Neural Networks, Reinforcement Learning, User Dynamics, Conversational Agents, Personalization, Real-Time Recommendations, Multi-Round Dialogue, AI/ML in Recommendations, User-Centered Design (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/983/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Large Language Models are Competitive Near Cold-start Recommenders for Language- and Item-based Preferences (2023)</h3>
    <p><strong>Authors:</strong> Lucas Dixon, Krisztian Balog, Filip Radlinski, Scott Sanner, Ben Wedin</p>
    <p>Traditional recommender systems leverage users’ item preference history to recommend novel content that users may like.  However, dialog interfaces that allow users to express language-based preferences offer a fundamentally different modality for preference input.  Inspired by recent successes of prompting paradigms for large language models (LLMs), we study their use for making recommendations from both item-based and language-based preferences in comparison to state-of-the-art item-based collaborative filtering (CF) methods.  To support this investigation, we collect a new dataset consisting of both item-based and language-based preferences elicited from users along with their ratings on a variety of (biased) recommended items and (unbiased) random items.  Among numerous experimental results, we find that LLMs provide competitive recommendation performance for pure language-based preferences (no item preferences) in the near cold-start case in comparison to item-based CF methods, despite having no supervised training for this specific task (zero-shot) or only a few labels (few-shot).  This is particularly promising as language-based preference representations are more explainable and scrutable than item-based or vector-based representations.</p>
    <p><strong>Categories:</strong> Recommender Systems, Cold Start, Language-Based Preferences, Item-Based Preferences, Large Language Models, Zero-Shot Learning, Few-Shot Learning, Dataset Collection, Evaluation Methodology, Collaborative Filtering, Algorithm Comparisons, Explainability (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/922/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Enhanced Privacy Preservation for Recommender Systems (2023)</h3>
    <p><strong>Authors:</strong> Ziqing Wu</p>
    <p>My research focuses on privacy preservation for recommender systems specifically in the following aspects: first, how to better address users’ realistic privacy concerns and offer enhanced privacy control by considering what and with whom to share sensitive information for decentralized recommender systems; second, how to enhance the privacy preservation capability of LLM-based recommender systems; last, how to formulate uniform metrics to compare the privacy-preservation efficacy of the recommender system.</p>
    <p><strong>Categories:</strong> Privacy Preservation, Recommender Systems, User-Centric Design, Decentralized Systems, Large Language Models, Machine Learning, Deep Learning, Evaluation Metrics, Performance Measurement, Algorithm Evaluation, Data Security, Trust (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/976/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation (2023)</h3>
    <p><strong>Authors:</strong> Keqin Bao, Jizhi Zhang, Xiangnan He, Fuli Feng, Wenjie Wang, Yang Zhang</p>
    <p>The impressive performance of Large Language Models (LLMs) across various fields has encouraged researchers to investigate their potential in recommendation tasks. To harness the LLMs’ extensive knowledge and powerful generalization abilities, initial efforts have tried to design instructions for recommendation tasks through In-context Learning. However, the recommendation performance of LLMs remains limited due to (i) significant differences between LLMs’ language-related pre-training tasks and recommendation tasks, and (ii) inadequate recommendation data during the LLMs’ pre-training. To fill the gap, we consider further tuning LLMs for recommendation tasks. To this end, we propose a lightweight tuning framework for LLMs-based recommendation, namely LLM4Rec, which constructs the recommendation data as tuning samples and utilizes LoRA for lightweight tuning. We conduct experiments on two datasets, validating that LLM4Rec is highly efficient w.r.t. computing costs (e.g., a single RTX 3090 is sufficient for tuning LLaMA-7B), and meanwhile, it can significantly enhance the recommendation capabilities of LLMs in the movie and book domains, even with limited tuning samples (< 100 samples). Furthermore, LLM4Rec exhibits strong generalization ability in cross-domain recommendation. Our code and data are available at https://anonymous.4open.science/r/LLM4rec.</p>
    <p><strong>Categories:</strong> Large Language Models, LoRA (Low-Rank Adaptation), Movies, Books, In-Context Learning, Cross-Domain Recommendation, Computational Efficiency, Dataset Evaluation, Recommendation Systems, Tuning Frameworks, Lightweight Fine-Tuning, Generalization Ability, Open-Source Implementation (<i>deepseek-r1:70b</i>)</p>
    <p><a href="/recsys-recommender/response/936/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Leveraging Large Language Models for Sequential Recommendation (2023)</h3>
    <p><strong>Authors:</strong> Panos Louridas, Dietmar Jannach, Marios Fragkoulis, Wouter Zorgdrager, Jesse Harte, Asterios Katsifodimos</p>
    <p>Sequential recommendation problems have received increasing attention in research during the past few years, leading to the inception of a large variety of algorithmic approaches. In this work, we explore how large language models (LLMs), which are nowadays introducing disruptive effects in many AI-based applications, can be used to build or improve sequential recommendation approaches. Specifically, we devise and evaluate three approaches to leverage the power of LLMs in different ways. Our results from experiments on two datasets show that initializing the state-of-the-art sequential recommendation model BERT4Rec with embeddings obtained from an LLM improves NDCG by 15-20% compared to the vanilla BERT4Rec model. Furthermore, we find that a simple approach that leverages LLM embeddings for producing recommendations, can provide competitive performance by highlighting semantically related items. We publicly share the code and data of our experiments to ensure reproducibility.</p>
    <p><strong>Categories:</strong> Large Language Models, Sequential Recommendation, Algorithmic Approaches, Recommendation Systems, Evaluation Methods, Natural Language Processing, Performance Improvement, Embeddings, Reproducibility, Datasets, Research Methods (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/956/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Towards Companion Recommenders Assisting Users’ Long-Term Journeys (2023)</h3>
    <p><strong>Authors:</strong> Minmin Chen, Konstantina Christakopoulou</p>
    <p>Nowadays, with the abundance of the internet content, users expect the recommendation platforms to not only help them with one-off decisions and short-term tasks, but to also support their persistent and overarching interest journeys, including their real-life goals that last days, months or even years. In order for recommender systems to truly assist users through their real-life journeys, they need to first be able to understand and reason about interests, needs, and goals users want to pursue; and then plan taking those into account. However, the task presents several challenges. In this talk, we will present the key steps and elements needed to tackle the problem — particularly (1) user research for interest journeys; (2) personalized and interpretable user profiles; (3) adapting large language models, and other foundational models, for better user understanding; (4) better planning at a macro-level through reinforcement learning and reason-and-act conversational agents; (5) novel journey-powered front end user experiences, allowing for more user control. We hope that the talk will help inspire other researchers, and will pave the way towards companion recommenders that can truly assist the users throughout their interest journeys.</p>
    <p><strong>Categories:</strong> Companion Recommenders, Long-term Goals, User Journeys, Personalized Recommendations, Large Language Models, Foundational Models, Reinforcement Learning, Conversational Agents, Reasoning Mechanisms, Interest Journey Modeling, User Control, Human-Centered Design, Sustained Engagement (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1019/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>