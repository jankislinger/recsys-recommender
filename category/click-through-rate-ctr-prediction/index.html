<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/real-world-applications/">Real-world Applications</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/multi-task-learning/">Multi-Task Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reinforcement-learning/">Reinforcement Learning</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>FLIP: Fine-grained Alignment between ID-based Models and Pretrained Language Models for CTR Prediction (2024)</h3>
    <p><strong>Authors:</strong> Bo Chen, Ruiming Tang, Xiangyang Li, Hangyu Wang, Yong Yu, Jianghao Lin, Weinan Zhang, Chenxu Zhu</p>
    <p>Click-through rate (CTR) prediction plays as a core function module in various personalized online services. The traditional ID-based models for CTR prediction take as inputs the one-hot encoded ID features of tabular modality, which capture the collaborative signals via feature interaction modeling. But the one-hot encoding discards the semantic information included in the textual features. Recently, the emergence of Pretrained Language Models (PLMs) has given rise to another paradigm, which takes as inputs the sentences of textual modality obtained by hard prompt templates and adopts PLMs to extract the semantic knowledge. However, PLMs often face challenges in capturing field-wise collaborative signals and distinguishing features with subtle textual differences. In this paper, to leverage the benefits of both paradigms and meanwhile overcome their limitations, we propose to conduct Fine-grained feature-level ALignment between ID-based Models and Pretrained Language Models (FLIP) for CTR prediction. Unlike most methods that solely rely on global views through instance-level contrastive learning, we design a novel jointly masked tabular/language modeling task to learn fine-grained alignment between tabular IDs and word tokens. Specifically, the masked data of one modality (\ie, IDs and tokens) has to be recovered with the help of the other modality, which establishes the feature-level interaction and alignment via sufficient mutual information extraction between dual modalities. Moreover, we propose to jointly finetune the ID-based model and PLM by adaptively combining the output of both models, thus achieving superior performance in downstream CTR prediction tasks. Extensive experiments on three real-world datasets demonstrate that FLIP outperforms SOTA baselines, and is highly compatible with various ID-based models and PLMs. The code is available for reviewers\footnote{\url{https://anonymous.4open.science/r/FLIP-2534}}.</p>
    <p><strong>Categories:</strong> Click-through Rate (CTR) Prediction, ID-based Models, Pretrained Language Models (PLMs), Multi-Modality, Fine-grained Alignment, Feature-level Contrastive Learning, Model Combination, Mutual Information, Joint Finetuning, Recommender Systems, General Machine Learning. (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1046/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>LARR: Large Language Model Aided Real-time Scene Recommendation with Semantic Understanding (2024)</h3>
    <p><strong>Authors:</strong> Zhizhong Wan, Fei Jiang, Wei Lin, Bin Yin, Junjie Xie, Xiang Li</p>
    <p>Click-Through Rate (CTR) prediction is crucial for Recommendation System(RS), aiming to provide personalized recommendation services for users in many aspects such as food delivery, e-commerce and so on. However, traditional RS relies on collaborative signals, which lacks semantic understanding to real-time scenes. We also noticed that a major challenge in utilizing Large Language Models (LLMs) for practical recommendation purposes is their efficiency in dealing with long text input. To break through the problems above, we propose Large Language Model Aided Real-time Scene Recommendation(LARR), adopt LLMs for semantic understanding, utilizing real-time scene information in RS without requiring LLM to process the entire real-time scene text directly, thereby enhancing the efficiency of LLM-based CTR modeling. Specifically, recommendation domain-specific knowledge is injected into LLM and then RS employs an aggregation encoder to build real-time scene information from separate LLM’s outputs. Firstly, a LLM is continue-pretrained on corpus built from recommendation data with the aid of special tokens. Subsequently, the LLM is fine-tuned via contrastive learning on three kinds of sample construction strategies. Through this step, LLM is transformed into a text embedding model. Finally, LLM’s separate outputs for different scene features are aggregated by an encoder, aligning to collaborative signals in RS, enhancing the performance of recommendation model.</p>
    <p><strong>Categories:</strong> Recommendation Systems (RS), Large Language Models (LLMs), Real-Time Processing, Semantic Understanding, Click-Through Rate (CTR) Prediction, Efficiency in LLMs, Aggregation Methods, Pre-training and Fine-tuning, Contrastive Learning, Text Embedding Models (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1043/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Loss Harmonizing for Multi-Scenario CTR Prediction (2023)</h3>
    <p><strong>Authors:</strong> Congcong Liu, Zhangang Lin, Changping Peng, Jingping Shao, Fei Teng, Xue Jiang, Pei Wang, Liang Shi</p>
    <p>Large-scale industrial systems often include multiple scenarios to satisfy diverse user needs. The common approach of using one model per scenario does not scale well and not suitable for minor scenarios with limited samples. An solution is to train a model on all scenarios, which can introduce domination and bias from the main scenario. MMoE-like structures have been proposed for multi-scenario prediction, but they do not explicitly address the issue of gradient unbalancing. This work proposes an adaptive loss harmonizing (ALH) algorithm for multi-scenario CTR prediction. It balances training by dynamically adjusting the learning speed, resulting in improved prediction performance. Experiments conducted on real production dataset and a rigorous A/B test prove the superiority of our method.</p>
    <p><strong>Categories:</strong> Click-Through Rate (CTR) Prediction, Multi-Scenario Prediction, Machine Learning Optimization, Large-Scale Systems, Recommendation Systems, Real-World Applications, Model Efficiency, Multi-Task Learning, Adaptive Algorithms (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1003/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Position Awareness Modeling with Knowledge Distillation for CTR Prediction (2022)</h3>
    <p><strong>Authors:</strong> Yuejiang Li, Fei Teng, Zhangang Lin, Jingping Shao, Congcong Liu, Changping Peng, Jian Zhu, Xiwei Zhao</p>
    <p>Click-through rate (CTR) Prediction is of great importance in real-world online ads systems. One challenge for the CTR prediction task is to capture the real interest of users from their clicked items, which is inherently influenced by presented positions of items, i.e., more front positions tend to obtain higher CTR values. Therefore, It is crucial to make CTR models aware of the exposed position of the items. A popular line of existing works focuses on explicitly model exposed position by result randomization which is expensive and inefficient, or by inverse propensity weighting (IPW) which relies heavily on the quality of the propensity estimation. Another common solution is modeling position as features during offline training and simply adopting fixed value or dropout tricks when serving. However, training-inference inconsistency can lead to sub-optimal performance. This work proposes a simple yet efficient knowledge distillation framework to model the impact of exposed position and leverage position information to improve CTR prediction. We demonstrate the performance of our proposed method on a real-world production dataset and online A/B tests, achieving significant improvements over competing baseline models. The proposed method has been deployed in the real world online ads systems of JD, serving main traffic of hundreds of millions of active users.</p>
    <p><strong>Categories:</strong> Click-Through Rate (CTR) Prediction, Online Advertising, Knowledge Distillation, Position Awareness, Inverse Propensity Weighting, Training-Inference Consistency, Real-World Applications, A/B Test, Recommendation Systems, Practical Deployment, Beyond Accuracy (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/795/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>AutoRec: An Automated Recommender System (2020)</h3>
    <p><strong>Authors:</strong> Qingquan Song, Xiaotian Han, Xia Hu, Zirui Liu, Haifeng Jin, Ting-Hsiang Wang</p>
    <p>Realistic recommender systems are often required to adapt to ever-changing data and tasks or to explore different models systematically. To address the need, we present AutoRec  1  2, an open-source automated machine learning (AutoML) platform extended from the TensorFlow [3] ecosystem and, to our knowledge, the first framework to leverage AutoML for model search and hyperparameter tuning in deep recommendation models. AutoRec also supports a highly flexible pipeline that accommodates both sparse and dense inputs, rating prediction and click-through rate (CTR) prediction tasks, and an array of recommendation models. Lastly, AutoRec provides a simple, user-friendly API. Experiments conducted on the benchmark datasets reveal AutoRec is reliable and can identify models which resemble the best model without prior knowledge.</p>
    <p><strong>Categories:</strong> AutoML, Recommender Systems, Deep Learning Models, Model Search, Hyperparameter Tuning, Rating Prediction, Click-Through Rate (CTR) Prediction, Open Source Tools, Machine Learning Framework, Sparse Data Handling (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/592/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Field-aware Factorization Machines for CTR Prediction (2016)</h3>
    <p><strong>Authors:</strong> Yuchin Juan, Wei-Sheng Chin, Yong Zhuang, Chih-Jen Lin</p>
    <p>Click-through rate (CTR) prediction plays an important role in computational advertising. Models based on degree-2 polynomial mappings and factorization machines (FMs) are widely used for this task. Recently, a variant of FMs, field-aware factorization machines (FFMs), outperforms existing models in some world-wide CTR-prediction competitions. Based on our experiences in winning two of them, in this paper we establish FFMs as an effective method for classifying large sparse data including those from CTR prediction. First, we propose efficient implementations for training FFMs. Then we comprehensively analyze FFMs and compare this approach with competing models. Experiments show that FFMs are very useful for certain classification problems. Finally, we have released a package of FFMs for public use.</p>
    <p><strong>Categories:</strong> Field-aware Factorization Machines, Factorization Machines, Click-through Rate (CTR) Prediction, Computational Advertising, Large Sparse Data, Experimental Analysis, Implementation, Real-world Applications, Classification, Scalability. (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/171/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>