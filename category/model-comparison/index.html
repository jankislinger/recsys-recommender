<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/explainability/">Explainability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/offline-evaluation/">Offline Evaluation</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/scalability/">Scalability</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>RePlay: a Recommendation Framework for Experimentation and Production Use (2024)</h3>
    <p><strong>Authors:</strong> Denis Kulandin, Tatiana Bysheva, Alexey Vasilev, Anton Klenitskiy, Anna Volodkevich</p>
    <p>Using a single tool to build and compare recommender systems significantly reduces the time to market for new models. In addition, the comparison results when using such tools look more consistent. This is why many different tools and libraries for researchers in the field of recommendations have recently appeared. Unfortunately, most of these frameworks are aimed primarily at researchers and require modification for use in production due to the inability to work on large datasets or an inappropriate architecture. In this demo, we present our open-source toolkit RePlay – a framework containing an end-to-end pipeline for building recommender systems, which is ready for production use. RePlay also allows you to use a suitable stack for the pipeline on each stage: Pandas, Polars, or Spark. This allows the library to scale computations and deploy to a cluster. Thus, RePlay allows data scientists to easily move from research mode to production mode using the same interfaces.</p>
    <p><strong>Categories:</strong> Recommender Systems, Research to Production, Model Comparison, Scalability, End-to-End Pipeline, Evaluation Pipelines, Open Source Tools, Multi-Stack Solutions, Data Science Workflow, Experimentation (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1209/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Challenging the Myth of Graph Collaborative Filtering: a Reasoned and Reproducibility-driven Analysis (2023)</h3>
    <p><strong>Authors:</strong> Claudio Pomo, Eugenio Di Sciascio, Daniele Malitesta, Tommaso Di Noia, Vito Walter Anelli, Alejandro Bellogin</p>
    <p>Among the most successful research directions in recommender systems, there are undoubtedly graph neural network-based models (GNNs). Through the natural modeling of users and items as a bipartite, undirected graph, GNNs have pushed up the performance bar for modern recommenders. Unfortunately, most of the original graph-based works cherry-pick results from previous baseline papers without bothering to check whether the results are valid for the configuration under analysis. Thus, our work stands first and foremost as a work on the replicability of results. We provide a code that succeeds in replicating the results proposed in the articles introducing six of the most popular and recent graph recommendation models (i.e., NGCF, DGCF, LightGCN, SGL, UltraGCN, and GFCF). In our experimental setup, we test these six models on three common benchmarking datasets (i.e., Gowalla, Yelp 2018, and Amazon Book). In addition, to understand how these models perform with respect to traditional models for collaborative filtering, we compare the graph models under analysis with some models that have historically emerged as the best performers in an offline evaluation context. Then, the study is extended on two new datasets (i.e., Allrecipes and BookCrossing) for which no known setup exists in the literature. Since the performance on such datasets is not entirely aligned with the previous benchmarking one, we further analyze the possible impact of specific dataset characteristics on the recommendation accuracy performance. By investigating the information flow to the users from their neighborhoods, the analysis aims to identify for which models these intrinsic features in the dataset structure impact accuracy performance. The code to reproduce the experiments is available at: https://split.to/Graph-Reproducibility.</p>
    <p><strong>Categories:</strong> Reproducibility, Collaborative Filtering, Graph Neural Networks, Books, Restaurants, Recommender Systems, Accuracy, Dataset Analysis, Beyond Accuracy, Model Comparison, Traditional Methods, Evaluation Metrics (<i>deepseek-r1:70b</i>)</p>
    <p><a href="/recsys-recommender/response/940/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>A Case Study on Sampling Strategies for Evaluating Neural Sequential Item Recommendation Models (2021)</h3>
    <p><strong>Authors:</strong> Daniel Zoller, Andreas Hotho, Alexander Dallmann</p>
    <p>At the present time, sequential item recommendation models are compared by calculating metrics on a small item subset (target set) to speed up computation. The target set contains the relevant item and a set of negative items that are sampled from the full item set. Two well-known strategies to sample negative items are uniform random sampling and sampling by popularity to better approximate the item frequency distribution in the dataset. Most recently published papers on sequential item recommendation rely on sampling by popularity to compare the evaluated models. However, recent work has already shown that an evaluation with uniform random sampling may not be consistent with the full ranking, that is, the model ranking obtained by evaluating a metric using the full item set as target set, which raises the question whether the ranking obtained by sampling by popularity is equal to the full ranking. In this work, we re-evaluate current state-of-the-art sequential recommender models from the point of view, whether these sampling strategies have an impact on the final ranking of the models. We therefore train four recently proposed sequential recommendation models on five widely known datasets. For each dataset and model, we employ three evaluation strategies. First, we compute the full model ranking. Then we evaluate all models on a target set sampled by the two different sampling strategies, uniform random sampling and sampling by popularity with the commonly used target set size of 100, compute the model ranking for each strategy and compare them with each other. Additionally, we vary the size of the sampled target set. Overall, we find that both sampling strategies can produce inconsistent rankings compared with the full ranking of the models. Furthermore, both sampling by popularity and uniform random sampling do not consistently produce the same ranking when compared over different sample sizes. Our results suggest that like uniform random sampling, rankings obtained by sampling by popularity do not equal the full ranking of recommender models and therefore both should be avoided in favor of the full ranking when establishing state-of-the-art.</p>
    <p><strong>Categories:</strong> Sequential Recommendation, Evaluation Strategy, Sampling Methods, Model Comparison, Metrics, Recommendation Systems, Uniform Random Sampling, Popularity-Based Sampling, Full Ranking, Evaluation Limitations, Sample Size, Evaluation Bias (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/672/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Transformers4Rec: Bridging the Gap between NLP and Sequential / Session-Based Recommendation (2021)</h3>
    <p><strong>Authors:</strong> Ronay Ak, Even Oldridge, Sara Rabhi, Gabriel de Souza Pereira Moreira, Jeong Min Lee</p>
    <p>Much of the recent progress in sequential and session-based recommendation has been driven by improvements in model architecture and pretraining techniques originating in the field of Natural Language Processing. Transformer architectures in particular have facilitated building higher-capacity models and provided data augmentation and training techniques which demonstrably improve the effectiveness of sequential recommendation. But with a thousandfold more research going on in NLP, the application of transformers for recommendation understandably lags behind. To remedy this we introduce Transformers4Rec, an open-source library built upon HuggingFace’s Transformers library with a similar goal of opening up the advances of NLP based Transformers to the recommender system community and making these advancements immediately accessible for the tasks of sequential and session-based recommendation. Like its core dependency, Transformers4Rec is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments.<br>In order to demonstrate the usefulness of the library and the applicability of Transformer architectures in next-click prediction for user sessions, where sequence lengths are much shorter than those commonly found in NLP, we have leveraged Transformers4Rec to win two recent session-based recommendation competitions. In addition, we present in this paper the first comprehensive empirical analysis comparing many Transformer architectures and training approaches for the task of session-based recommendation. We demonstrate that the best Transformer architectures have superior performance across two e-commerce datasets while performing similarly to the baselines on two news datasets. We further evaluate in isolation the effectiveness of the different training techniques used in causal language modeling, masked language modeling, permutation language modeling and replacement token detection for a single Transformer architecture, XLNet. We establish that training XLNet with replacement token detection performs well across all datasets. Finally, we explore techniques to include side information such as item and user context features in order to establish best practices and show that the inclusion of side information uniformly improves recommendation performance. Transformers4Rec library is available at https://github.com/NVIDIA-Merlin/Transformers4Rec/</p>
    <p><strong>Categories:</strong> Transformers, Session-based Recommendation, Sequential Recommendation, Open Source Libraries, Empirical Analysis, Model Comparison, Language Modeling Techniques, E-commerce, News, Side Information, Industrial Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/668/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Exploring Data Splitting Strategies for the Evaluation of Recommendation Models (2020)</h3>
    <p><strong>Authors:</strong> Craig Macdonald, Richard McCreadie, Zaiqiao Meng, Iadh Ounis</p>
    <p>Effective methodologies for evaluating recommender systems are critical, so that different systems can be compared in a sound manner. A commonly overlooked aspect of evaluating recommender systems is the selection of the data splitting strategy. In this paper, we both show that there is no standard splitting strategy and that the selection of splitting strategy can have a strong impact on the ranking of recommender systems during evaluation. In particular, we perform experiments comparing three common data splitting strategies, examining their impact over seven state-of-the-art recommendation models on two datasets. Our results demonstrate that the splitting strategy employed is an important confounding variable that can markedly alter the ranking of recommender systems, making much of the currently published literature non-comparable, even when the same datasets and metrics are used.</p>
    <p><strong>Categories:</strong> Evaluation Methods, Recommender Systems, Data Splitting, Model Comparison, Research Methodology, Impact of Evaluation Strategy, Reproducibility, Experimental Design, Cross-Dataset Analysis, Statistical Significance, Research Practices (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/608/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Negative-Aware Collaborative Filtering (2019)</h3>
    <p><strong>Authors:</strong> Yu-Neng Chuang, Chuan-Ju Wang, Ming-Feng Tsai, Sheng-Fang Yang, Sheng-Chieh Lin</p>
    <p>Most traditional recommender systems regard unseen user-item associations as negative user preferences and optimize recommendation models mainly based on observed associations and some negative instances sampled from unseen associations. However, such unseen user-item associations may contain potential positive user preferences on items and are not uniformly distributed in terms of the possibility of being negative (or positive) user preference; therefore, it is essential to quantify such associations for model training. Along this line, in this paper, in contrast to existing recommendation models, which equally treat all unseen associations as negative samples, we present a negative-aware recommendation approach that explicitly models the likelihood of each unseen association being a potentially positive preference. Empirical results on real-world datasets in different fields show that our approach consistently improves recommendation performance.</p>
    <p><strong>Categories:</strong> Recommender Systems, Collaborative Filtering, Negative Sampling, Positive Preference Prediction, Probabilistic Modeling, Likelihood Estimation, Traditional Methods vs. New Approaches, Real-World Applications, Cross-Domain Analysis, Model Comparison, Statistical Modeling, Recommendation Performance Metrics, Handling Unobserved Data (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/524/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Online Learning to Rank for Sequential Music Recommendation (2019)</h3>
    <p><strong>Authors:</strong> Alberto Ueda, Nivio Ziviani, Bruno L. Pereira, Gustavo Penha, Rodrygo L. T. Santos</p>
    <p>The prominent success of music streaming services has brought increasingly complex challenges for music recommendation. In particular, in a streaming setting, songs are consumed sequentially within a listening session, which should cater not only for the user’s historical preferences, but also for eventual preference drifts, triggered by a sudden change in the user’s context. In this paper, we propose a novel online learning to rank approach for music recommendation aimed to continuously learn from the user’s listening feedback. In contrast to existing online learning approaches for music recommendation, we leverage implicit feedback as the only signal of the user’s preference. Moreover, to adapt rapidly to preference drifts over millions of songs, we represent each song in a lower dimensional feature space and explore multiple directions in this space as duels of candidate recommendation models. Our thorough evaluation using listening sessions from Last.fm demonstrates the effectiveness of our approach at learning faster and better compared to state-of-the-art online learning approaches.</p>
    <p><strong>Categories:</strong> Music Recommendations, Online Learning, Implicit Feedback, Streaming Music, Sequential Recommendations, Preference Drift, Real-World Applications, Learning to Rank, Feature Engineering, Model Comparison, Recommendation Systems, Dynamic Recommendations, Evaluation Metrics (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/449/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Find My Next Job Labor Market Recommendations Using Administrative Big Data (2019)</h3>
    <p><strong>Authors:</strong> Snorre S. Frid-Nielsen</p>
    <p>Labor markets are undergoing change due to factors such as automatization and globalization, motivating the development of occupational recommender systems for jobseekers and caseworkers. This study generates occupational recommendations by utilizing a novel data set consisting of administrative records covering the entire Danish workforce. Based on actual labor market behavior in the period 2012-2015, how well can different models predict each users’ next occupation in 2016? Through offline experiments, the study finds that gradient-boosted decision tree models provide the best recommendations for future occupations in terms of mean reciprocal ranking and recall. Further, gradient-boosted decision tree models offer distinct advantages in the labor market domain due to their interpretability and ability to harness additional background information on workers. However, the study raises concerns regarding trade-offs between model accuracy and ethical issues, including privacy and the social reinforcement of gender divides. i>Presentation: Tuesday Poster Lunch</i</p>
    <p><strong>Categories:</strong> Labor Markets, Big Data, Recommender Systems, Administrative Records, Machine Learning, Gradient Boosting, Model Comparison, Evaluation Metrics, Ethics in AI, Privacy, Gender Bias, Workforce Development (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/478/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Predicting Online Performance of Job Recommender Systems With Offline Evaluation (2019)</h3>
    <p><strong>Authors:</strong> Adrien Mogenet, Tuan Anh Nguyen Pham, Masahiro Kazama, Jialin Kong</p>
    <p>Recommender systems can be used to recommend jobs. In this context, implicit and explicit feedback signals we can collect are rare events, making the task of evaluation more complex. Online evaluation (A-B testing) is usually the most reliable way to measure the results from our experiments, but it is a slow process. In contrast, the offline evaluation process is faster, but it is critical to make it reliable as it informs our decision to roll out new improvements in production. In this paper, we review the comparative offline and online performances of three recommendations models, we describe the evaluation metrics we use and analyze how the offline performance metrics correlate with online metrics to understand how an offline evaluation process can be leveraged to inform the decisions. i>Presentation: Wednesday Poster Lunch</i</p>
    <p><strong>Categories:</strong> Job Recommendations, Recommendation Models, Offline Evaluation, Online Evaluation, Evaluation Metrics, Model Comparison, Performance Prediction, Decision-Making, Methodology (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/487/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>RecGAN: Recurrent Generative Adversarial Networks for Recommendation Systems (2018)</h3>
    <p><strong>Authors:</strong> Homin Park, Homanga Bharadhwaj, Brian Y. Lim</p>
    <p>Recent studies in recommendation systems emphasize the significance of modeling latent features behind temporal evolution of user preference and item state to make relevant suggestions. However, static and dynamic behaviors and trends of users and items, which highly influence the feasibility of recommendations, were not adequately addressed in previous works. In this work, we leverage the temporal and latent feature modelling capabilities of Recurrent Neural Network (RNN) and Generative Adversarial Network (GAN), respectively, to propose a Recurrent Generative Adversarial Network (RecGAN). We use customized Gated Recurrent Unit (GRU) cells to capture latent features of users and items observable from short-term and long-term temporal profiles. The modification also includes collaborative filtering mechanisms to improve the relevance of recommended items. To evaluated RecGAN using two datasets on food and movie recommendation. Results indicate that our model outperforms other baseline models irrespective of user behavior and density of training data.</p>
    <p><strong>Categories:</strong> Generative Adversarial Networks (GANs), Recurrent Neural Networks (RNNs), Recommendation Systems, Temporal Modeling, Latent Features, Collaborative Filtering, Food Domain, Movie Domain, Real-World Applications, Dynamic User Behavior, Evaluation Metrics, Model Comparison (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/394/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>BERT, ELMo, USE and InferSent Sentence Encoders: The Panacea for Research-Paper Recommendation? (2019)</h3>
    <p><strong>Authors:</strong> Alessandro Micarelli, Giuseppe Sansonetti, Joeran Beel, Fabio Gasparetti, Hebatallah A. Mohamed Hassan</p>
    <p>Content-based approaches to research paper recommendation are important when user feedback is sparse or not available. The task of content-based matching is challenging, mainly due to the problem of determining the semantic similarity of texts. Nowadays, there exist many sentence embedding models that learn deep semantic representations by being trained on huge corpora, aiming to provide transfer learning to a wide variety of natural language processing tasks. In this work, we present a comparative evaluation among five well-known pre-trained sentence encoders deployed in the pipeline of title-based research paper recommendation. The experimented encoders are USE, BERT, InferSent, ELMo, and SciBERT. For our study, we propose a methodology for evaluating such models in reranking BM25-based recommendations. The experimental results show that the sole consideration of semantic information from these encoders does not lead to improved recommendation performance over the traditional BM25 technique, while their integration enables the retrieval of a set of relevant papers that may not be retrieved by the BM25 ranking function.</p>
    <p><strong>Categories:</strong> BERT, ELMo, USE, InferSent, Research Papers, Content-Based Recommendations, Natural Language Processing, Evaluation, Model Comparison, Recommendation Evaluation, BM25 Integration, Semantic Similarity (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/510/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Intent-Aware Diversification Using a Constrained PLSA (2016)</h3>
    <p><strong>Authors:</strong> Jacek Wasilewski, Neil Hurley</p>
    <p>The intent-aware diversification framework was introduced initially in information retrieval and adopted to the context of recommender systems in the work of Vargas et al. The framework considers a set of aspects associated with items to be recommended. For instance, aspects may correspond to genres in movie recommendations. The framework depends on input aspect model consisting of item selection or relevance probabilities, given an aspect, and user intents, in the form of probabilities that the user is interested in each aspect. In this paper, we examine a number of input aspect models and evaluate the impact that different models have on the framework. In particular, we propose a constrained PLSA model that allows for interpretable output, in terms of known aspects, while achieving greater performance that the explicit co-occurrence counting method used in previous work. We evaluate the proposed models using a well-known MovieLens dataset for which item genres are available.</p>
    <p><strong>Categories:</strong> Intents, Diversification, Recommender Systems, PLSA, Information Retrieval, Evaluation, MovieLens, Movies, Aspect-Based Recommendations, User Intent Modeling, Genre-Based Recommendations, Model Comparison, Diversification Strategies (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/197/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Implicit vs. Explicit Trust in Social Matrix Factorization (2014)</h3>
    <p><strong>Authors:</strong> Alejandro Bellogin, Peter Sloep, Hendrik Drachsler, Babak Loni, Soude Fazeli</p>
    <p>Incorporating social trust in Matrix Factorization (MF) methods demonstrably improves accuracy of rating prediction. Such approaches mainly use the trust scores explicitly expressed by users. However, it is often challenging to have users provide explicit trust scores of each other. There exist quite a few works, which propose Trust Metrics (TM) to compute and predict trust scores between users based on their interactions. In this paper, we first evaluate several TMs to find out which one can best predict trust scores compared to the actual trust scores explicitly expressed by users. And, second, we propose to incorporate these trust scores inferred from the candidate implicit TMs into social matrix factorization (MF). We investigate if incorporating the implicit trust scores in MF can make rating prediction as accurate as the MF on explicit trust scores. The reported results support the idea of employing implicit trust into MF whenever explicit trust is not available, since the performance of both models is similar.</p>
    <p><strong>Categories:</strong> Matrix Factorization, Social Matrix Factorization, Trust Metrics, Implicit Feedback, Explicit Feedback, Rating Prediction, Recommendation Systems, Social Networks, Model Comparison, Beyond Accuracy, User Behavior, Trust in Recommenders (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/45/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>