<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/bias-mitigation/">Bias Mitigation</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/explainability/">Explainability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/e-commerce/">E-commerce</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Stalactite: toolbox for fast prototyping of vertical federated learning systems (2024)</h3>
    <p><strong>Authors:</strong> Maria Khodorchenko, Anastasiia Zakharova, Dmitriy Alexandrov, Alexey Vasilev, Maxim Savchenko, Nikolay Butakov, Alexander Grigorievskiy</p>
    <p>Machine learning (ML) models trained on datasets owned by different organizations and physically located in remote databases offer benefits in many real-world use cases. State regulations or business requirements often prevent data transfer to a central location, making it difficult to utilize standard machine learning algorithms. Federated Learning (FL) is a technique that enables models to learn from distributed datasets without revealing the original data. Vertical Federated learning (VFL) is a type of FL where data samples are divided by features across several data owners. For instance, in a recommendation task, a user can interact with various sets of items, and the logs of these interactions are stored by different organizations. In this demo paper, we present Stalactite – an open-source framework for VFL that provides the necessary functionality for building prototypes of VFL systems. It has several advantages over the existing frameworks. In particular, it allows researchers to focus on the algorithmic side rather than engineering and to easily deploy learning in a distributed environment. It implements several VFL algorithms and has a built-in homomorphic encryption layer. We demonstrate its use on a real-world recommendation datasets.</p>
    <p><strong>Categories:</strong> Federated Learning, Vertical Federated Learning (VFL), Recommendation Systems, Machine Learning Frameworks, Algorithm Implementation, Homomorphic Encryption, Security and Privacy, Distributed Systems, Multi-Party Computation, Rapid Prototyping, Real-World Applications, Data Privacy, Open Source Tools (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1208/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>RePlay: a Recommendation Framework for Experimentation and Production Use (2024)</h3>
    <p><strong>Authors:</strong> Denis Kulandin, Tatiana Bysheva, Alexey Vasilev, Anton Klenitskiy, Anna Volodkevich</p>
    <p>Using a single tool to build and compare recommender systems significantly reduces the time to market for new models. In addition, the comparison results when using such tools look more consistent. This is why many different tools and libraries for researchers in the field of recommendations have recently appeared. Unfortunately, most of these frameworks are aimed primarily at researchers and require modification for use in production due to the inability to work on large datasets or an inappropriate architecture. In this demo, we present our open-source toolkit RePlay – a framework containing an end-to-end pipeline for building recommender systems, which is ready for production use. RePlay also allows you to use a suitable stack for the pipeline on each stage: Pandas, Polars, or Spark. This allows the library to scale computations and deploy to a cluster. Thus, RePlay allows data scientists to easily move from research mode to production mode using the same interfaces.</p>
    <p><strong>Categories:</strong> Recommender Systems, Research to Production, Model Comparison, Scalability, End-to-End Pipeline, Evaluation Pipelines, Open Source Tools, Multi-Stack Solutions, Data Science Workflow, Experimentation (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1209/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Can editorial decisions impair journal recommendations? Analysing the impact of journal characteristics on recommendation systems (2024)</h3>
    <p><strong>Authors:</strong> Elias Entrup, Anett Hoppe, Ralph Ewerth</p>
    <p>Recommendation services for journals help scientists choose appropriate publication venues for their research results. They often use a semantic matching process to compare e.g. an abstract against already published articles. As these services can guide a researcher’s decision, their fairness and neutrality are critical qualities. However, the impact of journal characteristics (such as the abstract length) on recommendations is understudied. In this paper, we investigate whether editorial journal characteristics can lead to biased rankings from recommendation services, i.e. if editorial choices can systematically lead to a better ranking of one’s own journal. The performed experiments show that longer abstracts or a higher number of articles per journal can boost the rank of a journal in the recommendations. We apply these insights to an active, open-source journal recommendation system. The adaptation of the algorithm leads to an increased accuracy for smaller journals.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Journal Publishing, Bias in Recommendations, Editorial Decisions, Algorithm Adaptation, Open-Source Tools, Academic Publishing, Journal Characteristics, Fairness in AI/ML, Semantic Matching, User Impact (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1083/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Improving Recommender Systems Through the Automation of Design Decisions (2023)</h3>
    <p><strong>Authors:</strong> Lukas Wegmeth</p>
    <p>Recommender systems developers are constantly faced with difficult design decisions. Additionally, the number of options that a recommender systems developer has to consider continually grows over time with new innovations. The machine learning community is in a similar situation and has come together to tackle the problem. They invented concepts and tools to make machine learning development both easier and faster. These developments are categorized as automated machine learning (AutoML). As a result, the AutoML community formed and continuously innovates new approaches. Inspired by AutoML, the recommender systems community has recently understood the need for automation and sparsely introduced AutoRecSys. The goal of AutoRecSys is not to replace recommender systems developers but to improve performance through the automation of design decisions. With AutoRecSys, recommender systems engineers do not have to focus on easy but time-consuming tasks and are free to pursue difficult engineering tasks instead. Additionally, AutoRecSys enables easier access to recommender systems for beginners as it reduces the amount of knowledge required to get started with the development of recommender systems. AutoRecSys, like AutoML, is still early in its development and does not yet cover the whole development pipeline. Additionally, it is not yet clear, under which circumstances AutoML approaches can be transferred to recommender systems. Our research intends to close this gap by improving AutoRecSys both with regard to the transfer of AutoML and novel approaches. Furthermore, we focus specifically on the development of novel automation approaches for data processing and training. We note that the realization of AutoRecSys is going to be a community effort. Our part in this effort is to research AutoRecSys fundamentals, build practical tools for the community, raise awareness of the advantages of automation, and catalyze AutoRecSys development.</p>
    <p><strong>Categories:</strong> Automation, Recommender Systems, AutoML, AutoRecSys, Design Decisions, Developer Tools, Machine Learning in Recommendations, Data Processing Automation, Training Automation, System Optimization, Research, Community Collaboration, Education, Emerging Technologies, Fundamental Research, Open Source Tools (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/982/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>V-Elliot: Design, Evaluate and Tune Visual Recommender Systems (2021)</h3>
    <p><strong>Authors:</strong> Claudio Pomo, Felice Antonio Merra, Alejandro Bellogin, Vito Walter Anelli, Daniele Malitesta, Antonio Ferrara, Francesco M Donini, Tommaso Di Noia</p>
    <p>The paper introduces Visual-Elliot (V-Elliot), a reproducibility framework for Visual Recommendation systems (VRSs) based on Elliot. framework provides the widest set of VRSs compared to other recommendation frameworks in the literature (i.e., 6 state-of-the-art models which have been commonly employed as baselines in recent works). The framework pipeline spans from the dataset preprocessing and item visual features loading to easily train and test complex combinations of visual models and evaluation settings. V-Elliot provides an extended set of features to ease the design, testing, and integration of novel VRSs into V-Elliot. The framework exploits of dataset filtering/splitting functions, 40 evaluation metrics, five hyper-parameter optimization methods, more than 50 recommendation algorithms, and two statistical hypothesis tests. The files of this demonstration are available at: github.com/sisinflab/elliot.</p>
    <p><strong>Categories:</strong> Visual Recommender Systems, Recommender System Frameworks, Reproducibility, Baseline Models, Evaluation Metrics, Hyperparameter Optimization, Dataset Preprocessing, Statistical Tests, Open Source Tools, System Design, Framework Evaluation, Tools and Systems (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/710/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>You Do Not Need a Bigger Boat: Recommendations at Reasonable Scale in a (Mostly) Serverless and Open Stack. (2021)</h3>
    <p><strong>Authors:</strong> Jacopo Tagliabue</p>
    <p>We argue that immature data pipelines are preventing a large portion of industry practitioners from leveraging the latest research on recommender systems. We propose our template data stack for machine learning at “reasonable scale”, and show how many challenges are solved by embracing a serverless paradigm. Leveraging our experience, we detail how modern open source tools can provide a pipeline processing terabytes of data with minimal infrastructure work.</p>
    <p><strong>Categories:</strong> Recommender Systems, Serverless Architecture, Open Source Tools, Data Pipelines, Scalability, Big Data Processing (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/734/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>AutoRec: An Automated Recommender System (2020)</h3>
    <p><strong>Authors:</strong> Qingquan Song, Xiaotian Han, Xia Hu, Zirui Liu, Haifeng Jin, Ting-Hsiang Wang</p>
    <p>Realistic recommender systems are often required to adapt to ever-changing data and tasks or to explore different models systematically. To address the need, we present AutoRec  1  2, an open-source automated machine learning (AutoML) platform extended from the TensorFlow [3] ecosystem and, to our knowledge, the first framework to leverage AutoML for model search and hyperparameter tuning in deep recommendation models. AutoRec also supports a highly flexible pipeline that accommodates both sparse and dense inputs, rating prediction and click-through rate (CTR) prediction tasks, and an array of recommendation models. Lastly, AutoRec provides a simple, user-friendly API. Experiments conducted on the benchmark datasets reveal AutoRec is reliable and can identify models which resemble the best model without prior knowledge.</p>
    <p><strong>Categories:</strong> AutoML, Recommender Systems, Deep Learning Models, Model Search, Hyperparameter Tuning, Rating Prediction, Click-Through Rate (CTR) Prediction, Open Source Tools, Machine Learning Framework, Sparse Data Handling (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/592/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Carousel Personalization in Music Streaming Apps with Contextual Bandits (2020)</h3>
    <p><strong>Authors:</strong> Théo Bontempelli, Guillaume Salha, Walid Bendada</p>
    <p>Media services providers, such as music streaming platforms, frequently leverage swipeable carousels to recommend personalized content to their users. However, selecting the most relevant items (albums, artists, playlists...) to display in these carousels is a challenging task, as items are numerous and as users have different preferences. In this paper, we model carousel personalization as a contextual multi-armed bandit problem with multiple plays, stochastic arm display and delayed batch feedback. We empirically show the effectiveness of our framework at capturing characteristics of real-world carousels by addressing a large-scale playlist recommendation task on a global music streaming mobile app. Along with this paper, we publicly release industrial data from our experiments, as well as an open-source environment to simulate comparable carousel personalization learning problems.</p>
    <p><strong>Categories:</strong> Contextual Bandits, Multi-Armed Bandits, Recommendation Systems, Personalization, Scalability, Carousel Design, Music Streaming, Real-World Applications, A/B Testing, Open Source Tools, Data Sharing (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/563/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Are We Evaluating Rigorously? Benchmarking Recommendation for Reproducible Evaluation and Fair Comparison (2020)</h3>
    <p><strong>Authors:</strong> Zhu Sun, Jie Zhang, Di Yu, Xinghua Qu, Hui Fang, Jie Yang, Cong Geng</p>
    <p>With tremendous amount of recommendation algorithms proposed every year, one critical issue has attracted a considerable amount of attention: there are no effective benchmarks for evaluation, which leads to two major concerns, i.e., unreproducible evaluation and unfair comparison. This paper aims to conduct rigorous (i.e., reproducible and fair) evaluation for implicit-feedback based top-N recommendation algorithms. We first systematically review 85 recommendation papers published at eight top-tier conferences (e.g., RecSys, SIGIR) to summarize important evaluation factors, e.g., data splitting and parameter tuning strategies, etc. Through a holistic empirical study, the impacts of different factors on recommendation performance are then analyzed in-depth. Following that, we create benchmarks with standardized procedures and provide the performance of seven well-tuned state-of-the-arts across six metrics on six widely-used datasets as a reference for later study. Additionally, we release a user-friendly Python toolkit, which differs from existing ones in addressing the broad scope of rigorous evaluation for recommendation. Overall, our work sheds light on the issues in recommendation evaluation and lays the foundation for further investigation. Our code and datasets are available at GitHub (https://github.com/AmazingDD/daisyRec).</p>
    <p><strong>Categories:</strong> Recommender Systems, Methodology, Reproducibility in Science, Benchmarking, Evaluation Protocols, Implicit Feedback, Fairness in AI, Performance Metrics, Reproducible Research, Open Source Tools, Literature Review, RecSys, SIGIR, Research Methodology (<i>deepseek-r1:70b</i>)</p>
    <p><a href="/recsys-recommender/response/584/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>BETA-Rec: Build, Evaluate and Tune Automated Recommender Systems (2020)</h3>
    <p><strong>Authors:</strong> Iadh Ounis, Yaxiong Wu, Shangsong Liang, Zaiqiao Meng, Craig Macdonald, Guangtao Zeng, Richard McCreadie, Siwei Liu, Yucheng Liang, Qiang Zhang, Junhua Liang</p>
    <p>The field of recommender systems has rapidly evolved over the last few years, with significant advances made due to the in-flux of deep learning techniques. However, as a result of this rapid progress, escalating barriers-to-entry for new researchers is emerging. In particular, state-of-the-art approaches have fragmented into a large number of code-bases, often requiring different input formats, pre-processing stages and evaluating with different metric packages. Hence, it is time-consuming for new researchers to reach the point of having both an effective baseline set and a sound comparative environment. As a step towards elevating this problem, we have developed BETA-Rec, an open source project for Building, Evaluating and Tuning Automated Recommender Systems. BETA-Rec aims to provide a practical data toolkit for building end-to-end recommendation systems in a standardized way. It provides means for dataset preparation and splitting using common strategies, a generalized model engine for implementing recommender models using Pytorch with 9 models available out-of-the-box, as well as a unified training, validation, tuning and testing pipeline. Furthermore, BETA-Rec is designed to be both modular and extensible, enabling new models to be quickly added to the framework. It is deployable in a wide range of environments via pre-built docker containers and supports distributed parameter tuning using Ray. In this demo, we will illustrate the deployment and use of BETA-Rec for researchers and practitioners on a number of standard recommendation datasets. The source code of the project is available at github: https://github.com/beta-team/beta-recsys.</p>
    <p><strong>Categories:</strong> Recommender Systems, Machine Learning Methods, Open Source Tools, Pre-trained Models, Hyperparameter Optimization, Deployment, Practical Applications, Modular Frameworks, Distributed Computing, Data Preprocessing, Benchmarking (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/594/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Microsoft Recommenders – Tools to Accelerate Developing Recommender Systems (2019)</h3>
    <p><strong>Authors:</strong> Scott Graham, Tao Wu, Jun Min</p>
    <p>The purpose of this demonstration is to highlight the content of the Microsoft Recommenders repository and show how it can be used to reduce the time involved in developing recommender systems. The open source repository provides python utilities to simplify common recommender-related data science work as well as example Jupyter notebooks that demonstrate use of the algorithms and tools under various environments.</p>
    <p><strong>Categories:</strong> Open Source Tools, Python Libraries, Recommender Systems Development, Development Efficiency, Tutorials &amp; Examples, Data Science Tools for Recommendation, General Recommendation, Educational Resources, Interactive Tools, Real-World Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/511/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>StreamingRec: A Framework for Benchmarking Stream-based News Recommenders (2018)</h3>
    <p><strong>Authors:</strong> Dietmar Jannach, Michael Jugovac, Mozhgan Karimi</p>
    <p>News is one of the earliest application domains of recommender systems, and recommending items from a virtually endless stream of news is still a relevant problem today. News recommendation is different from other application domains in a variety of ways, e.g., because new items constantly become available for recommendation. To be effective, news recommenders therefore have to continuously consider the latest items in the incoming stream of news in their recommendation models. However, today’s public software libraries for algorithm benchmarking mostly do not consider these particularities of the domain. As a result, authors often rely on proprietary protocols, which hampers the comparability of the obtained results. In this paper, we present StreamingRec as a framework for evaluating streaming-based news recommenders in a replicable way. The open-source framework implements a replay-based evaluation protocol that allows algorithms to update the underlying models in real-time when new events are recorded and new articles are available for recommendation. Furthermore, a variety of baseline algorithms for session-based recommendation are part of StreamingRec. For these, we also report a number of performance results for two datasets, which confirm the importance of immediate model updates.</p>
    <p><strong>Categories:</strong> Streaming Recommenders, News, Benchmarking Frameworks, Open Source Tools, Real-time Processing, Continuous Learning, Session-based Recommendations, Algorithm Evaluation (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/387/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Evaluating Tag Recommender Algorithms in Real-World Folksonomies: A Comparative Study (2015)</h3>
    <p><strong>Authors:</strong> Elisabeth Lex, Dominik Kowald</p>
    <p>To date, the evaluation of tag recommender algorithms has mostly been conducted in limited ways, including -core pruned datasets, a small set of compared algorithms and solely based on recommender accuracy. In this study, we use an open-source evaluation framework to compare a rich set of state-of-the-art algorithms in six unfiltered, open datasets via various metrics, measuring not only accuracy but also the diversity, novelty and computational costs of the approaches. We therefore provide a transparent and reproducible tag recommender evaluation in real-world folksonomies. Our results suggest that the efficacy of an algorithm highly depends on the given needs and thus, they should be of interest to both researchers and developers in the field of tag-based recommender systems.</p>
    <p><strong>Categories:</strong> Tag Recommender Algorithms, Folksonomy, Evaluation Metrics, Algorithm Comparison, Open Source Tools, Real-World Applications, Diversity of Recommendations, Beyond Accuracy, Computational Efficiency, Personalization, Reproducibility (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/116/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>