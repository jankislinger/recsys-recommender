<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">Evaluation Methodology</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/explainability/">Explainability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/bias-mitigation/">Bias Mitigation</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/real-world-applications/">Real-world Applications</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reinforcement-learning/">Reinforcement Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Fairness Matters: A look at LLM-generated group recommendations (2024)</h3>
    <p><strong>Authors:</strong> Antonela Tommasel</p>
    <p>Recommender systems play a crucial role in how users consume information, with group recommendation receiving considerable attention. Ensuring fairness in group recommender systems entails providing recommendations that are useful and relevant to all group members rather than solely reflecting the majority’s preferences, while also addressing fairness concerns related to sensitive attributes (e.g., gender). Recently, the advancements on Large Language Models (LLMs) have enabled the development of new kinds of recommender systems. However, LLMs can perpetuate social biases present in training data, posing risks of unfair outcomes and harmful impacts. We investigated LLMs impact on group recommendation fairness, establishing and instantiating a framework that encompasses group definition, sensitive attribute combinations, and evaluation methodology. Our findings revealed the interactions patterns between sensitive attributes and LLMs and how they affected recommendation. This study advances the understanding of fairness considerations in group recommendation systems, laying the groundwork for future research.</p>
    <p><strong>Categories:</strong> Recommender Systems, Fairness, Group Recommendations, Large Language Models (LLMs), Sensitive Attributes, Bias Mitigation, Natural Language Processing (NLP), Evaluation Methodology, Social Biases, Societal Impact (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1089/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Large Language Models are Competitive Near Cold-start Recommenders for Language- and Item-based Preferences (2023)</h3>
    <p><strong>Authors:</strong> Lucas Dixon, Krisztian Balog, Filip Radlinski, Scott Sanner, Ben Wedin</p>
    <p>Traditional recommender systems leverage users’ item preference history to recommend novel content that users may like.  However, dialog interfaces that allow users to express language-based preferences offer a fundamentally different modality for preference input.  Inspired by recent successes of prompting paradigms for large language models (LLMs), we study their use for making recommendations from both item-based and language-based preferences in comparison to state-of-the-art item-based collaborative filtering (CF) methods.  To support this investigation, we collect a new dataset consisting of both item-based and language-based preferences elicited from users along with their ratings on a variety of (biased) recommended items and (unbiased) random items.  Among numerous experimental results, we find that LLMs provide competitive recommendation performance for pure language-based preferences (no item preferences) in the near cold-start case in comparison to item-based CF methods, despite having no supervised training for this specific task (zero-shot) or only a few labels (few-shot).  This is particularly promising as language-based preference representations are more explainable and scrutable than item-based or vector-based representations.</p>
    <p><strong>Categories:</strong> Recommender Systems, Cold Start, Language-Based Preferences, Item-Based Preferences, Large Language Models, Zero-Shot Learning, Few-Shot Learning, Dataset Collection, Evaluation Methodology, Collaborative Filtering, Algorithm Comparisons, Explainability (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/922/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Evaluating The Effects of Calibrated Popularity Bias Mitigation: A Field Study (2023)</h3>
    <p><strong>Authors:</strong> Astrid Tessem, Anastasiia Klimashevskaia, Dietmar Jannach, Lars Skjærven, Christoph Trattner, Mehdi Elahi</p>
    <p>Despite their proven various benefits, Recommender Systems can cause or amplify certain undesired effects. In this paper, we focus on Popularity Bias, i.e., the tendency of a recommender system to utilize the effect of recommending popular items to the user. Prior research has studied the negative impact of this type of bias on individuals and society as a whole and proposed various approaches to mitigate this in various domains. However, almost all works adopted offline methodologies to evaluate the effectiveness of the proposed approaches. Unfortunately, such offline simulations can potentially be rather simplified and unable to capture the full picture. To contribute to this line of research and given a particular lack of knowledge about how debiasing approaches work not only offline, but online as well, we present in this paper the results of user study on a national broadcaster movie streaming platform in [country]1, i.e., [platform], following the A/B testing methodology. We deployed an effective mitigation approach for popularity bias, called Calibrated Popularity (CP), and monitored its performance in comparison to the platform’s existing collaborative filtering recommendation approach as a baseline over a period of almost four months. The results obtained from a large user base interacting in real-time with the recommendations indicate that the evaluated debiasing approach can be effective in addressing popularity bias while still maintaining the level of user interest and engagement</p>
    <p><strong>Categories:</strong> Recommender Systems, Popularity Bias, Field Study, A/B Testing, Collaborative Filtering, Real-Time Interaction, User Interest/Engagement, Streaming Platforms, Bias Mitigation, Evaluation Methodology, Real-World Applications, User Behavior Analysis (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/953/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Widespread flaws in offline evaluation of recommender systems (2023)</h3>
    <p><strong>Authors:</strong> Ádám Tibor Czapp, Balázs Hidasi</p>
    <p>Even though offline evaluation is just an imperfect proxy of online performance — due to the interactive nature of recommenders — it will probably remain the primary way of evaluation in recommender systems research for the foreseeable future, since the proprietary nature of production recommenders prevents independent validation of A/B test setups and verification of online results. Therefore, it is imperative that offline evaluation setups are as realistic and as flawless as they can be. Unfortunately, evaluation flaws are quite common in recommenders systems research nowadays, due to later works copying flawed evaluation setups from their predecessors without questioning their validity. In the hope of improving the quality of offline evaluation of recommender systems, we discuss four of these widespread flaws and why researchers should avoid them.</p>
    <p><strong>Categories:</strong> Evaluation Methodology, Offline Evaluation, Research Limitations, Research Flaws, Evaluation Challenges, Methodology Design, Underlying Assumptions, Real-World Applications, Research Practices, Evaluation Setup and Execution, Best Practices. (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/917/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>TinyKG: Memory-Efficient Training Framework for Knowledge Graph Neural Recommender Systems (2022)</h3>
    <p><strong>Authors:</strong> Hao Yang, Xia Hu, Chin-Chia Michael Yeh, Huiyuan Chen, Xiaoting Li, Kaixiong Zhou, Yan Zheng</p>
    <p>There has been an explosion of interest in designing various Knowledge Graph Neural Networks (KGNNs), which achieve state-of-the-art performance and provide great explainability for recommendation. The promising performance is mainly resulting from their capability of capturing high-order proximity messages over the knowledge graphs. However, training KGNNs at scale is challenging due to the high memory usage. In the forward pass, the automatic differentiation engines (e.g., TensorFlow/PyTorch) generally need to cache all intermediate activation maps in order to compute gradients in the backward pass, which leads to a large GPU memory footprint. Existing work solves this problem by utilizing multi-GPU distributed frameworks. Nonetheless, this poses a practical challenge when seeking to deploy KGNNs in memory-constrained environments, especially for industry-scale graphs.<br>Here we present TinyKG, a memory-efficient GPU-based training framework for KGNNs for the tasks of recommendation. Specifically, TinyKG uses exact activations in the forward pass while storing a quantized version of activations in the GPU buffers. During the backward pass, these low-precision activations are dequantized back to full-precision tensors, in order to compute gradients. To reduce the quantization errors, TinyKG applies a simple yet effective quantization algorithm to compress the activations, which ensures unbiasedness with low variance. As such, the training memory footprint of KGNNs is largely reduced with negligible accuracy loss. To evaluate the performance of our TinyKG, we conduct comprehensive experiments on real-world datasets. We found that our TinyKG with INT2 quantization aggressively reduces the memory footprint of activation maps with 7 ×, only with 2% loss in accuracy, allowing us to deploy KGNNs on memory-constrained devices.</p>
    <p><strong>Categories:</strong> Memory Optimization, Knowledge Graph Neural Networks, Recommendation Systems, Real-World Applications, Quantization Methods, GPU-Based Training, Memory Efficiency, Industry-Scale Problems, Resource Constraints, Evaluation Methodology, System Design, Activation Compression. (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/784/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>An Analysis Of Entire Space Multi-Task Models For Post-Click Conversion Prediction (2021)</h3>
    <p><strong>Authors:</strong> Rafael Barreto, Conor O’Brien, James Neufield, Kin Sum Liu, Jonathan J Hunt</p>
    <p>Industrial recommender systems are frequently tasked with approximating probabilities for multiple, often closely related, user actions. For example, predicting if a user will click on an advertisement and if they will then purchase the advertised product. The conceptual similarity between these tasks has promoted the use of multi-task learning: a class of algorithms that aim to bring positive inductive transfer from related tasks. Here, we empirically evaluate multi-task learning approaches with neural networks for an online advertising task. Specifically, we consider approximating the probability of post-click conversion events (installs) (CVR) for mobile app advertising on a large-scale advertising platform, using the related click events (CTR) as an auxiliary task. We use an ablation approach to systematically study recent approaches that incorporate both multitask learning and “entire space modeling” which train the CVR on all logged examples rather than learning a conditional likelihood of conversion given clicked. Based on these results we show that several different approaches result in similar levels of positive transfer from the data-abundant CTR task to the CVR task and offer some insight into how the multi-task design choices address the two primary problems affecting the CVR task: data sparsity and data bias. Our findings add to the growing body of evidence suggesting that standard multi-task learning is a sensible approach to modelling related events in real-world large-scale applications and suggest the specific multitask approach can be guided by ease of implementation in an existing system.</p>
    <p><strong>Categories:</strong> Recommender Systems, Online Advertising, Multi-Task Learning, Neural Networks, Entire Space Modeling, Evaluation Methodology, Data Sparsity, Data Bias, Mobile App Advertising, System Design (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/679/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Burst-induced Multi-Armed Bandit for Learning Recommendation (2021)</h3>
    <p><strong>Authors:</strong> Rodrigo Alves, Antoine Ledent, Marius Kloft</p>
    <p>In this paper, we introduce a non-stationary and context-free Multi-Armed Bandit (MAB) problem and a novel algorithm (which we refer to as BMAB) to solve it. The problem is context-free in the sense that no side information about users or items is needed. We work in a continuous-time setting where each timestamp corresponds to a visit by a user and a corresponding decision regarding recommendation. The main novelty is that we model the reward distribution as a consequence of variations in the intensity of the activity, and thereby we assist the exploration/exploitation dilemma by exploring the temporal dynamics of the audience. To achieve this, we assume that the recommendation procedure can be split into two different states: the loyal and the curious state. We identify the current state by modelling the events as a mixture of two Poisson processes, one for each of the possible states. We further assume that the loyal audience is associated with a single stationary reward distribution, but each bursty period comes with its own reward distribution. We test our algorithm and compare it to several baselines in two strands of experiments: synthetic data simulations and real-world datasets. The results demonstrate that BMAB achieves competitive results when compared to state-of-the-art methods.</p>
    <p><strong>Categories:</strong> Multi-Armed Bandits, Recommendation Systems, Non-Stationary Environments, Temporal Dynamics, Audience Behavior Analysis, Evaluation Methodology, Real-World Applications, Context-Free Methods, Algorithmic Innovation, Scalability, Online Learning, Poisson Processes (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/627/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Combining Text Summarization and Aspect-based Sentiment Analysis of Users’ Reviews to Justify Recommendations (2019)</h3>
    <p><strong>Authors:</strong> Pasquale Lops, Giovanni Semeraro, Marco de Gemmis, Cataldo Musto, Gaetano Rossiello</p>
    <p>In this paper we present a methodology to justify recommendations that relies on the information extracted from users’ reviews discussing the available items. The intuition behind the approach is to conceive the justification as a summary of the most relevant and distinguishing aspects ofthe item, automatically obtained by analyzing the available reviews. To this end, we designed a pipeline of natural language processing techniques based on aspect extraction, sentiment analysis and text summarization to gather the reviews, process the relevant excerpts,and generate a unique synthesis presenting the main characteristics of the item. Such a summary is finally presented to the target user as justification of the recommendation she received. In the experimental evaluation we carried out a user study in the movie domain (N=141) and the results showed that our approach is able to make the recommendation process more transparent, engaging and trustful for the users. Moreover, the proposed method also beat another review-based explanation technique, thus confirming the validity of our intuition. i>Presentation: Monday Poster Lunch</i</p>
    <p><strong>Categories:</strong> Recommendation Systems, Natural Language Processing, Text Summarization, Aspect-Based Sentiment Analysis, Movie Domain, User Study, Explainability, Trust, Experimental Results, Evaluation Methodology, Pipeline Design, Transparency in AI, User Feedback, Sentiment Analysis, Aspect Extraction, User-Centered Design (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/469/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Contrasting Offline and Online Results when Evaluating Recommendation Algorithms (2016)</h3>
    <p><strong>Authors:</strong> Marco Rossetti, Fabio Stella, Markus Zanker</p>
    <p>Most evaluations of novel algorithmic contributions assess their accuracy in predicting what was withheld in an offline evaluation scenario. However, several doubts have been raised that standard offline evaluation practices are not appropriate to select the best algorithm for field deployment. The goal of this work is therefore to compare the offline and the online evaluation methodology with the same study participants, i.e. a within users experimental design. This paper presents empirical evidence that the ranking of algorithms based on offline accuracy measurements clearly contradicts the results from the online study with the same set of users. Thus the external validity of the most commonly applied evaluation methodology is not guaranteed.</p>
    <p><strong>Categories:</strong> Offline Evaluation, Online Evaluation, Algorithmic Evaluation Methods, Recommendation Algorithms, External Validity, User-Centered Evaluation, Evaluation Methodology, Algorithm Selection, Empirical Evidence, Deployment Considerations, User Study, Recommendation Systems (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/199/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Overlapping Community Regularization for Rating Prediction in Social Recommender Systems (2015)</h3>
    <p><strong>Authors:</strong> Dingming Wu, Nikos Mamoulis, Wenbin Tang, Hui Li</p>
    <p>Recommender systems have become de facto tools for suggesting items that are of potential interest to users. Predicting a user’s rating on an item is the fundamental recommendation task. Traditional methods that generate predictions by analyzing the user-item rating matrix perform poorly when the matrix is sparse. Recent approaches use data from social networks to improve accuracy. However, most of the social-network based recommender systems only consider direct friendships and they are less effective when the targeted user has few social connections. In this paper, we propose two alternative models that incorporate the overlapping community regularization into the matrix factorization framework. Our empirical study on four real datasets shows that our approaches outperform the state-of-the-art algorithms in both traditional and social-network based recommender systems regarding both cold-start users and normal users.</p>
    <p><strong>Categories:</strong> Recommender Systems, Social Networks, Matrix Factorization, Cold Start, Regularization, Community Detection, Overlapping Communities, Rating Prediction, Evaluation Methodology, Empirical Study, System Comparisons, User Behavior Analysis, Accuracy Improvement (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/102/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Item Familiarity Effects in User-Centric Evaluations of Recommender Systems (2015)</h3>
    <p><strong>Authors:</strong> Michael Jugovac, Dietmar Jannach, Lukas Lerche</p>
    <p>Laboratory studies are a common way of comparing recommendation approaches with respect to different quality dimensions that might be relevant for real users. One typical experimental setup is to first present the participants with recommendation lists that were created with different algorithms and then ask the participants to assess these recommendations individually or to compare two item lists. The cognitive effort required by the participants for the evaluation of item recommendations in such settings depends on whether or not they already know the (features of the) recommended items. Furthermore, lists containing popular and broadly known items are correspondingly easier to evaluate. In this paper we report the results of a user study in which participants recruited on a crowdsourcing platform assessed system-provided recommendations in a between-subjects experimental design. The results surprisingly showed that . An analysis revealed a measurable correlation between item familiarity and user acceptance. Overall, the observations indicate that item familiarity can be a potential confounding factor in such studies and should be considered in experimental designs.</p>
    <p><strong>Categories:</strong> Recommender Systems, User-Centric Evaluation, Evaluation Methodology, Crowdsourcing, Human Factors in Recommendation, Item Familiarity, Experimental Design, User Studies, Behavioral Analysis, Confounding Factors (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/151/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>