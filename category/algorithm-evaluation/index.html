<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reinforcement-learning/">Reinforcement Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/e-commerce/">E-commerce</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/evaluation-metrics/">Evaluation Metrics</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Calibrating the Predictions for Top-N Recommendations (2024)</h3>
    <p><strong>Authors:</strong> Masahiro Sato</p>
    <p>Well-calibrated predictions of user preferences are essential for many applications. Since recommender systems typically select the top-N items for users, calibration for those top-N items, rather than for all items, is important.  We show that previous calibration methods result in miscalibrated predictions for the top-N items, despite their excellent calibration performance when evaluated on all items.  In this work, we address the miscalibration in the top-N recommended items. We first define evaluation metrics for this objective and then propose a generic method to optimize calibration models focusing on the top-N items. It groups the top-N items by their ranks and optimizes distinct calibration models for each group with rank-dependent training weights.  We verify the effectiveness of the proposed method for both explicit and implicit feedback datasets, using diverse classes of recommender models.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Top-N Recommendations, Calibration of Predictions, Evaluation Metrics, Optimization Methods, Explicit Feedback, Implicit Feedback, Diverse Recommenders, Accuracy of Recommendations, Matrix Factorization, Multi-Armed Bandits, Algorithm Evaluation, Item Ranking (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1082/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Transparently Serving the Public: Enhancing Public Service Media Values through Exploration (2023)</h3>
    <p><strong>Authors:</strong> Andreas Grün, Xenija Neufeld</p>
    <p>In the last few years, we have reportedly underlined the importance of the Public Service Media Remit for ZDF as a Public Service Media provider. Offering fair, diverse, and useful recommendations to users is just as important for us as being transparent about our understanding of these values, the metrics that we are using to evaluate their extent, and the algorithms in our system that produce such recommendations. This year, we have made a major step towards transparency of our algorithms and metrics describing them for a broader audience, offering the possibility for the audience to learn details about our systems and to provide direct feedback to us. Having the possibility to measure and track PSM metrics, we have started to improve our algorithms towards PSM values. In this work, we describe these steps and the results of actively debasing and adding exploration into our recommendations to achieve more fairness.</p>
    <p><strong>Categories:</strong> Public Service Media (PSM), Recommendation Systems, Transparency in Algorithms, Fairness in Recommendations, Diversity of Recommendations, Algorithm Evaluation, User Feedback, Exploration Strategies (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1015/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Enhanced Privacy Preservation for Recommender Systems (2023)</h3>
    <p><strong>Authors:</strong> Ziqing Wu</p>
    <p>My research focuses on privacy preservation for recommender systems specifically in the following aspects: first, how to better address users’ realistic privacy concerns and offer enhanced privacy control by considering what and with whom to share sensitive information for decentralized recommender systems; second, how to enhance the privacy preservation capability of LLM-based recommender systems; last, how to formulate uniform metrics to compare the privacy-preservation efficacy of the recommender system.</p>
    <p><strong>Categories:</strong> Privacy Preservation, Recommender Systems, User-Centric Design, Decentralized Systems, Large Language Models, Machine Learning, Deep Learning, Evaluation Metrics, Performance Measurement, Algorithm Evaluation, Data Security, Trust (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/976/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>User Bias in Beyond-Accuracy Measurement of Recommendation Algorithms (2021)</h3>
    <p><strong>Authors:</strong> Ningxia Wang, Li Chen</p>
    <p>There are various biases in recommender systems. Recognizing biases, as well as unfairness caused by problematic biases, is the first step of system optimization. Related studies on algorithmic biases are mainly from the perspective of either items or users. For the latter (we call it “algorithmic user bias”), existing works have considered algorithms’ accuracy performances measured by accuracy metrics like RMSE. However, algorithmic user biases in beyond-accuracy measurements have rarely been studied, even though beyond-accuracy oriented recommendation algorithms have been increasingly investigated, with the purpose of breaking through the personalization limits of traditional accuracy-oriented algorithms (such as the typical “filter bubble” phenomenon). To fill in the research gap, in this work, we employ a large-scale survey dataset collected from a commercial platform, in which more than 11,000 users’ ratings on the recommendation’s 5 performance objectives (i.e., relevance, diversity, novelty, unexpectedness, and serendipity) and 8 kinds of user characteristics (i.e., gender, age, big-5 personality traits, and curiosity) are available. We study user biases of four algorithms (i.e., HOT, Rel-CF, Nov-CF, and Ser-CF) in terms of those five measurements between user groups of the eight user characteristics. We further look into users’ behavior patterns like the preference of using more positive ratings, in order to interpret the observed biases. Finally, based on the observed algorithmic user bias and users’ behavior patterns, we analyze the possible factors leading to the biases and recognize problematic biases that may lead to unfairness.</p>
    <p><strong>Categories:</strong> Algorithmic Bias, User Bias, Beyond Accuracy, Recommendation Algorithms, Algorithm Evaluation, User Characteristics, Fairness, Survey Methods, Diversity, Personalization (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/669/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>The Connection Between Popularity Bias, Calibration, and Fairness in Recommendation (2020)</h3>
    <p><strong>Authors:</strong> Bamshad Mobasher, Robin Burke, Himan Abdollahpouri, Masoud Mansoury</p>
    <p>Recently there has been a growing interest in fairness-aware recommender systems including fairness in providing consistent performance across different users or groups of users. A recommender system could be considered unfair if the recommendations do not fairly represent the tastes of a certain group of users while other groups receive recommendations that are consistent with their preferences. In this paper, we use a metric called miscalibration for measuring how a recommendation algorithm is responsive to users’ true preferences and we consider how various algorithms may result in different degrees of miscalibration for different users. In particular, we conjecture that popularity bias which is a well-known phenomenon in recommendation is one important factor leading to miscalibration in recommendation. Our experimental results using two real-world datasets show that there is a connection between how different user groups are affected by algorithmic popularity bias and their level of interest in popular items. Moreover, we show that the more a group is affected by the algorithmic popularity bias, the more their recommendations are miscalibrated.</p>
    <p><strong>Categories:</strong> Fairness in Recommendations, Popularity Bias, Miscalibration, Real-World Applications, Algorithm Evaluation, User Group Analysis, Recommendation Fairness, Consistency in Recommendations, Algorithmic Impact, Recommendation Dynamics (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/611/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>From the Lab to Production: A Case Study of Session-Based Recommendations in the Home-Improvement Domain (2020)</h3>
    <p><strong>Authors:</strong> Pigi Kouki, Xiquan Cui, Khalifeh Al Jadda, Ilias Fountalis, Edo Liberty, Nikolaos Vasiloglou</p>
    <p>E-commerce applications rely heavily on session-based recommendation algorithms to improve the shopping experience of their customers. Recent progress in session-based recommendation algorithms shows great promise. However, translating that promise to real-world outcomes is a challenging task for several reasons, but mostly due to the large number and varying characteristics of the available models. In this paper, we discuss the approach and lessons learned from the process of identifying and deploying a successful session-based recommendation algorithm for a leading e-commerce application in the home-improvement domain. To this end, we initially evaluate fourteen session-based recommendation algorithms in an offline setting using eight different popular evaluation metrics on three datasets. The results indicate that offline evaluation does not provide enough insight to make an informed decision since there is no clear winning method on all metrics. Additionally, we observe that standard offline evaluation metrics fall short for this application. Specifically, they reward an algorithm only when it predicts the exact same item that the user clicked next or eventually purchased. In a practical scenario, however, there are near-identical products which, although they are assigned different identifiers, they should be considered as equally-good recommendations. To overcome these limitations, we perform an additional round of evaluation, where human experts provide both objective and subjective feedback for the recommendations of five algorithms that performed the best in the offline evaluation. We find that the experts’ opinion is oftentimes different from the offline evaluation results. Analysis of the feedback confirms that the performance of all models is significantly higher when we evaluate near-identical product recommendations as relevant. Finally, we run an A/B test with one of the models that performed the best in the human evaluation phase. The treatment model increased conversion rate by 15.6% and revenue per visit by 18.5% when compared with a leading third-party solution.</p>
    <p><strong>Categories:</strong> Session-Based Recommendations, Case Study, E-commerce, Retail, Home Improvement, Algorithm Evaluation, Human Evaluation, A/B Test, Offline Evaluation, Deployment, Beyond Accuracy (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/536/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>HybridSVD: When Collaborative Information is Not Enough (2019)</h3>
    <p><strong>Authors:</strong> Ivan Oseledets, Evgeny Frolov</p>
    <p>We propose a new hybrid algorithm that allows incorporating both user and item side information within the standard collaborative filtering technique. One of its key features is that it naturally extends a simple PureSVD approach and inherits its unique advantages, such as highly efficient Lanczos-based optimization procedure, simplified hyper-parameter tuning and a quick folding-in computation for generating recommendations instantly even in highly dynamic online environments. The algorithm utilizes a generalized formulation of the singular value decomposition, which adds flexibility to the solution and allows imposing the desired structure on its latent space. Conveniently, the resulting model also admits an efficient and straightforward solution for the cold start scenario. We evaluate our approach on a diverse set of datasets and show its superiority over similar classes of hybrid models. ,</p>
    <p><strong>Categories:</strong> Hybrid Recommendation Systems, Matrix Factorization, Collaborative Filtering, Algorithm Evaluation, Cold Start, Scalability, Latent Space Modeling (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/442/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>StreamingRec: A Framework for Benchmarking Stream-based News Recommenders (2018)</h3>
    <p><strong>Authors:</strong> Dietmar Jannach, Michael Jugovac, Mozhgan Karimi</p>
    <p>News is one of the earliest application domains of recommender systems, and recommending items from a virtually endless stream of news is still a relevant problem today. News recommendation is different from other application domains in a variety of ways, e.g., because new items constantly become available for recommendation. To be effective, news recommenders therefore have to continuously consider the latest items in the incoming stream of news in their recommendation models. However, today’s public software libraries for algorithm benchmarking mostly do not consider these particularities of the domain. As a result, authors often rely on proprietary protocols, which hampers the comparability of the obtained results. In this paper, we present StreamingRec as a framework for evaluating streaming-based news recommenders in a replicable way. The open-source framework implements a replay-based evaluation protocol that allows algorithms to update the underlying models in real-time when new events are recorded and new articles are available for recommendation. Furthermore, a variety of baseline algorithms for session-based recommendation are part of StreamingRec. For these, we also report a number of performance results for two datasets, which confirm the importance of immediate model updates.</p>
    <p><strong>Categories:</strong> Streaming Recommenders, News, Benchmarking Frameworks, Open Source Tools, Real-time Processing, Continuous Learning, Session-based Recommendations, Algorithm Evaluation (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/387/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Automating Recommender Systems Experimentation with librec-auto (2018)</h3>
    <p><strong>Authors:</strong> Robin Burke, Xavier Sepulveda, Masoud Mansoury, Aldo Ordonez-Gauger</p>
    <p>Recommender systems research often requires the creation and execution of large numbers of algorithmic experiments to determine the sensitivity of results to the values of various hyperparameters. Existing recommender systems platforms fail to provide a basis for systematic experimentation of this type. In this paper, we describe librec-auto, a wrapper for the well-known LibRec library, which provides an environment that supports automated experimentation.</p>
    <p><strong>Categories:</strong> Recommender Systems, Experimentation, Automation, Hyperparameter Tuning, Algorithmic Experiments, LibRec, Wrapper Tools, Automated Experimentation Environment, Research Tools, Software Development, Algorithm Evaluation, Systematic Testing (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/399/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>A Crowdsourcing Triage Algorithm for Geopolitical Forecasting (2018)</h3>
    <p><strong>Authors:</strong> David Huber, Tsai-Ching Lu, Mohammad Rostami</p>
    <p>Predicting the outcome of geopolitical events is of huge importance to many organizations, as these forecasts provide actionable intelligence that may be used to make consequential decisions. Prediction polling is a common method used in crowdsourcing platforms for geopolitical forecasting, where a group of non-expert participants are asked to predict the outcome of a geopolitical event and the collected responses are aggregated to generate a forecast. It has been demonstrated that forecasts by such a crowd can be more accurate than the forecasts of experts. However, geopolitical prediction polling is challenging because participants are highly heterogeneous and diverse in terms of their skills and background knowledge and human resources are often limited. As a result, it is crucial to refer each question to the subset of participants that possess suitable skills to answer it, such that individual efforts are not wasted. In this paper, we propose an algorithm based on multitask learning to learn the skills of participants of a forecasting platform by using their performance history. The learned model then can be used to recommend suitable questions to forecasters. Our experimental results demonstrate that the prediction accuracy can be increased based on the proposed algorithm as opposed to when questions have been randomly assigned.</p>
    <p><strong>Categories:</strong> Crowdsourcing, Geopolitical Forecasting, Multitask Learning, Recommendation Systems, Crowd Intelligence, Decision-Making, User Modeling, Algorithm Evaluation, Machine Learning Algorithms, Prediction Accuracy (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/364/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Data-Driven Repricing Strategies in Competitive Markets: An Interactive Simulation Platform (2017)</h3>
    <p><strong>Authors:</strong> Nikolai Jannik Podlesny, Martin Boissier, Jan Selke, Rainer Schlosser, Matthias Uflacker, Johanna Latt, Sebastian Serth, Marvin Bornstein, Jan Lindemann</p>
    <p>Modern e-commerce platforms pose both opportunities as well as hurdles for merchants. While merchants can observe markets at any point in time and automatically reprice their products, they also have to compete simultaneously with dozens of competitors. Currently, retailers lack the possibility to test, develop, and evaluate their algorithms appropriately before releasing them into the real world. At the same time, it is challenging for researchers to investigate how pricing strategies interact with each other under heavy competition. To study dynamic pricing competition on online marketplaces, we built an open simulation platform. To be both flexible and scalable, the platform has a microservice-based architecture and handles large numbers of competing merchants and arriving consumers. It allows merchants to deploy the full width of pricing strategies, from simple rule-based strategies to more sophisticated data-driven strategies using machine learning. Our platform enables analyses of how a strategy’s performance is affected by customer behavior, price adjustment frequencies, the competitors’ strategies, and the exit/entry of competitors. Moreover, our platform allows to study the long-term behavior of self-adapting strategies.</p>
    <p><strong>Categories:</strong> E-commerce, Competitive Markets, Dynamic Pricing Strategies, Simulation Platforms, Machine Learning Applications, Microservices Architecture, Algorithm Evaluation, Customer Behavior Analysis, Competitor Strategy Interaction, Scalability and Flexibility, Data-Driven Decision Making, Real-World Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/305/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Visual Analysis of Recommendation Performance (2017)</h3>
    <p><strong>Authors:</strong> Markus Zanker, Ludovik Çoba, Panagiotis Symeonidis</p>
    <p>rrecsys is a novel library in R for developing and assessing recommendation algorithms. In this demo, we extend rrecsys with functions for visual analytics of recommendation performance, that is one of the strong capabilities of the R environment. In particular, we show how the library can be used to depict dataset characteristics, train and test recommendation algorithms and to visually assess, for instance, their capability to exploit long-tail items for making correct predictions.</p>
    <p><strong>Categories:</strong> Visualization, Recommendation Systems, Algorithm Evaluation, R (programming language), Tools/Frameworks, Statistical Computing, Data Analysis, Machine Learning, Performance Analysis (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/304/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Recommendation with the Right Slice: Speeding Up Collaborative Filtering with Factorization Machines (2015)</h3>
    <p><strong>Authors:</strong> Babak Loni, Alan Hanjalic, Alexandros Karatzoglou, Martha Larson</p>
    <p>We propose an alternative way to efficiently exploit rating data for collaborative filtering with Factorization Machines (FMs). Our approach partitions user-item matrix into ‘slices’ which are mutually exclusive with respect to items. The training phase makes direct use of the slice of interest ( slice), while incorporating information from other slices indirectly. FMs represent user-item interactions as feature vectors, and they offer the advantage of easy incorporation of complementary information. We exploit this advantage to integrate information from other slices. We demonstrate, using experiments on two benchmark datasets, that improved performance can be achieved, while the time complexity of training can be reduced significantly.</p>
    <p><strong>Categories:</strong> Collaborative Filtering, Factorization Machines, Recommendations, Scalability, Efficiency Improvements, Rating-based Collaborative Filtering, Data Integration, Algorithm Evaluation, Time Complexity Analysis, Performance Optimization (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/154/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Correcting Popularity Bias by Enhancing Recommendation Neutrality (2014)</h3>
    <p><strong>Authors:</strong> Shotaro Akaho, Hideki Asoh, Toshihiro Kamishima, Jun Sakuma</p>
    <p>In this paper, we attempt to correct a popularity bias, which is the tendency for popular items to be recommended more frequently, by enhancing recommendation neutrality. Recommendation neutrality involves excluding specified information from the prediction process of recommendation. This neutrality was formalized as the statistical independence between a recommendation result and the specified information, and we developed a recommendation algorithm that satisfies this independence constraint. We correct the popularity bias by enhancing neutrality with respect to information regarding whether candidate items are popular or not. We empirically show that a popularity bias in the predicted preference scores can be corrected.</p>
    <p><strong>Categories:</strong> Recommendation Algorithm, Popularity Bias, Recommendation Neutrality, Bias Correction, Beyond Accuracy, Statistical Independence, Algorithm Evaluation, Fairness in AI/ML, Machine Learning (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/73/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>