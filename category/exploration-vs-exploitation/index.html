<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reinforcement-learning/">Reinforcement Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/explainability/">Explainability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/e-commerce/">E-commerce</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>LLMs for User Interest Exploration: A Hybrid Approach (2024)</h3>
    <p><strong>Authors:</strong> He Ma, Haokai Lu, Yueqi Wang, Yifan Liu, Ed H. Chi, Lexi Baugher, Ningren Han, Shuzhou Zhang, Yang Gu, Jianling Wang, Minmin Chen, Shuchao Bi</p>
    <p>Traditional recommendation systems are subject to a strong feedback loop by learning from and reinforcing past user-item interactions, which in turn limits the discovery of novel user interests. To address this, we introduce a hybrid hierarchical framework combining Large Language Models (LLMs) and classic recommendation models for user interest exploration. The framework controls the interfacing between the LLMs and the classic recommendation models through “interest clusters”, the granularity of which can be explicitly determined by algorithm designers. It recommends the next novel interests by first representing “interest clusters” using language, and employs a fine-tuned LLM to generate novel interest descriptions that are strictly within these predefined clusters. At the low level, it grounds these generated interests to an item-level policy by restricting classic recommendation models, in this case a transformer-based sequence recommender to return items that fall within the novel clusters generated at the high level. We showcase the efficacy of this approach on an industrial-scale commercial platform serving billions of users. Live experiments show a significant increase in both exploration of novel interests and overall user enjoyment of the platform.</p>
    <p><strong>Categories:</strong> Hybrid Models, Large Language Models (LLMs), Recommendation Systems, User Interest Exploration, Transformer-Based Recommenders, Content Generation, Interest Clustering, Real-World Applications, Live Experiments, User Behavior, Hierarchical Structures, Industrial Application, Scalability, Exploration vs Exploitation (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1092/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>The Fault in Our Recommendations: On the Perils of Optimizing the Measurable (2024)</h3>
    <p><strong>Authors:</strong> Akshit Kumar, Yash Kanoria, Omar Besbes</p>
    <p>Recommendation systems play a pivotal role on digital platforms by curating content, products and services for users. These systems are widespread, and through customized recommendations, promise to match users with options they will like. To that end,  data on engagement is collected and used, with, e.g., measurements of clicks, but also  purchases or consumption times.   Most recommendation systems are ranking-based, where they rank and recommend items based on their predicted engagement.  However, the engagement signals  are often only a crude proxy for user utility, as data on the latter is rarely collected or available. This paper explores the following critical research question: By optimizing for measurable proxies, are recommendation systems at risk of significantly under-delivering on user utility? If that is indeed the case, how can one improve utility which is seldom measured? To study these questions, we introduce a model of repeated user consumption in which, at each interaction, users select between an outside option and the best option from a recommendation set. Our model accounts for user heterogeneity, with the majority preferring “popular” content, and a minority favoring “niche” content. The system initially lacks knowledge of individual user preferences but can learn these preferences through observations of users’ choices over time. Our theoretical and numerical analysis demonstrate that optimizing for engagement signals can lead to significant utility losses. Instead, we propose a utility-aware policy that initially recommends a mix of popular and niche content. We show that such a policy substantially improves  utility despite not measuring it. In fact, in the limit of a forward-looking platform with discount factor $\delta \to 1$, our utility-aware policy achieves the best of both worlds: near-optimal user utility and near-optimal engagement simultaneously. Our study elucidates  an important feature of recommendation systems; given the ability to suggest multiple items, one can perform significant exploration without incurring significant reductions in short term engagement. By recommending high-risk, high-reward items alongside popular items, systems can enhance discovery of high utility items without significantly affecting engagement.</p>
    <p><strong>Categories:</strong> Optimization, Evaluation Metrics, Recommendation Systems, User Behavior, Engagement, Machine Learning, Reinforcement Learning, Bandit Algorithms, Ranking Algorithms, Digital Platforms, Personalization, Cold Start, Exploration vs Exploitation, Long-term User Satisfaction, Scalability, Theoretical Modeling, Numerical Analysis, User Heterogeneity, Ethical Considerations in AI, Beyond Accuracy, High-Risk High-Reward Recommendations (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1062/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Encouraging Exploration in Spotify Search through Query Recommendations (2024)</h3>
    <p><strong>Authors:</strong> Henrik Lindstrom, Humberto Jesús Corona Pampín, Alva Liu, Enrico Palumbo</p>
    <p>At Spotify, search has been traditionally seen as a tool for retrieving content, with the search system optimized for when the user has a specific search target in mind. In particular we have relied on an instant search system providing results for each keystroke, which works well for known-item search, when queries are straightforward, and the catalog is small. However, as Spotify’s catalog grows in size and variety, it becomes increasingly difficult for users to define their search intents accurately. Furthermore, as we expand the offering, we need to help users discover more content both when it comes to new content types, e.g. audiobooks, as well as for new content/creators within existing content types. To solve this we have introduced a hybrid Query Recommendation system (QR) that helps the user formulate more complex exploratory search intents, while still serving known-item lookups efficiently. This experience has been rolled out worldwide to all mobile users resulting in an increase in exploratory intent queries of ~9% in online A/B tests.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Search, Exploration vs Exploitation, Query Recommendations, Music/Media, Audiobooks, A/B Test, Beyond Accuracy, Content Discovery, Real World Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1160/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Balancing Habit Repetition and New Activity Exploration: A Longitudinal Micro-Randomized Trial in Physical Activity Recommendations (2024)</h3>
    <p><strong>Authors:</strong> Ine Coppens, Luc Martens, Toon De Pessemier</p>
    <p>As repetition of activities can establish habits and exploration of new ones can provide a healthy variety, we investigate how a recommender system for physical activities can optimally balance these two approaches. We conducted an eight-week user study with 62 physically inactive participants who receive personalized repetition and exploration recommendations in a random order. We distinguish between location, workout, and general activities, and collect participants’ subjective perceptions. Our findings indicate that participants initially preferred exploring general activities, but rated repeating recommendations higher after two weeks. By exploring the optimal transition point from exploration to repetition in personalized recommendations, this study contributes to designing more effective recommender systems for health improvement and healthy habit formation.</p>
    <p><strong>Categories:</strong> Recommender Systems, Personalized Recommendations, Exploration vs Exploitation, Health, Longitudinal Study, User Preferences, Behavior Change, A/B Test, Habit Formation, Activity Recommendations (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1189/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Incentivizing Exploration in Linear Contextual Bandits under Information Gap (2023)</h3>
    <p><strong>Authors:</strong> Zhiyuan Liu, Hongning Wang, Huazheng Wang, Chuanhao Li, Haifeng Xu</p>
    <p>Contextual bandit algorithms have been popularly used to address interactive recommendation, where the users are assumed to be cooperative to explore all recommendations from a system. In this paper, we relax this strong assumption and study the problem of incentivized exploration with myopic users, where the users are only interested in recommendations with their currently highest estimated reward. As a result, in order to obtain long-term optimality, the system needs to offer compensation to incentivize the users to take the exploratory recommendations. We consider a new and practically motivated setting where the context features employed by the user are more <i>informative</i> than those used by the system: for example, features based on users’ private information are not accessible by the system. We develop an effective solution for incentivized exploration under such an information gap, and prove that the method achieves a sublinear rate in both regret and compensation. We theoretically and empirically analyze the added compensation due to the information gap, compared with the case where the system has access to the same context features as the user does, i.e., without information gap. Moreover, we also provide a compensation lower bound of this problem.</p>
    <p><strong>Categories:</strong> Contextual Bandits, Reinforcement Learning, Exploration vs Exploitation, Incentivizing Exploration, User Behavior Modeling, Interactive Recommendations, Information Gap, Regret Analysis, Compensation Mechanisms, Theoretical Analysis, Empirical Evaluation, Multi-Armed Bandits (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/874/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>The role of preference consistency, defaults and musical expertise in users’ exploration behavior in a genre exploration recommender (2021)</h3>
    <p><strong>Authors:</strong> Yu Liang, Martijn C. Willemsen</p>
    <p>Recommender systems are efficient at predicting users’ current preferences, but how users’ preferences develop over time is still under-explored. In this work, we study the development of users’ musical preferences. Exploring musical preference consistency between short-term and long-term preferences in data from earlier studies, we find that users with higher musical expertise have more consistent preferences at their top-listened artists and tags than those with lower musical expertise. Users typically chose to explore genres that were close to their current preferences, and this effect was stronger for expert users. Based on these findings we conducted a user study on genre exploration to investigate (1) whether it is possible to nudge users to explore more distant genres, and (2) how users’ exploration behaviors within a genre are influenced by default recommendation settings that balance personalization with genre representativeness in different ways. Our results show that users were more likely to select the more distant genres if these were presented at the top of the list. However, users with high musical expertise were less likely to do so, consistent with our earlier findings. When given a representative or mixed (balanced) default for exploration within a genre, users selected less personalized recommendation settings and explored further away from their current preferences, than with a personalized default. However, this effect was moderated by users’ slider usage behaviors. Overall, our results suggest that (personalized) defaults can nudge users to explore new, more distant genres and songs. However, the effect is smaller for those with higher musical expertise levels.</p>
    <p><strong>Categories:</strong> Recommendation Systems, User Behavior, Preference Consistency, Genre Exploration, Musical Expertise, Defaults and Defaults Impact, Personalization vs Representativeness, User Study, Slider Usage, Exploration vs Exploitation, Default Settings Impact (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/665/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Deep Bayesian Bandits: Exploring in Online Personalized Recommendations (2020)</h3>
    <p><strong>Authors:</strong> Dalin Guo, Sourav Das, Michael Kneier, Pranay Kumar Kumar Myana, Alykhan Tejani, Sofia Ira Ira Ktena, Ferenc Huszar, Wenzhe Shi</p>
    <p>Recommender systems trained in a continuous learning fashion are plagued by the feedback loop problem, also known as algorithmic bias. This causes a newly trained model to act greedily and favor items that have already been engaged by users. This behavior is particularly harmful in personalised ads recommendations, as it can also cause new campaigns to remain unexplored. Exploration aims to address this limitation by providing new information about the environment, which encompasses user preference, and can lead to higher long-term reward. In this work, we formulate a display advertising recommender as a contextual bandit and implement exploration techniques that require sampling from the posterior distribution of click-through-rates in a computationally tractable manner. Traditional large-scale deep learning models do not provide uncertainty estimates by default. We approximate these uncertainty measurements of the predictions by employing a bootstrapped model with multiple heads and dropout units. We benchmark a number of different models in an offline simulation environment using a publicly available dataset of user-ads engagements. We test our proposed deep Bayesian bandits algorithm in the offline simulation and online AB setting with large-scale production traffic, where we demonstrate a positive gain of our exploration model.</p>
    <p><strong>Categories:</strong> Recommender Systems, Contextual Bandits, Bayesian Methods, Personalization, Deep Learning, Exploration vs Exploitation, Algorithmic Bias, Display Advertising, Uncertainty Estimation, Feedback Loop, Online AB Testing, Large-Scale Applications. (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/571/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Exploring Clustering of Bandits for Online Recommendation System (2020)</h3>
    <p><strong>Authors:</strong> Feng Xia, Qiang Yang, Bo Liu, Kai Chen, Leyu Lin, Liu Yang</p>
    <p>Cluster-of-bandit policy leverages contextual bandits in a collaborative filtering manner and aids personalized services in the online recommendation system (RecSys). When facing insufficient observations, the cluster-of-bandit policy could achieve more outstanding performance because of knowledge sharing. Cluster-of-bandit policy aims to maximize the cumulative feedback, e.g., clicks, from users. Nevertheless, in the way of their goal exist two kinds of uncertainties. First, cluster-of-bandit algorithms make recommendations according to their uncertain estimation of user interests. Second, cluster-of-bandit algorithms transfer relevant knowledge upon uncertain and noisy user clusters. Existing algorithms only consider the first one, while leaving the latter one untouched. To address the two challenges together, in this paper, we propose the ClexB policy for online RecSys. On the one hand, ClexB estimates user clustering more accurately and with less uncertainty via explorable-clustering. On the other hand, ClexB also exploits and explores user interests by sharing information within and among user clusters. In summary, ClexB explores knowledge transfer and further aids the inferences about user interests. Besides, we provide extensive empirical experiments on both the synthetic and real-world datasets and regret analysis, further consolidating the superiority of ClexB.</p>
    <p><strong>Categories:</strong> Clustering, Multi-Armed Bandits, Recommendation System, Contextual Bandits, Collaborative Filtering, Personalized Services, Uncertainty Handling, User Clustering, Knowledge Transfer, Exploration vs Exploitation, Empirical Experiments, Algorithm Design (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/533/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Explore, Exploit, and Explain: Personalizing Explainable Recommendations with Bandits (2018)</h3>
    <p><strong>Authors:</strong> Rishabh Mehrotra, Karl Higley, Benjamin Lacker, James McInerney, Hugues Bouchard, Alois Gruson, Samantha Hansen</p>
    <p>The multi-armed bandit is an important framework for balancing exploration with exploitation in recommendation. Exploitation recommends content (e.g., products, movies, music playlists) with the highest predicted user engagement and has traditionally been the focus of recommender systems. Exploration recommends content with uncertain predicted user engagement for the purpose of gathering more information. The importance of exploration has been recognized in recent years, particularly in settings with new users, new items, non-stationary preferences and attributes. In parallel, explaining recommendations (“recsplanations”) is crucial if users are to understand their recommendations. Existing work has looked at bandits and explanations independently. We provide the first method that combines both in a principled manner. In particular, our method is able to jointly (1) learn which explanations each user responds to; (2) learn the best content to recommend for each user; and (3) balance exploration with exploitation to deal with uncertainty. Experiments with historical log data and tests with live production traffic in a large-scale music recommendation service show a significant improvement in user engagement.</p>
    <p><strong>Categories:</strong> Multi-Armed Bandits, Personalization, Explainability, Recommendation Systems, Exploration vs Exploitation, Methodology, User Modeling, Music, Real-World Applications, A/B Testing, Recsplations, User Engagement (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/345/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Expediting Exploration by Attribute-to-Feature Mapping for Cold-Start Recommendations (2017)</h3>
    <p><strong>Authors:</strong> Michal Aharon, Deborah Cohen, Raz Nissim, Oren Somekh, Yair Koren</p>
    <p>The item cold-start problem is inherent to collaborative filtering (CF) recommenders where items and users are represented by vectors in a latent space. It emerges since CF recommenders rely solely on historical user interactions to characterize their item inventory. As a result, an effective serving of new and trendy items to users may be delayed until enough user feedback is received, thus, reducing both users’ and content suppliers’ satisfaction. To mitigate this problem, many commercial recommenders apply random exploration and devote a small portion of their traffic to explore new items and gather interactions from random users. Alternatively, content or context information is combined into the CF recommender, resulting in a hybrid system. Another hybrid approach is to learn a mapping between the item attribute space and the CF latent feature space, and use it to characterize the new items providing initial estimates for their latent vectors. In this paper, we adopt the attribute-to-feature mapping approach to expedite random exploration of new items and present LearnAROMA – an advanced algorithm for learning the mapping, previously proposed in the context of classification. In particular, LearnAROMA learns a Gaussian distribution over the mapping matrix. Numerical evaluation demonstrates that this learning technique achieves more accurate initial estimates than logistic regression methods. We then consider a random exploration setting, in which new items are further explored as user interactions arrive. To leverage the initial latent vector estimates with the incoming interactions, we propose DynamicBPR – an algorithm for updating the new item latent vectors without retraining the CF model. Numerical evaluation reveals that DynamicBPR achieves similar accuracy as a CF model trained on all the ratings, using 70% less exploring users than conventional random exploration.</p>
    <p><strong>Categories:</strong> Collaborative Filtering, Cold Start, Matrix Factorization, Hybrid Systems, Attribute-to-Feature Mapping, Exploration vs Exploitation, Algorithm Updates, Gaussian Distribution, Recommendation Accuracy, Random Exploration (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/257/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Clinical Online Recommendation with Subgroup Rank Feedback (2014)</h3>
    <p><strong>Authors:</strong> Yanan Sui, Joel Burdick</p>
    <p>Many real applications in experimental design need to make decisions online. Each decision leads to a stochastic reward with initially unknown distribution. New decisions are made based on the observations of previous rewards. To maximize the total reward, one needs to solve the tradeoff between exploring different strategies and exploiting currently optimal strategies. This kind of tradeoff problems can be formalized as Multi-armed bandit problem. We recommend strategies in series and generate new recommendations based on noisy rewards of previous strategies. When the reward for a strategy is difficult to quantify, classical bandit algorithms are no longer optimal. This paper, studies the Multi-armed bandit problem with feedback given as a stochastic rank list instead of quantified reward value. We propose an algorithm for this new problem and show its optimality. A real application of this algorithm on clinical treatment is helping paralyzed patient to regain the ability to stand on their own feet.</p>
    <p><strong>Categories:</strong> Multi-Armed Bandits, Healthcare, Clinical Treatment, Subgroup Rank Feedback, Recommendation Systems, Real-World Application, Algorithm Design, Medical Rehabilitation, Exploration vs Exploitation, Online Decision-Making (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/37/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>