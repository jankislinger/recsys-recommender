<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/explainability/">Explainability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reinforcement-learning/">Reinforcement Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/real-world-applications/">Real-world Applications</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/evaluation-methods/">Evaluation Methods</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/evaluation-metrics/">Evaluation Metrics</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Exploring False Hard Negative Sample in Cross-Domain Recommendation (2023)</h3>
    <p><strong>Authors:</strong> Haokai Ma, Xu Zhang, Lei Meng, Xin Chen, Jie Zhou, Leyu Lin, Ruobing Xie</p>
    <p>Negative Sampling in recommendation aims to capture informative negative instances for the sparse user-item interactions to improve the performance. Conventional negative sampling methods tend to select informative hard negative samples (HNS) besides the default random samples. However, these hard negative sampling methods usually struggle with false hard negative samples (FHNS), which happens when a user-item interaction has not been observed yet and is picked as a negative sample, while the user will actually interact with this item once exposed to it. Such FHNS issues may seriously confuse the model training, while most conventional hard negative sampling methods do not systematically explore and distinguish FHNS from HNS. To address this issue, we propose a novel model-agnostic Real Hard Negative Sampling (RealHNS) framework specially for cross-domain recommendation (CDR), which aims to discover the false and refine the real from all HNS via both general and cross-domain real hard negative sample selectors. For the general part, we conduct the coarse-grained and fine-grained real HNS selectors sequentially, armed with a dynamic item-based FHNS filter to find high-quality HNS. For the cross-domain part, we further design a new cross-domain HNS for alleviating negative transfer in CDR and discover its corresponding FHNS via a dynamic user-based FHNS filter to keep its power. We conduct experiments on four datasets based on three representative model-agnostic hard negative sampling methods, along with extensive model analyses, ablation studies, and universality analyses. The consistent improvements indicate the effectiveness, robustness, and universality of RealHNS, which is also easy-to-deploy in real-world systems as a plug-and-play strategy. The source code will be released in the future.</p>
    <p><strong>Categories:</strong> Negative Sampling, Hard Negative Samples (HNS), Cross-Domain Recommendation (CDR), Model-Agnostic Methods, False Hard Negative Samples (FHNS), Evaluation Methods, Applications, Data Filtering, Recommender Systems, Model Analysis (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/863/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Turning Dross Into Gold Loss: is BERT4Rec really better than SASRec? (2023)</h3>
    <p><strong>Authors:</strong> Alexey Vasilev, Anton Klenitskiy</p>
    <p>Recently sequential recommendations and next-item prediction task has become increasingly popular in the field of recommender systems. Currently, two state-of-the-art baselines are Transformer-based models SASRec and BERT4Rec. Over the past few years, there have been quite a few publications comparing these two algorithms and proposing new state-of-the-art models. In most of the publications, BERT4Rec achieves better performance than SASRec. But BERT4Rec uses cross-entropy over softmax for all items, while SASRec uses negative sampling and calculates binary cross-entropy loss for one positive and one negative item. In our work, we show that if both models are trained with the same loss, which is used by BERT4Rec, then SASRec will significantly outperform BERT4Rec both in terms of quality and training speed. In addition, we show that SASRec could be effectively trained with negative sampling and still outperform BERT4Rec, but the number of negative examples should be much larger than one.</p>
    <p><strong>Categories:</strong> Sequential Recommendations, Transformer-Based Models, BERT4Rec, SASRec, Algorithm Comparison, Loss Functions, Evaluation Metrics, State-of-the-Art Models, Training Efficiency, Model Performance, Recommender Systems, Negative Sampling, Cross-Entropy Loss, Binary Cross-Entropy Loss (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/966/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Scaling Session-Based Transformer Recommendations using Optimized Negative Sampling and Loss Functions (2023)</h3>
    <p><strong>Authors:</strong> Philipp Normann, Timo Wilm, Paul-Vincent Kobow, Sophie Baumeister</p>
    <p>This work introduces TRON, a scalable session-based Transformer Recommender using Optimized Negative-sampling. Motivated by the scalability and performance limitations of prevailing models such as SASRec and GRU4Rec+, TRON integrates top-k negative sampling and listwise loss functions to enhance its recommendation accuracy. Evaluations on relevant large-scale e-commerce datasets show that TRON improves upon the recommendation quality of current methods while maintaining training speeds similar to SASRec. A live A/B test yielded an 18.14% increase in click-through rate over SASRec, highlighting the potential of TRON in practical settings. For further research, we provide access to our source code and an anonymized dataset.</p>
    <p><strong>Categories:</strong> Transformer-Based Recommenders, Session-Based Recommendations, Negative Sampling, Listwise Loss Functions, E-commerce, Scalability, Recommender Systems Evaluation, A/B Testing, Real-World Applications, Training Efficiency, Recommendation Accuracy (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1007/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Augmented Negative Sampling for Collaborative Filtering (2023)</h3>
    <p><strong>Authors:</strong> Li Chen, Riwei Lai, Hongtao Song, Qilong Han, Rui Chen, Yuhan Zhao</p>
    <p>Negative sampling is essential for implicit-feedback-based collaborative filtering, which is used to constitute negative signals from massive unlabeled data to guide supervised learning. The state-of-the-art idea is to utilize hard negative samples that carry more useful information to form a better decision boundary. To balance efficiency and effectiveness, the vast majority of existing methods follow the two-pass approach, in which the first pass samples a fixed number of unobserved items by a simple static distribution and then the second pass selects the final negative items using a more sophisticated negative sampling strategy. However, selecting negative samples from the original items from a dataset is inherently limited due to the limited available choices, and thus may not be able to contrast positive samples well. In this paper, we confirm this observation via carefully designed experiments and introduce two major limitations of existing solutions: ambiguous trap and information discrimination. Our response to such limitations is to introduce “augmented” negative samples that may not exist in the original dataset. This direction renders a substantial technical challenge because constructing unconstrained negative samples may introduce excessive noise that eventually distorts the decision boundary. To this end, we introduce a novel generic augmented negative sampling (ANS) paradigm and provide a concrete instantiation. First, we disentangle the hard and easy factors of negative items. Next, we generate new candidate negative samples by augmenting only the easy factors in a regulated manner: the direction and magnitude of the augmentation are carefully calibrated. Finally, we design an advanced negative sampling strategy to identify the final augmented negative samples, which considers not only the score used in existing methods but also a new metric called augmentation gain. Extensive experiments on five real-world datasets demonstrate that our method significantly outperforms state-of-the-art baselines. Our code is publicly available at https://anonymous.4open.science/r/ANS-Recbole-B070/.</p>
    <p><strong>Categories:</strong> Collaborative Filtering, Negative Sampling, Augmented Data, Recommendation Systems, Real-World Applications, Algorithm Innovation, Experimental Design, Evaluation Metrics, Innovation in Metrics (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/854/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling (2023)</h3>
    <p><strong>Authors:</strong> Craig Macdonald, Aleksandr V. Petrov</p>
    <p>Large catalogue size is one of the central challenges in training recommendation models: a large number of items makes it infeasible to compute scores for all items during training, forcing models to deploy negative sampling. However, negative sampling increases the proportion of positive interactions in the training data. Therefore models trained with negative sampling tend to overestimate the probabilities of positive interactions — a phenomenon we call overconfidence. While the absolute values of the predicted scores/probabilities are unimportant for ranking retrieved recommendations, overconfident models may fail to estimate nuanced differences in the top-ranked items, resulting in degraded performance. This paper shows that overconfidence explains why the popular SASRec model underperforms when compared to BERT4Rec (contrary to the BERT4Rec authors’ attribution to the bi-directional attention mechanism). We propose a novel Generalised Binary Cross-Entropy Loss function (gBCE) to mitigate overconfidence and theoretically prove that it can mitigate overconfidence. We further propose the gSASRec model, an improvement over SASRec that deploys an increased number of negatives and gBCE loss. We show through detailed experiments on three datasets that gSASRec does not exhibit the overconfidence problem. As a result, gSASRec can outperform BERT4Rec (e.g.\ +9.47\% NDCG on MovieLens-1M), while requiring less training time (e.g.\ -73\% training time on MovieLens-1M). Moreover, in contrast to BERT4Rec, gSASRec is suitable for large datasets that contain more than 1 million items.</p>
    <p><strong>Categories:</strong> Sequential Recommendation, Negative Sampling, Cross-Entropy Loss, Overconfidence, Recommendation Systems, Movies, Diversity of Recommendations, Beyond Accuracy, Scalability, Training Efficiency (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/864/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Negative-Aware Collaborative Filtering (2019)</h3>
    <p><strong>Authors:</strong> Yu-Neng Chuang, Chuan-Ju Wang, Ming-Feng Tsai, Sheng-Fang Yang, Sheng-Chieh Lin</p>
    <p>Most traditional recommender systems regard unseen user-item associations as negative user preferences and optimize recommendation models mainly based on observed associations and some negative instances sampled from unseen associations. However, such unseen user-item associations may contain potential positive user preferences on items and are not uniformly distributed in terms of the possibility of being negative (or positive) user preference; therefore, it is essential to quantify such associations for model training. Along this line, in this paper, in contrast to existing recommendation models, which equally treat all unseen associations as negative samples, we present a negative-aware recommendation approach that explicitly models the likelihood of each unseen association being a potentially positive preference. Empirical results on real-world datasets in different fields show that our approach consistently improves recommendation performance.</p>
    <p><strong>Categories:</strong> Recommender Systems, Collaborative Filtering, Negative Sampling, Positive Preference Prediction, Probabilistic Modeling, Likelihood Estimation, Traditional Methods vs. New Approaches, Real-World Applications, Cross-Domain Analysis, Model Comparison, Statistical Modeling, Recommendation Performance Metrics, Handling Unobserved Data (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/524/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Relaxed Softmax for PU Learning (2019)</h3>
    <p><strong>Authors:</strong> Ugo Tanielian, Flavian Vasile</p>
    <p>In recent years, the softmax model and its fast approximations have become the de-facto loss functions for deep neural networks when dealing with multi-class prediction. This loss has been extended to language modeling and recommendation, two fields that fall into the framework of learning from Positive and Unlabeled data. In this paper, we stress the different drawbacks of the current family of softmax losses and sampling schemes when applied in a Positive and Unlabeled learning setup. We propose both a Relaxed Softmax loss (RS) and a new negative sampling scheme based on a Boltzmann formulation. We show that the new training objective is better suited for the tasks of density estimation, item similarity and next-event prediction by driving uplifts in performance on textual and recommendation datasets against classical softmax.</p>
    <p><strong>Categories:</strong> PU Learning, Softmax, Language Modeling, Recommendation Systems, Relaxed Softmax, Negative Sampling, Density Estimation, Item Similarity, Next-Event Prediction, Textual Data, Deep Neural Networks, Multi-Class Classification (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/455/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Towards Large Scale Training Of AutoEncoders For Collaborative Filtering (2018)</h3>
    <p><strong>Authors:</strong> Abdallah Moussawi</p>
    <p>In this paper, we apply a mini-batch based negative sampling method to efficiently train a latent factor autoencoder model on large scale and sparse data for implicit feedback collaborative filtering. We compare our work against a state-of-the-art baseline model on different experimental datasets and show that this method can lead to a good and fast approximation of the baseline model performance.</p>
    <p><strong>Categories:</strong> Collaborative Filtering, Autoencoders, Large Scale, Implicit Feedback, Mini-Batch Training, Negative Sampling, Performance Metrics, Latent Factor Models (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/412/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>