<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reinforcement-learning/">Reinforcement Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/diversity-of-recommendations/">Diversity of Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/explainability/">Explainability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Fairness Matters: A look at LLM-generated group recommendations (2024)</h3>
    <p><strong>Authors:</strong> Antonela Tommasel</p>
    <p>Recommender systems play a crucial role in how users consume information, with group recommendation receiving considerable attention. Ensuring fairness in group recommender systems entails providing recommendations that are useful and relevant to all group members rather than solely reflecting the majority’s preferences, while also addressing fairness concerns related to sensitive attributes (e.g., gender). Recently, the advancements on Large Language Models (LLMs) have enabled the development of new kinds of recommender systems. However, LLMs can perpetuate social biases present in training data, posing risks of unfair outcomes and harmful impacts. We investigated LLMs impact on group recommendation fairness, establishing and instantiating a framework that encompasses group definition, sensitive attribute combinations, and evaluation methodology. Our findings revealed the interactions patterns between sensitive attributes and LLMs and how they affected recommendation. This study advances the understanding of fairness considerations in group recommendation systems, laying the groundwork for future research.</p>
    <p><strong>Categories:</strong> Recommender Systems, Fairness, Group Recommendations, Large Language Models (LLMs), Sensitive Attributes, Bias Mitigation, Natural Language Processing (NLP), Evaluation Methodology, Social Biases, Societal Impact (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1089/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Fairness explanation in recommender systems (2024)</h3>
    <p><strong>Authors:</strong> Luan Souza</p>
    <p>Fairness in recommendations is an emerging area in recommender systems, aiming to mitigate discriminations against individuals or/and groups of individuals in recommendations. These mitigation strategies rely on statistical bias detection, which is a non-trivial task that requires complex analysis and interventions to ensure fairness in these engines. Furthermore, fairness interventions in recommender systems involve a tradeoff between fairness and performance of the recommendation lists, impacting the user experience with less accurate lists. In this context, fairness interventions with explanations have been proposed recently, mitigating discrimination in recommendation lists and providing explainability about the recommendation process and the impact of the fairness interventions. However, in spite of the different approaches it is still not clear how these proposals compare with each other, even those that propose to mitigate the same kind of bias. In addition, the contribution of these different explainable algorithmic fairness approaches to users’ fairness perceptions was not explored until the moment. Looking at these gaps, our doctorate project aims to investigate how these explainable fairness proposals compare to each other and how they are perceived by the users, in order to identify which fairness interventions and explanation strategies are most promising to increase transparency and fairness perceptions of recommendation lists.</p>
    <p><strong>Categories:</strong> Fairness, Explainability, Recommender Systems, Bias Mitigation, Tradeoff Between Fairness and Performance, User Perception of Fairness, Transparency, Emerging Areas, Research Project Overview (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1142/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Understanding Fairness in Recommender Systems: A Healthcare Perspective (2024)</h3>
    <p><strong>Authors:</strong> Veronica Kecki, Alan Said</p>
    <p>Fairness in AI-driven decision-making systems has become a critical concern, especially when these systems directly affect human lives. This paper explores the public’s comprehension of fairness in healthcare recommendations. We conducted a survey where participants selected from four fairness metrics – Demographic Parity, Equal Accuracy, Equalized Odds, and Positive Predictive Value – across different healthcare scenarios to assess their understanding of these concepts. Our findings reveal that fairness is a complex and often misunderstood concept, with a generally low level of public understanding regarding fairness metrics in recommender systems. This study highlights the need for enhanced information and education on algorithmic fairness to support informed decision-making in using these systems. Furthermore, the results suggest that a one-size-fits-all approach to fairness may be insufficient, pointing to the importance of context-sensitive designs in developing equitable AI systems.</p>
    <p><strong>Categories:</strong> Fairness, Recommender Systems, Healthcare, Ethics in AI, Public Understanding, Algorithmic Fairness, Education, Context-aware Systems, Healthcare Applications, User Survey (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1201/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Oh, Behave! Country Representation Dynamics Created by Feedback Loops in Music Recommender Systems (2024)</h3>
    <p><strong>Authors:</strong> Dominik Kowald, Markus Schedl, Oleg Lesota, Jonas Geiger, Max Walder</p>
    <p>Recent work suggests that music recommender systems are prone to disproportionally frequent recommendations of music from countries more prominently represented in the training data, notably the US. However, it remains unclear to what extent feedback loops in music recommendation influence the dynamics of such imbalance. In this work, we investigate the dynamics of representation of local (i.e., country-specific) and US-produced music in user profiles and recommendations. To this end, we conduct a feedback loop simulation study using the standardized LFM-2b dataset. The results suggest that most of the investigated recommendation models decrease the proportion of music from local artists in their recommendations. Furthermore, we find that models preserving average proportions of US and local music do not necessarily provide country-calibrated recommendations. We also look into popularity calibration and, surprisingly, find that the most popularity-calibrated model in our study (ItemKNN) provides the least country-calibrated recommendations. In addition, users from less represented countries (e.g., Finland) are, in the long term, most affected by the under-representation of their local music in recommendations.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Music Recommender, Country Representation, User Behavior, Feedback Loops, Bias, Fairness, Algorithmic Bias, Empirical Study, Real World Applications, Cultural Dynamics, Calibration, Evaluation Metrics, Performance Analysis, User Impact (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1099/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Promoting Two-sided Fairness with Adaptive Weights for Providers and Customers in Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Wayne Xin Zhao, Ji-Rong Wen, Lanling Xu, Sheng Chen, Zihan Lin, Jinpeng Wang</p>
    <p>At present, most recommender systems involve two stakeholders, providers and customers. Apart from maximizing the recommendation accuracy, the fairness issue for both sides should also be considered. However, there is a trade-off for multi-objective optimization problems, where optimizing one objective (e.g. provider-side fairness) may degrade the performance of others (e.g. accuracy). Most of previous studies try to improve two-sided fairness with post-processing algorithms or fairness-aware loss constraints, which are highly dependent on the heuristic adjustments without respect to the optimization goal of accuracy. In contrast, we propose a novel training framework, adaptive weighting  towards two-sided fairness-aware recommendation (named Ada2Fair), which lies in the extension of the accuracy-focused objective to a controllable preference learning loss over the interaction data. Specifically, we adjust the optimization scale of an interaction sample with an adaptive weight generator, and estimate the two-sided fairness-aware weights within model training. During the training process, the recommender is trained with two-sided fairness-aware weights to boost the utility of niche providers and inactive customers in a unified way. Extensive experiments on three public datasets verify the effectiveness of Ada2Fair, which can achieve Pareto improvements in two-sided fairness-aware recommendation. Our code implementation is available at https://anonymous.4open.science/r/Ada2Fair.</p>
    <p><strong>Categories:</strong> Adaptive Weights, Recommender Systems, Fairness, Two-sided Fairness, Multi-objective Optimization, Recommendation Frameworks, Provider-Customer Dynamics, User-Centric Design, Real-world Applications, Adaptive Learning, Evaluation Metrics (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1112/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Fairness and Transparency in Music Recommender Systems: Improvements for Artists (2024)</h3>
    <p><strong>Authors:</strong> Karlijn Dinnissen</p>
    <p>Music streaming services have become one of the main sources of music consumption in the last decade, with recommender systems as an important component. As those systems partially decide the songs that music consumers listen to, the systems greatly impact the artists who created the songs. However, when evaluating performance and fairness of these music recommending systems (MRSs), the perspective of the item providers or other music industry professionals is often not considered. Additionally, artists indicate they would appreciate more transparency – both towards and users and the artists themselves – regarding why certain items are recommended and others are not. This research project takes a multi-stakeholder approach to bridge the gap between music systems and their item providers. We first establish artists’ and music industry professionals’ perspective on MRSs through interviews and questionnaires. Based on those insights, we then aim to increase matching between end users and lesser-known artists by generating rich item and user representations. Results will be evaluated both quantitatively and qualitatively. Lastly, we plan to effectively communicate MRS fairness by increasing transparency for both end users and artists.</p>
    <p><strong>Categories:</strong> Fairness, Transparency, Music Recommender Systems (MRS), Artist Perspective, Stakeholder Approach, Evaluation Methods, Recommendation Algorithms, Representation Learning, Multi-Stakeholder Systems, User-Centric Design, Algorithmic Transparency, Diversity in Recommendations (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1136/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>AMBAR: A dataset for Assessing Multiple Beyond-Accuracy Recommenders (2024)</h3>
    <p><strong>Authors:</strong> David Contreras, Ludovico Boratto, Elizabeth Gómez, Maria Salamo</p>
    <p>Nowadays a recommendation model should exploit additional information from both the user and item perspectives, in addition to utilizing user-item interaction data. Datasets are central in offering the required information for evaluating new models or algorithms. Although there are many datasets in the literature with user and item properties, there are several issues not covered yet: (i) it is difficult to perform cross-analysis of properties at user and item level as they are not related in most cases; and (ii) on top of that, in many occasions datasets do not allow analysis at different granularity levels. In this paper, we propose a new dataset in the music domain, named AMBAR, that includes the above-mentioned issues. Besides detailing in depth the structure of the new dataset, we also show its application in contexts (i.e., multi-objective, fair, and calibrated recommendations) where both the effectiveness and the beyond-accuracy perspectives of recommendation are assessed.</p>
    <p><strong>Categories:</strong> Beyond Accuracy, Recommendation Systems, Dataset Construction, Music Domain, Multi-Objective Optimization, Fairness, Model Evaluation, Data Granularity, Ethics in ML, Evaluation Metrics. (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1105/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Fair Augmentation for Graph Collaborative Filtering (2024)</h3>
    <p><strong>Authors:</strong> Gianni Fenu, Ludovico Boratto, Giacomo Medda, Mirko Marras, Francesco Fabbri</p>
    <p>Recent developments in recommendation have harnessed the collaborative power of graph neural networks (GNNs) in learning users’ preferences from user-item networks. Despite emerging regulations addressing fairness of automated systems, unfairness issues in graph collaborative filtering remain underexplored, especially from the consumer’s perspective. While the recommendation literature has seen numerous contributions in the form of mitigation algorithms and comprehensive evaluation studies on consumer unfairness, only a few of these works have delved into GNNs. A notable gap exists in the formalization of the latest mitigation algorithms, as well as in their effectiveness and reliability on cutting-edge models. In this paper, we conduct an extensive analysis of one of the latest mitigation methods tailored for consumer fairness in GNN-based recommendation. The reproduced technique adjusts the system fairness level by learning a fair graph augmentation. Our study serves as a solid response to recent research highlighting unfairness issues in graph collaborative filtering. Under an experimental setup based on 11 GNNs, 5 non-GNN models, and 5 real-world networks across diverse domains, our investigation reveals that fair graph augmentation is consistently effective on high-utility models and large datasets. Experiments on the transferability of the fair augmented graph open new issues for future recommendation~studies.</p>
    <p><strong>Categories:</strong> Fairness, Graph Collaborative Filtering, Algorithm Family - GNNs, Mitigation Algorithms, Evaluation Studies, Consumer Unfairness, Recommendation Systems, Real-World Applications, Model Transferability (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1122/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Enhancing Privacy in Recommender Systems through Differential Privacy Techniques (2024)</h3>
    <p><strong>Authors:</strong> Angela Di Fazio</p>
    <p>Recommender systems have become essential tools for addressing information overload in the digital age. However, the collection and usage of user data for personalized recommendations raise significant privacy concerns. This research focuses on enhancing privacy in recommender systems through the application of differential privacy techniques, particularly in the domain of privacy-preserving data publishing. Our study aims to address three key research questions: (1) developing standardized metrics to characterize and compare recommendation datasets in the context of privacy-preserving data publishing, (2) designing differential privacy algorithms for private data publishing that preserve recommendation quality, and (3) examining the impact of differential privacy on beyond-accuracy objectives in recommender systems. We propose to develop domain-specific metrics for evaluating the similarity between recommendation datasets, analogous to those used in other domains such as trajectory data publication. Additionally, we will investigate methods to balance the trade-off between privacy guarantees and recommendation accuracy, considering the potential disparate impacts on different user subgroups. Finally, we aim to assess the broader implications of implementing differential privacy on beyond-accuracy objectives such as diversity, popularity bias, and fairness. By addressing these challenges, our research seeks to contribute to the advancement of privacy-preserving techniques in recommender systems, facilitating the responsible and secure use of recommendation data while maintaining the utility of personalized suggestions. The outcomes of this study have the potential to significantly benefit the field by enabling the reuse of existing algorithms with minimal adjustments while ensuring robust privacy guarantees.</p>
    <p><strong>Categories:</strong> Privacy Preservation, Differential Privacy, Recommender Systems, Metrics Evaluation, Beyond Accuracy Objectives, Domain-Specific Metrics, Privacy-Preserving Data Publishing, Trade-Off Analysis, User Subgroup Impacts, Diversity, Popularity Bias, Fairness, Responsible Data Use (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1139/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Societal Sorting as a Systemic Risk of Recommenders (2024)</h3>
    <p><strong>Authors:</strong> Carmine Ventre, Luke Thorburn, Maria Polukarov</p>
    <p>Political scientists distinguish between polarization (loosely, people moving further apart along a single dimension) and sorting (an increase in the probabilistic dependence between multiple dimensions of individual difference). Among other harms, sorting can increase the risk of conflict escalation by reinforcing us-and-them group identities and reducing the prevalence of cross-cutting affiliations. In this paper, we (i) review normative arguments for high or low sortedness, (ii) summarize the mechanisms by which sortedness can change, and (iii) show that under a simple model of social media recommender-driven preference change, personalized engagement-based ranking creates a systematic tendency towards sorting, while ranking by diverse engagement (sometimes called “bridging-based ranking”) mitigates this tendency. We conclude by considering the implications for those conducting systemic risk assessments of very large online platforms under the EU Digital Services Act.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Social Networks, Polarization, Fairness, Trust &amp; Ethics, Recommendation Mechanisms, Regulatory Frameworks, Systemic Risk Assessment, Conflict Analysis, Personalized Ranking, Diversity in Recommendations, Engagement-Based Ranking, Assessing Systemic Risks (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1111/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>When Fairness meets Bias: a Debiased Framework for Fairness aware Top-N Recommendation (2023)</h3>
    <p><strong>Authors:</strong> Zhi Gong, Jingsen Zhang, Jiakai Tang, Xu Chen, Shiqi Shen, Zhipeng Wang</p>
    <p>Fairness in the recommendation domain has recently attracted increasing attention due to the more and more concerns on the algorithm discrimination and ethics. While recent years have witnessed many promising fairness aware recommender models, an important problem has been largely ignored, that is, the fairness can be biased due to the user personalized selection tendencies or the non-uniform item exposure probabilities. To study this problem, in this paper, we formally define a novel task named as unbiased fairness aware Top-N recommendation. For solving this task, we firstly define an ideal loss function based on all the user-item pairs. Considering that, in real-world datasets, only a small number of user-item interactions can be observed, we then approximate the above ideal loss with a more tractable objective based on the inverse propensity score (IPS). Since the recommendation datasets can be noisy and quite sparse, which brings difficulties for accurately estimating the IPS, we propose to optimize the objective in an IPS range instead of a specific point, which improve the model fault tolerance capability. In order to make our model more applicable to the commonly studied Top-N recommendation, we soften the ranking metrics such as Precision, Hit-Ratio and NDCG to derive an fully differentiable framework. We conduct extensive experiments to demonstrate the effectiveness of our model based on four real-world datasets.</p>
    <p><strong>Categories:</strong> Bias Mitigation, Fairness, Recommendation Systems, Top-N Recommendation, Inverse Propensity Score (IPS), User Preferences, Bias-Variance Tradeoff, Social Aspects of Recommendation, Evaluation Metrics, Framework Proposal, Novel Methodology (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/892/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Knowledge-Aware Recommender Systems based on Multi-Modal Information Sources (2023)</h3>
    <p><strong>Authors:</strong> Giuseppe Spillo</p>
    <p>The last few years saw a growing interest in Knowledge-Aware Recommender Systems (KARSs), given their capability in encoding and exploiting several data sources, both structured (such as <i>knowledge graphs</i>) and unstructured (such as plain text); indeed, several pieces of research show the competitiveness of these models. Nowadays, a lot of models at the state-of-the-art in KARSs use deep learning, enabling them to exploit large amounts of information, including knowledge graphs (KGs), user reviews, plain text, and multimedia content (pictures, audio, videos). In my Ph.D. I will explore and study techniques for designing KARSs leveraging embeddings deriving from multi-modal information sources; the models I will design will aim at providing fair, accurate, and explainable recommendations.</p>
    <p><strong>Categories:</strong> Knowledge Graphs, Deep Learning, Recommender Systems, Structured Data, Unstructured Data, Multi-Modal Information Sources, Embeddings, Fairness, Explainability, Academic Research, Real-World Applications, Multi-Source Information (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/985/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Fast and Examination-agnostic Reciprocal Recommendation in Matching Markets (2023)</h3>
    <p><strong>Authors:</strong> Yoji Tomita, Riku Togashi, Yuriko Hashizume, Naoto Ohsaka</p>
    <p>n matching markets such as job posting and online dating platforms, the recommender system plays a critical role in the success of the platform. Unlike standard recommender systems that suggest items to users, reciprocal recommender systems (RRSs) that suggest other users must take into account the mutual interests of users. In addition, ensuring that recommendation opportunities do not disproportionately favor popular users is essential for the total number of matches and for fairness among users. Existing recommendation methods in matching markets, however, face computational challenges on large-scale platforms and depend on specific examination functions in the position-based model (PBM). In this paper, we introduce the reciprocal recommendation method based on the matching with transferable utility (TU matching) model in the context of ranking recommendations in matching markets and propose a fast and examination-model-free algorithm. Furthermore, we evaluate our approach on experiments with synthetic data and real-world data from an online dating platform in Japan. Our method performs better than or as well as existing methods in terms of the number of total matches and works well even in a large-scale dataset for which one existing method does not work.</p>
    <p><strong>Categories:</strong> TU Matching Model, Reciprocal Recommendation, Fairness, Scalability, Real-World Applications, Algorithmic Innovation, Beyond Accuracy, Matching Markets, Recommendation Systems, Balance (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/859/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Collaborative filtering algorithms are prone to mainstream-taste bias (2023)</h3>
    <p><strong>Authors:</strong> Philipp Hager, Pantelis Analytis</p>
    <p>Collaborative filtering has been the main steam engine of the recommender systems community since the early 1990s. Collaborative filtering (and other) algorithms, however, have been predominantly evaluated by aggregating results across users or user groups. These performance averages hide large disparities: an algorithm may perform very well for some users (or groups) and very poorly for others. We show that performance variation is large and systematic. In experiments on three large scale datasets and using an array of collaborative filtering algorithms, we demonstrate the large performance disparities for different users across algorithms and datasets. We then show that performance variation is systematic and that two key features that characterize users, their mean taste similarity with other users and the dispersion in taste similarity, can explain performance variation better than previously identified features. We use these two features to visualize algorithm performance for different users, and point out that this mapping can be used to capture different categories of users that have been proposed before. Our results demonstrate an extensive mainstream-taste bias in all collaborative filtering algorithms, and they imply a fundamental fairness limitation that needs to be mitigated.</p>
    <p><strong>Categories:</strong> Collaborative Filtering, Fairness, Mainstream Bias, Algorithm Limitations, User Performance Variation, Taste Similarity, Evaluation Methods, Recommender Systems, Large Scale Datasets, Diversity of Recommendations, Visualization Techniques, User Centric Design (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/906/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Towards Health-Aware Fairness in Food Recipe Recommendation (2023)</h3>
    <p><strong>Authors:</strong> Mehrdad Rostami, Mourad Oussalah, Mohammad Aliannejadi</p>
    <p>Food recommendation systems play a crucial role in suggesting personalized recommendations designed to help users find food and recipes that align with their preferences. However, many existing food recommendation systems have overlooked the important aspect of considering the health and nutritional value of recommended foods, thereby limiting their effectiveness in generating truly healthy recommendations. Our preliminary analysis indicates that users tend to respond positively to unhealthy food and recipes. As a result, existing food recommender systems that neglect health considerations often assign high scores to popular items, inadvertently encouraging unhealthy choices among users. In this study, we propose the development of a fairness-based model that prioritizes health considerations. Our model incorporates fairness constraints from both the user and item perspectives, integrating them into a joint objective framework. Experimental results conducted on real-world food datasets demonstrate that the proposed system not only maintains the ability of food recommendation systems to suggest users’ favorite foods but also improves the health factor compared to unfair models, with an average enhancement of approximately 35%.</p>
    <p><strong>Categories:</strong> Fairness, Health Awareness, Food Recommendation Systems, Personalization, Nutrition, Equity, Optimization, Real-World Applications, Evaluation Aspects, Diversity of Recommendations, Beyond Accuracy, Culinary Domain, Algorithmic Approaches, Machine Learning (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/960/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Group Fairness for Content Creators: the Role of Human and Algorithmic Biases under Popularity-based Recommendations (2023)</h3>
    <p><strong>Authors:</strong> Nicolo Pagan, Stefania Ionescu, Aniko Hannak</p>
    <p>The Creator Economy faces concerning levels of unfairness. Content creators (CCs) publicly accuse platforms of purposefully reducing the visibility of their content based on protected attributes, while platforms place the blame on viewer biases. Meanwhile, prior work warns about the “rich-get-richer”  effect perpetuated by existing popularity biases in recommender systems: Any initial advantage in visibility will likely be exacerbated over time. What remains unclear is how the biases based on protected attributes from platforms and viewers interact and contribute to the observed inequality in the context of popularity-biased recommender systems. The difficulty of the question lies in the complexity and opacity of the system. To overcome this challenge, we create a simple agent-based model (ABM) that unifies the platform systems which allocate the visibility of CCs (e.g., recommender systems, moderation) into a single popularity-based function, which we call the visibility allocation system (VAS). Through simulations, we find that although viewer homophilic biases do alone create inequalities, small levels of additional biases in VAS are more harmful. From the perspective of interventions, our results suggest that (a) attempts to reduce attribute-biases in moderation and recommendations should precede those reducing viewer homophilic tendencies, (b) decreasing the popularity-biases in VAS decreases but not eliminates inequalities, (c) boosting the visibility of protected CCs to overcome viewer homophily with respect to one metric is unlikely to produce fair outcomes with respect to all metrics, and (d) the process is also unfair for viewers and this unfairness could be overcome through the same interventions. More generally, this work demonstrates the potential of using ABMs to better understand the causes and effects of biases and interventions within complex sociotechnical systems.</p>
    <p><strong>Categories:</strong> Fairness, Bias, Recommendation Systems, Algorithmic Fairness, Creator Economy, Agent-Based Modeling (ABM), Sociotechnical Systems, Interventions in Recommendation Systems, Human Biases, Algorithmic Biases, Complexity of Systems, Group Fairness (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/915/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>