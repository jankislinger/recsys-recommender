<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">Interpretability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/explainability/">Explainability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/beyond-accuracy/">Beyond Accuracy</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Explainable Graph Neural Network Recommenders; Challenges and Opportunities (2023)</h3>
    <p><strong>Authors:</strong> Amir Reza Mohammadi</p>
    <p>Graph Neural Networks (GNNs) have demonstrated significant potential in recommendation tasks by effectively capturing intricate connections among users, items, and their associated features. Given the escalating demand for interpretability, current research endeavors in the domain of GNNs for Recommender Systems (RecSys) necessitate the development of explainer methodologies to elucidate the decision-making process underlying GNN-based recommendations. In this work, we aim to present our research focused on techniques to extend beyond the existing approaches for addressing interpretability in GNN-based RecSys.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Graph Neural Networks (GNNs), Explainable AI (XAI), Interpretability, Research Review/Analysis, Methodological Extensions, User-Item Interaction, Trust in AI/Recommenders, Machine Learning, Challenges &amp; Opportunities, Beyond Accuracy (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/980/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Matrix Factorization for Collaborative Filtering Is Just Solving an Adjoint Latent Dirichlet Allocation Model After All (2021)</h3>
    <p><strong>Authors:</strong> Florian Wilhelm</p>
    <p>Matrix factorization-based methods are among the most popular methods for collaborative filtering tasks with implicit feedback. The most effective of these methods do not apply sign constraints, such as non-negativity, to their factors. Despite their simplicity, the latent factors for users and items lack interpretability, which is becoming an increasingly important requirement. In this work, we provide a theoretical link between unconstrained and the interpretable non-negative matrix factorization in terms of the personalized ranking induced by these methods. We also introduce a novel, latent Dirichlet allocation-inspired model for recommenders and extend our theoretical link to also allow the interpretation of an unconstrained matrix factorization as an adjoint formulation of our new model. Our experiments indicate that this novel approach represents the unknown processes of implicit user-item interactions in the real world much better than unconstrained matrix factorization while being interpretable.</p>
    <p><strong>Categories:</strong> Matrix Factorization, Collaborative Filtering, Implicit Feedback, Latent Dirichlet Allocation, Interpretability, Recommendation Systems, Theoretical Insights, User-Item Interactions, Adjoint Formulation (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/647/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Style Conditioned Recommendations (2019)</h3>
    <p><strong>Authors:</strong> Kamelia Aryafar, Murium Iqbal, Timothy Anderton</p>
    <p>We propose Style Conditioned Recommendations (SCR) and introduce style injection as a method to diversify recommendations. We use Conditional Variational Autoencoder (CVAE) architecture, where both the encoder and decoder are conditioned on a user profile learned from item content data. This allows us to apply style transfer methodologies to the task of recommendations, which we refer to as injection. To enable style injection, user profiles are learned to be interpretable such that they express users’ propensities for specific predefined styles. These are learned via label-propagation from a dataset of item content, with limited labeled points. To perform injection, the condition on the encoder is learned while the condition on the decoder is selected per explicit feedback. Explicit feedback can be taken either from a user’s response to a style or interest quiz, or from item ratings. In the absence of explicit feedback, the condition at the encoder is applied to the decoder. We show a 12% improvement on NDCG@20 over the traditional VAE based approach on the task of recommendations. We show an average 22% improvement on AUC across all classes for predicting user style profiles against our best performing baseline. After injecting styles we compare the user style profile to the style of the recommendations and show that injected styles have an average +133% increase in presence. Our results show that style injection is a powerful method to diversify recommendations while maintaining personal relevance. Our main contribution is an application of a semi-supervised approach that extends item labels to interpretable user profiles.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Variational Autoencoders, Diversification of Recommendations, Explicit Feedback, Content-Based Filtering, Semi-Supervised Learning, Evaluation of Recommendation Systems, Personalization, Beyond Accuracy, Style/Theme, Interpretability, Diversity of Recommendations, Conditional Generative Models, Performance Metrics, User Feedback Mechanisms, Novelty in Recommendations (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/453/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Content-based Feature Exploration for Transparent Music Recommendation using Self-attentive Genre Classification (2018)</h3>
    <p><strong>Authors:</strong> Juheon Lee, Kyogu Lee, Seungjin Lee</p>
    <p>Interpretation of retrieved results is an important issue in music recommender systems, particularly from a user perspective. In this study, we investigate the methods for providing interpretability of content features using self-attention. We extract lyric features with the self-attentive genre classification model trained on 140,000 tracks of lyrics. Likewise, we extract acoustic features using the acoustic model with self-attention trained on 120,000 tracks of acoustic signals. The experimental results show that the proposed methods provide the characteristics that are interpretable in terms of both lyrical and musical contents. We demonstrate this by visualizing the attention weights, and by presenting the most similar songs found using lyric or audio features.</p>
    <p><strong>Categories:</strong> Music Recommendations, Content-Based Recommendations, Deep Learning, Explainable AI, Interpretability, Scalability, User-Centric Design, Model Interpretation, Feature Extraction, Self-attention, Genre Classification, Beyond Accuracy, Transparency, Lyric Analysis, Acoustic Features (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/415/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Explanation Chains: Recommendations by Explanation (2017)</h3>
    <p><strong>Authors:</strong> Derek Bridge, Arpit Rana</p>
    <p>Given a set of candidate items, Recommendation by Explanation constructs a justification for recommending each item, in the form of what we call an Explanation Chain, and then recommends those candidates that have the best explanations. By unifying recommendation and explanation, this approach enables us to find relevant recommendations with explanations that have a high degree of both fidelity and interpretability. Experimental results on a movie recommendation dataset show that our approach also provides sets of recommendations that have a high degree of serendipity, low popularity-bias and high diversity.</p>
    <p><strong>Categories:</strong> Explanation-Based Recommendations, Movies, Fidelity, Interpretability, Serendipity, Popularity Bias, Diversity, Explanation Chains, Beyond Accuracy, Recommendation by Explanation (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/300/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Interpretable Convolutional Neural Networks with Dual Local and Global Attention for Review Rating Prediction (2017)</h3>
    <p><strong>Authors:</strong> Jing Huang, Sungyong Seo, Hao Yang, Yan Liu</p>
    <p>Recently, many e-commerce websites have encouraged their users to rate shopping items and write review text. This review text information has been very useful for understanding user preferences and item properties and it enhances the capability to make personalized recommendations of these websites. In this paper, we propose to model user preferences and item properties using convolutional neural networks (CNNs) with dual local and global attention, motivated by the superiority of CNNs to extract complex features. By using aggregated review text from a user and aggregated review text for an item, our model can learn the unique features (embedding) of each user and each item. These features are then used to predict ratings. We train these user and item networks jointly and this enables the interaction between users and items in a similar way to matrix factorization. The local attention gives us insight on a user’s preferences or an item’s properties. The global attention helps CNNs focus on semantic meanings of the whole review text. Thus, the combined local and global attentions enable an interpretable and better-learned representation of users and items. We validate the proposed models by applying popular review datasets in Yelp and Amazon and compare the results with matrix factorization (MF), the hidden factors as topics (HFT) model, and the recently proposed convolutional matrix factorization (ConvMF+). The proposed CNNs with dual attention model outperforms HFT and ConvMF+ in terms of mean square errors (MSE). In addition, we compare the user/item embeddings learned from these models for classification and recommendation. These results also confirm the superior quality of user/item embeddings learned from our model.</p>
    <p><strong>Categories:</strong> Convolutional Neural Networks, Attention Mechanisms, Matrix Factorization, Review Analysis, Sentiment Analysis, E-commerce, Interpretability, Feature Extraction, Rating Prediction, Real World Application, Recommendation Systems, User Experience (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/268/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Using Explainability for Constrained Matrix Factorization (2017)</h3>
    <p><strong>Authors:</strong> Olfa Nasraoui, Behnoush Abdollahi</p>
    <p>Accurate model-based Collaborative Filtering (CF) approaches tend to be black-box machine learning models, such as Matrix Factorization (MF), that lack interpretability and do not provide a straightforward explanation for their outputs. Yet explanations can improve the transparency of a recommender system by justifying recommendations, and this in turn can enhance the user’s trust in the recommendations. Hence, one main challenge in designing a recommender system is mitigating the trade-off between an explainable technique with moderate prediction accuracy and a more accurate technique with no explainable recommendations. In this paper, we focus on MF and further assume the absence of any additional data source, such as item content or user attributes. We propose an explainability constrained MF technique that computes the top-n recommendation list from items that are explainable. Experimental results show that our method is effective in generating accurate and explainable recommendations.</p>
    <p><strong>Categories:</strong> Matrix Factorization, Collaborative Filtering, Explainable AI, Transparency, Trust, User Trust, Recommendation Systems, Algorithmic Transparency, Interpretability, Evaluation Metrics, Performance Analysis (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/309/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Towards Understanding Latent Factors and User Profiles by Enhancing Matrix Factorization with Tags (2016)</h3>
    <p><strong>Authors:</strong> Jürgen Ziegler, Tim Donkers, Benedikt Loepp</p>
    <p>With the interactive recommending approach we have recently proposed, users are given more control over model-based Collaborative Filtering while the results are perceived as more transparent. Integrating the latent factors derived by Matrix Factorization with tags users provided for the items has, however, even more advantages. In this paper, we show how general understanding of the abstract factor space, and of user and item positions inside it, can benefit from the semantics introduced by considering additional information. Moreover, our approach allows us to explain the user’s (former latent) preference profile by means of tags.</p>
    <p><strong>Categories:</strong> Matrix Factorization, Collaborative Filtering, User Profiles, Latent Factors, Tags, Transparency, Interpretability, Enhanced Recommendations, User Control, User Preferences (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/245/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Factored MDPs for Detecting Topics of User Sessions (2014)</h3>
    <p><strong>Authors:</strong> Maryam Tavakol, Ulf Brefeld</p>
    <p>Recommender systems aim to capture interests of users to provide tailored recommendations. User interests are however often unique and depend on many unobservable factors including a user’s mood and the local weather. We take a contextual session-based approach and propose a sequential framework using factored Markov decision processes (fMDPs) to detect the user’s goal (the topic) of a session. We show that an independence assumption on the attributes of items leads to a set of independent models that can be optimised efficiently. Our approach results in interpretable topics that can be effectively turned into recommendations. Empirical results on a real world click log from a large e-commerce company exhibit highly accurate topic prediction rates of about 90%. Translating our approach into a topic-driven recommender system outperforms collaborative filtering methods by one order of magnitude.</p>
    <p><strong>Categories:</strong> Recommender Systems, Markov Decision Processes, Session-Based Recommendations, Contextual Bandits, User Modeling, Real World Applications, Topic Detection, A/B Test, Collaborative Filtering, Interpretability, Contextual Recommendations, Empirical Evaluation. (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/17/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>