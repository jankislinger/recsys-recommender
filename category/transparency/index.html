<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">Transparency</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/explainability/">Explainability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/real-world-applications/">Real-world Applications</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Explainability in Music Recommender System (2024)</h3>
    <p><strong>Authors:</strong> Shahrzad Shashaani</p>
    <p>Recommendation systems play a crucial role in our daily lives, influencing many of our significant and minor decisions. These systems also have become integral to the music industry, guiding users to discover new content based on their tastes. However, the lack of transparency in these systems often leaves users questioning the rationale behind recommendations. To address this issue, adding transparency and explainability to recommender systems is a promising solution. Enhancing the explainability of these systems can significantly improve user trust and satisfaction. This research focuses on exploring transparency and explainability in the context of recommendation systems, focusing on the music domain. This research can help to understand the gaps in explainability in music recommender systems to create more engaging and trustworthy music recommendations.</p>
    <p><strong>Categories:</strong> Music Recommendations, Explainability, Transparency, User Trust, Recommendation Systems, Human-Computer Interaction, Algorithmic Transparency, Ethical Considerations, Case Study, Music Industry Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1132/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Fairness explanation in recommender systems (2024)</h3>
    <p><strong>Authors:</strong> Luan Souza</p>
    <p>Fairness in recommendations is an emerging area in recommender systems, aiming to mitigate discriminations against individuals or/and groups of individuals in recommendations. These mitigation strategies rely on statistical bias detection, which is a non-trivial task that requires complex analysis and interventions to ensure fairness in these engines. Furthermore, fairness interventions in recommender systems involve a tradeoff between fairness and performance of the recommendation lists, impacting the user experience with less accurate lists. In this context, fairness interventions with explanations have been proposed recently, mitigating discrimination in recommendation lists and providing explainability about the recommendation process and the impact of the fairness interventions. However, in spite of the different approaches it is still not clear how these proposals compare with each other, even those that propose to mitigate the same kind of bias. In addition, the contribution of these different explainable algorithmic fairness approaches to users’ fairness perceptions was not explored until the moment. Looking at these gaps, our doctorate project aims to investigate how these explainable fairness proposals compare to each other and how they are perceived by the users, in order to identify which fairness interventions and explanation strategies are most promising to increase transparency and fairness perceptions of recommendation lists.</p>
    <p><strong>Categories:</strong> Fairness, Explainability, Recommender Systems, Bias Mitigation, Tradeoff Between Fairness and Performance, User Perception of Fairness, Transparency, Emerging Areas, Research Project Overview (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1142/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Explainable Multi-Stakeholder Job Recommender Systems (2024)</h3>
    <p><strong>Authors:</strong> Roan Schellingerhout</p>
    <p>Public opinion on recommender systems has become increasingly wary in recent years. In line with this trend, lawmakers have also started to become more critical of such systems, resulting in the introduction of new laws focusing on aspects such as privacy, fairness, and explainability for recommender systems and AI at large. These concepts are especially crucial in high-risk domains such as recruitment. In recruitment specifically, decisions carry substantial weight, as the outcomes can significantly impact individuals’ careers and companies’ success. Additionally, there is a need for a multi-stakeholder approach, as these systems are used by job seekers, recruiters, and companies simultaneously, each with its own requirements and expectations. In this paper, I summarize my current research on the topic of explainable, multi-stakeholder job recommender systems and set out a number of future research directions.</p>
    <p><strong>Categories:</strong> Transparency, Legal Frameworks, Domain: Recruitment, Explainability, Multi-Stakeholder Systems, Job Recommendations, Fairness and Bias, Future Directions, Human Resources (HR) (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1133/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Fairness and Transparency in Music Recommender Systems: Improvements for Artists (2024)</h3>
    <p><strong>Authors:</strong> Karlijn Dinnissen</p>
    <p>Music streaming services have become one of the main sources of music consumption in the last decade, with recommender systems as an important component. As those systems partially decide the songs that music consumers listen to, the systems greatly impact the artists who created the songs. However, when evaluating performance and fairness of these music recommending systems (MRSs), the perspective of the item providers or other music industry professionals is often not considered. Additionally, artists indicate they would appreciate more transparency – both towards and users and the artists themselves – regarding why certain items are recommended and others are not. This research project takes a multi-stakeholder approach to bridge the gap between music systems and their item providers. We first establish artists’ and music industry professionals’ perspective on MRSs through interviews and questionnaires. Based on those insights, we then aim to increase matching between end users and lesser-known artists by generating rich item and user representations. Results will be evaluated both quantitatively and qualitatively. Lastly, we plan to effectively communicate MRS fairness by increasing transparency for both end users and artists.</p>
    <p><strong>Categories:</strong> Fairness, Transparency, Music Recommender Systems (MRS), Artist Perspective, Stakeholder Approach, Evaluation Methods, Recommendation Algorithms, Representation Learning, Multi-Stakeholder Systems, User-Centric Design, Algorithmic Transparency, Diversity in Recommendations (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1136/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>CEERS: Counterfactual Evaluations of Explanations in Recommender Systems (2024)</h3>
    <p><strong>Authors:</strong> Mikhail Baklanov</p>
    <p>The growing emphasis on explainability in ethical AI, driven by regulations like GDPR, underscores the need for robust explanations of Recommender Systems (RS). Key to the development and research progress of such methods are reproducible, quantifiable evaluation metrics. Traditional human-involved evaluation methods are not reproducible, subjective, costly, and fail to capture the counterfactual nuances of AI explanations. Hence, there is a pressing need for objective and scalable metrics to accurately measure the correctness of explanation methods for recommender systems. Inspired by similar approaches in computer vision, this research aims to propose a counterfactual approach to evaluate explanation accuracy in RS. While counterfactual evaluation methods have been established in other domains, they are underexplored in RS. Our goal is to introduce quantifiable metrics that objectively assess the correctness of local explanations. This approach enhances evaluation reliability and scalability, integrating various recommenders, explanation algorithms, and datasets. Our goal is to provide a comprehensive mechanism combining model fidelity with explanation correctness, advancing transparency and trustworthiness in AI-driven recommender systems.</p>
    <p><strong>Categories:</strong> Explainability, Recommender Systems, Evaluation Metrics, Ethical AI, Counterfactual Analysis, Transparency, Trustworthy AI, Model Fidelity, Scalability (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1134/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Demystifying Recommender Systems: A Multi-faceted Examination of Explanation Generation, Impact, and Perception (2023)</h3>
    <p><strong>Authors:</strong> Giacomo Balloccu</p>
    <p>Recommender systems have become an integral component of the digital landscape, impacting a multitude of services and industries ranging from e-commerce to entertainment and beyond. By offering personalised suggestions, these systems challenge a fundamental problem in our modern information society named information overload. As users face a deluge of choices, recommender systems help sift through this immense sea of possibilities, delivering a personalised subset of options that align with user preferences and historical behaviour. However, despite their considerable utility, recommender systems often operate as “black boxes,” obscuring the rationale behind recommendations. This opacity can engender mistrust and undermine user engagement, thus attenuating the overall effectiveness of the system. Researchers have emphasized the importance of explanations in recommender systems, highlighting how explanations can enhance system transparency, foster user trust, and improve decision-making processes, thereby enriching user experiences and yielding potential business benefits. Yet, a significant gap persists in the current state of human-understandable explanations research. While recommender systems have grown increasingly complex, our capacity to generate clear, concise, and relevant explanations that reflect this complexity remains limited. Crafting explanations that are both understandable and reflective of sophisticated algorithmic decision-making processes poses a significant challenge, especially in a manner that meets the user’s cognitive and contextual needs.</p>
    <p><strong>Categories:</strong> Explainability, User Trust, Information Overload, Transparency, Artificial Intelligence, Human-Computer Interaction, Personalization, Relevance, Algorithmic Transparency, Cross-Domain Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/975/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Who do you think I am? Interactive User Modelling with Item Metadata (2022)</h3>
    <p><strong>Authors:</strong> Bart Goethals, Koen Ruymbeek, Joey De Pauw</p>
    <p>Recommender systems are used in many different applications and contexts, however their main goal can always be summarised as “connecting relevant content to interested users”. Explanations have been found to help recommender systems achieve this goal by giving users a look under the hood that helps them understand why they are recommended certain items. Furthermore, explanations can be considered to be the first step towards interacting with the system. Indeed, for a user to give feedback and guide the system towards better understanding her preferences, it helps if the user has a better idea of what the system has already learned.<br>To this end, we propose a linear collaborative filtering recommendation model that builds user profiles within the domain of item metadata. Our method is hence inherently transparent and explainable. Moreover, since recommendations are computed as a linear function of item metadata and the interpretable user profile, our method seamlessly supports interactive recommendation. In other words, users can directly tweak the weights of the learned profile for more fine-grained browsing and discovery of content based on their current interests. We demonstrate the interactive aspect of this model in an online application for discovering cultural events in Belgium.</p>
    <p><strong>Categories:</strong> Explainable Recommendations, Transparency, Collaborative Filtering, Item Metadata, Interactive Recommendation, User Profiling, User Feedback, Real World Application, Usability, Dynamic Adaptation, A/B Test, Core Recommendation. (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/802/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Designing and evaluating explainable AI for non-AI experts: challenges and opportunities (2022)</h3>
    <p><strong>Authors:</strong> Katrien Verbert, Vero Vanden Abeele, Maxwell Szymanski</p>
    <p>Artificial intelligence (AI) has seen a steady increase in use in the health and medical field, where it is used by lay users and health experts alike. However, these AI systems often lack transparency regarding the inputs and decision making process (often called black boxes), which in turn can be detrimental to the user’s satisfaction and trust towards these systems. Explainable AI (XAI) aims to overcome this problem by opening up certain aspects of the black box, and has proven to be a successful means of increasing trust, transparency and even system effectiveness. However, for certain groups (i.e. lay users in health), explanation methods and evaluation metrics still remain underexplored. In this paper, we will outline our research regarding designing and evaluating explanations for health recommendations for lay users and domain experts, as well as list a few takeaways we were already able to find in our initial studies.</p>
    <p><strong>Categories:</strong> Explainable AI, Transparency, Trust in AI, Healthcare, Medicine, User-Centered Design, Evaluation Metrics, Health Recommendations, Explanation Methods, User Trust, Challenges and Opportunities, User Satisfaction (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/811/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Self-Supervised Bot Play for Transcript-Free Conversational Recommendation with Rationales (2022)</h3>
    <p><strong>Authors:</strong> Julian McAuley, Bodhisattwa Prasad Majumder, Shuyang Li</p>
    <p>Conversational recommender systems offer a way for users to engage in multi-turn conversations to find items they enjoy. For users to trust an agent and give effective feedback, the recommender system must be able to explain its suggestions and rationales. We develop a two-part framework for training multi-turn conversational recommenders that provide recommendation rationales that users can effectively interact with to receive better recommendations. First, we train a recommender system to jointly suggest items and explain its reasoning via subjective rationales. We then fine-tune this model to incorporate iterative user feedback via self-supervised bot-play. Experiments on three real-world datasets demonstrate that our system can be applied to different recommendation models across diverse domains to achieve state-of-the-art performance in multi-turn recommendation. Human studies show that systems trained with our framework provide more useful, helpful, and knowledgeable suggestions in warm- and cold-start settings. Our framework allows us to use only product reviews during training, avoiding the need for expensive dialog transcript datasets that limit the applicability of previous conversational recommender agents.</p>
    <p><strong>Categories:</strong> Conversational Recommender Systems, Self-Supervised Learning, Multi-Turn Conversations, Recommendation Rationales, Transparency, Cold Start, Human Studies, Beyond Accuracy, Data Efficiency, Bot Play (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/778/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>ProtoMF: Prototype-based Matrix Factorization for Effective and Explainable Recommendations (2022)</h3>
    <p><strong>Authors:</strong> Christian Ganhör, Alessandro B. Melchiorre, Navid Rekabsaz, Markus Schedl</p>
    <p>Recent studies show the benefits of reformulating common machine learning models through the concept of prototypes – representatives of the underlying data, used to calculate the prediction score as a linear combination of similarities of a data point to prototypes. Such prototype-based formulation of a model, in addition to preserving (sometimes enhancing) the performance, enables explainability of the model’s decisions, as the prediction can be linearly broken down into the contributions of distinct definable prototypes. Following this direction, we extend the idea of prototypes to the recommender system domain by introducing ProtoMF, a novel collaborative filtering algorithm. ProtoMF learns sets of user/item prototypes that represent the general consumption characteristics of users/items in the underlying dataset. Using these prototypes, ProtoMF then represents users and items as vectors of similarities to the corresponding prototypes. These user/item representations are ultimately leveraged to make recommendations that are both effective in terms of accuracy metrics, and explainable through the interpretation of prototypes’ contributions to the affinity scores. We conduct experiments on three datasets to assess both the effectiveness and the explainability of ProtoMF. Addressing the former, we show that ProtoMF exhibits higher Hit Ratio and NDCG compared to other relevant collaborative filtering approaches. As for the latter, we qualitatively show how ProtoMF can provide explainable recommendations and how its explanation capabilities can expose the existence of statistical biases in the learned representations, which we exemplify for the case of gender bias.</p>
    <p><strong>Categories:</strong> Matrix Factorization, Explainability, Collaborative Filtering, Recommendation Systems, Evaluation Metrics, Fairness, Explainable Recommendations, Prototype-based Models, Collaborative Filtering Techniques, Transparency (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/773/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Recommending for a Multi-Sided Marketplace with Heterogeneous Contents (2022)</h3>
    <p><strong>Authors:</strong> Xian Xing Zhang, Yuyan Wang, Long Tao</p>
    <p>Many online personalization platforms today are recommending heterogeneous contents in a multi-sided marketplace consisting of consumers, merchants and other partners. For a recommender system to be successful in these contexts, it faces two main challenges. First, each side in the marketplace has different and potentially conflicting utilities. Recommending for a multi-sided marketplace therefore entails jointly optimizing multiple objectives with trade-offs. Second, the off-the-shelf recommendation algorithms are not applicable to the heterogeneous content space, where a recommendation item could be an aggregation of other recommendation items. In this work, we develop a general framework for recommender systems in a multi-sided marketplace with heterogeneous and hierarchical contents. We propose a constrained optimization framework with machine learning models for each objective as inputs, and a probabilistic structural model for users’ engagement patterns on heterogeneous contents. Our proposed structural modeling approach ensures consistent user experience across different levels of aggregation of the contents, and provides levels of transparency to the merchants and content providers. We further develop an efficient optimization solution for ranking and recommendation in large-scale online systems in real time. We implement the framework at Uber Eats, one of the largest online food delivery platforms in the world and a three-sided marketplace consisting of eaters, restaurant partners and delivery partners. Online experiments demonstrate the effectiveness of our framework in ranking heterogeneous contents and optimizing for the three sides in the marketplace. Our framework has been deployed globally as the recommendation algorithm for Uber Eats’ homepage.</p>
    <p><strong>Categories:</strong> Multi-Sided Marketplaces, Heterogeneous Content, Multi-Objective Optimization, Constrained Optimization, Machine Learning Models, User Engagement, User Experience, Transparency, Real-World Application, A/B Testing, Food Delivery Platforms, Global Applications, Marketplace Optimization, Aggregated Content (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/820/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Designing for the Better by Taking Users into Account: A Qualitative Evaluation of User Control Mechanisms in (News) Recommender Systems (2019)</h3>
    <p><strong>Authors:</strong> Jaron Harambam, Mykola Makhortykh, Dimitrios Bountouridis, Joris van Hoboken</p>
    <p>Recommender systems (RS) are on the rise in many domains. While they offer great promises, they also raise concerns: lack of transparency, reduction of diversity, little to no user control. In this paper, we align with the normative turn in computer science which scrutinizes the ethical and societal implications of RS. We focus and elaborate on the concept of user control because that mitigates multiple problems at once. Taking the news industry as our domain, we conducted four focus groups, or moderated think-aloud sessions, with Dutch news readers (N=21) to systematically study how people evaluate different control mechanisms (at the input, process, and output phase) in a News Recommender Prototype (NRP). While these mechanisms are sometimes met with distrust about the actual control they offer, we found that an intelligible user profile (including reading history and flexible preferences settings), coupled with possibilities to influence the recommendation algorithms is highly valued, especially when these control mechanisms can be operated in relation to achieving personal goals. By bringing (future) users’ perspectives to the fore, this paper contributes to a  richer understanding of why and how to design for user control in recommender systems.</p>
    <p><strong>Categories:</strong> Recommender Systems, News Domain, User Control Mechanisms, Qualitative Evaluation, Ethical Considerations, Societal Implications, User Study, Input Control Mechanisms, Process Control Mechanisms, Output Control Mechanisms, Transparency, Personalization, News Recommender Systems, User-Centered Design, Usability (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/438/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Content-based Feature Exploration for Transparent Music Recommendation using Self-attentive Genre Classification (2018)</h3>
    <p><strong>Authors:</strong> Juheon Lee, Kyogu Lee, Seungjin Lee</p>
    <p>Interpretation of retrieved results is an important issue in music recommender systems, particularly from a user perspective. In this study, we investigate the methods for providing interpretability of content features using self-attention. We extract lyric features with the self-attentive genre classification model trained on 140,000 tracks of lyrics. Likewise, we extract acoustic features using the acoustic model with self-attention trained on 120,000 tracks of acoustic signals. The experimental results show that the proposed methods provide the characteristics that are interpretable in terms of both lyrical and musical contents. We demonstrate this by visualizing the attention weights, and by presenting the most similar songs found using lyric or audio features.</p>
    <p><strong>Categories:</strong> Music Recommendations, Content-Based Recommendations, Deep Learning, Explainable AI, Interpretability, Scalability, User-Centric Design, Model Interpretation, Feature Extraction, Self-attention, Genre Classification, Beyond Accuracy, Transparency, Lyric Analysis, Acoustic Features (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/415/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Using Explainability for Constrained Matrix Factorization (2017)</h3>
    <p><strong>Authors:</strong> Olfa Nasraoui, Behnoush Abdollahi</p>
    <p>Accurate model-based Collaborative Filtering (CF) approaches tend to be black-box machine learning models, such as Matrix Factorization (MF), that lack interpretability and do not provide a straightforward explanation for their outputs. Yet explanations can improve the transparency of a recommender system by justifying recommendations, and this in turn can enhance the user’s trust in the recommendations. Hence, one main challenge in designing a recommender system is mitigating the trade-off between an explainable technique with moderate prediction accuracy and a more accurate technique with no explainable recommendations. In this paper, we focus on MF and further assume the absence of any additional data source, such as item content or user attributes. We propose an explainability constrained MF technique that computes the top-n recommendation list from items that are explainable. Experimental results show that our method is effective in generating accurate and explainable recommendations.</p>
    <p><strong>Categories:</strong> Matrix Factorization, Collaborative Filtering, Explainable AI, Transparency, Trust, User Trust, Recommendation Systems, Algorithmic Transparency, Interpretability, Evaluation Metrics, Performance Analysis (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/309/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Towards Understanding Latent Factors and User Profiles by Enhancing Matrix Factorization with Tags (2016)</h3>
    <p><strong>Authors:</strong> Jürgen Ziegler, Tim Donkers, Benedikt Loepp</p>
    <p>With the interactive recommending approach we have recently proposed, users are given more control over model-based Collaborative Filtering while the results are perceived as more transparent. Integrating the latent factors derived by Matrix Factorization with tags users provided for the items has, however, even more advantages. In this paper, we show how general understanding of the abstract factor space, and of user and item positions inside it, can benefit from the semantics introduced by considering additional information. Moreover, our approach allows us to explain the user’s (former latent) preference profile by means of tags.</p>
    <p><strong>Categories:</strong> Matrix Factorization, Collaborative Filtering, User Profiles, Latent Factors, Tags, Transparency, Interpretability, Enhanced Recommendations, User Control, User Preferences (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/245/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>A Hybrid Explanations Framework for Collaborative Filtering Recommender Systems (2014)</h3>
    <p><strong>Authors:</strong> Shay Ben Elazar, Noam Koenigstein</p>
    <p>Augmenting personalized recommendations with explanations is believed to improve users' trust, loyalty, satisfaction, and recommender's persuasiveness. We present a flexible explanations framework for collaborative filtering recom-mender systems. Our algorithms utilizes item tags to automatically generate personalized explanations in a natural language format. Given a specific user and a recommended item, the algorithm utilizes the user's personal information as well as global information (e.g., item similarities, metadata) in order to rank item tags based on their "explanatory power". The top tags are chosen to construct a personalized explanation sentence which helps shed light on the underlying recommender. Our system has been well received by both focus groups as well as in expert evaluations and is scheduled to be evaluated in an online experiment.</p>
    <p><strong>Categories:</strong> Collaborative Filtering, Recommender Systems, Explanations, Transparency, A/B Testing, User Surveys, Trust, Real World Application, Matrix Factorization, Natural Language Processing, Content-Based Filtering (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/66/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>