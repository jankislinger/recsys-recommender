<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">User-Centric Design</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/scalability/">Scalability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/real-world-applications/">Real-world Applications</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/e-commerce/">E-commerce</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Promoting Two-sided Fairness with Adaptive Weights for Providers and Customers in Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Wayne Xin Zhao, Ji-Rong Wen, Lanling Xu, Sheng Chen, Zihan Lin, Jinpeng Wang</p>
    <p>At present, most recommender systems involve two stakeholders, providers and customers. Apart from maximizing the recommendation accuracy, the fairness issue for both sides should also be considered. However, there is a trade-off for multi-objective optimization problems, where optimizing one objective (e.g. provider-side fairness) may degrade the performance of others (e.g. accuracy). Most of previous studies try to improve two-sided fairness with post-processing algorithms or fairness-aware loss constraints, which are highly dependent on the heuristic adjustments without respect to the optimization goal of accuracy. In contrast, we propose a novel training framework, adaptive weighting  towards two-sided fairness-aware recommendation (named Ada2Fair), which lies in the extension of the accuracy-focused objective to a controllable preference learning loss over the interaction data. Specifically, we adjust the optimization scale of an interaction sample with an adaptive weight generator, and estimate the two-sided fairness-aware weights within model training. During the training process, the recommender is trained with two-sided fairness-aware weights to boost the utility of niche providers and inactive customers in a unified way. Extensive experiments on three public datasets verify the effectiveness of Ada2Fair, which can achieve Pareto improvements in two-sided fairness-aware recommendation. Our code implementation is available at https://anonymous.4open.science/r/Ada2Fair.</p>
    <p><strong>Categories:</strong> Adaptive Weights, Recommender Systems, Fairness, Two-sided Fairness, Multi-objective Optimization, Recommendation Frameworks, Provider-Customer Dynamics, User-Centric Design, Real-world Applications, Adaptive Learning, Evaluation Metrics (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1112/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Explainable and Faithful Educational Recommendations through Causal Language Modelling via Knowledge Graphs (2024)</h3>
    <p><strong>Authors:</strong> Neda Afreen</p>
    <p>The rapid expansion of digital education has significantly increased the need for recommender systems to help learners navigate the extensive variety of available learning resources. Recent advancements in these systems have notably improved the personalization of course recommendations. However, many existing systems fail to provide clear explanations for their recommendations, making it difficult for learners to understand why a particular suggestion was made. This lack of transparency can negatively impact trust and acceptance of the system. Researchers have emphasized the importance of explanations in various other domains such as ecommerce, media, and entertainment, demonstrating how explanations can enhance system transparency, foster user trust, and improve decision-making processes. Despite these insights, such approaches have been rarely applied to the educational domain, and their effectiveness in practical use remains largely unexamined. My research focuses on developing explainable recommender systems for digital education. First, I aim to design knowledge graphs that can support high-quality recommendations in the context of education. Second, I will create models backed by these knowledge graphs that not only deliver accurate recommendations but also provide faithful explanations for each suggestion, helping learners make informed decisions. Third, I will evaluate the effectiveness of these explainable recommender systems in real-world scenarios.</p>
    <p><strong>Categories:</strong> Causal Language Modeling, Knowledge Graphs, Explainable Recommendations, Faithful Explanations, Transparency in Recommendations, Trust in Systems, Real-World Evaluation, Education Domain, User-Centric Design, Personalization (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1138/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Personal Values and Community-Centric Environmental Recommender Systems: enhancing Sustainability through User Engagement (2024)</h3>
    <p><strong>Authors:</strong> Bianca Maria Deconcini</p>
    <p>The concept of sustainability has become a central focus across multiple sectors, driven by the urgent need to address climate change and protect the environment. Technological advancements and capabilities, together with the emergence of new ecological issues, are leading to growing awareness and influencing shifts in multiple areas such as energy, transportation, and waste management. Within this context, the roles of recommender systems represent a promising solution, since people need guidance and occasionally a gentle push to translate their intentions into actions or to bring goals to life. However, existing literature reveals a fragmented landscape, with solutions often addressing specific aspects or recommendation contribution in isolation. Many sustainability interventions focus solely on providing consumption data and environmental insights, while others emphasize learning and behavior change strategies. My doctoral project aims to address this gap by leveraging various approaches to recommender systems and applying them in sustainability contexts, with the goal to build a holistic system that maximizes the contributions of these diverse methods, also integrating user-centric and value-driven perspectives. This research project delves into two distinct facets: energy sustainability and sustainable mobility. The first case centers on enhancing energy efficiency within energy communities through personalized recommendations and engagement strategies. The second facet focuses on reshaping user commuting patterns towards sustainable alternatives, by recommending suitable and more sustainable modes of transportation, such as cycling, carpooling, and public transportation. Both cases share the same objective: align user behaviors with sustainability goals, thereby reducing individual environmental impact and enhancing the sense of belonging to a community, whether this is confined to a group of individuals or pertains to society at large. An innovative comprehensive recommendation system approach is highly beneficial since it can take advantage of all the existing contributions combined in a framework that makes at the same time different types of recommendations: explainable, educative, behavioral and social-aware, addressing the complexities of this multifaceted domain.</p>
    <p><strong>Categories:</strong> Environmental Sustainability, Sustainable Mobility, Energy Efficiency, Community Engagement, User-Centric Design, Value-Driven Approaches, Recommender Systems, Integrated Frameworks, Explainable Recommendations, Educative Recommendations, Behavioral Change, Social-Aware Recommendations, Fragmented Approaches (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1148/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Fairness and Transparency in Music Recommender Systems: Improvements for Artists (2024)</h3>
    <p><strong>Authors:</strong> Karlijn Dinnissen</p>
    <p>Music streaming services have become one of the main sources of music consumption in the last decade, with recommender systems as an important component. As those systems partially decide the songs that music consumers listen to, the systems greatly impact the artists who created the songs. However, when evaluating performance and fairness of these music recommending systems (MRSs), the perspective of the item providers or other music industry professionals is often not considered. Additionally, artists indicate they would appreciate more transparency – both towards and users and the artists themselves – regarding why certain items are recommended and others are not. This research project takes a multi-stakeholder approach to bridge the gap between music systems and their item providers. We first establish artists’ and music industry professionals’ perspective on MRSs through interviews and questionnaires. Based on those insights, we then aim to increase matching between end users and lesser-known artists by generating rich item and user representations. Results will be evaluated both quantitatively and qualitatively. Lastly, we plan to effectively communicate MRS fairness by increasing transparency for both end users and artists.</p>
    <p><strong>Categories:</strong> Fairness, Transparency, Music Recommender Systems (MRS), Artist Perspective, Stakeholder Approach, Evaluation Methods, Recommendation Algorithms, Representation Learning, Multi-Stakeholder Systems, User-Centric Design, Algorithmic Transparency, Diversity in Recommendations (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1136/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Co-optimize Content Generation and Consumption in a Large Scale Video Recommendation System (2024)</h3>
    <p><strong>Authors:</strong> Yuening Li, Mingyan Gao, Qingyun Liu, Sourabh Bansod, Shuchao Bi, Liang Liu, Yaping Zhang, Zhen Zhang</p>
    <p>Multi-task prediction models and value models are the de-facto standard ranking components in modern large-scale content recommendation systems. However, they are typically optimized to model users’ passive consumption behaviors, and rank content in a way to grow only consumption-centric values. In this talk, we discuss the key insight that it is possible to model sparse participatory content-generation actions as well and grow ecosystem value through a new ranking system. We made the following key technical contributions in this system: (1) introducing ranking for content generation based on a categorization of user participation actions of different sparsity, including proxy intent action or access point clicks. (2) improving sparse task prediction quality and stability by causal task relationship modeling, conditional loss modeling and ResNet based shared bottom network. (3) personalizing the value model to minimize conflicts between different values, through e.g. ranking inspiring content higher for users who actively generate content. (4) conducting systematic evaluation of proposed approach in a large short-form video UGC (User-Generated Content) platform.</p>
    <p><strong>Categories:</strong> Multi-Task Learning, Video Recommendation, User-Generated Content, Content Generation, Large-Scale Systems, Evaluation Metrics, Causal Modeling, Network Architecture, User Participation, Scalability, Content Creation, User-Centric Design (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1156/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Topic-Level Bayesian Surprise and Serendipity for Recommender Systems (2023)</h3>
    <p><strong>Authors:</strong> Razvan Bunescu, Tonmoy Hasan</p>
    <p>A recommender system that optimizes its recommendations solely to fit a user’s history of ratings for consumed items can create a filter bubble, wherein the user does not get to experience items from novel, unseen categories. One approach to mitigate this undesired behavior is to recommend items with high potential for serendipity, namely surprising items that are likely to be highly rated. In this paper, we propose a content-based formulation of serendipity that is rooted in Bayesian surprise and use it to measure the serendipity of items after they are consumed and rated by the user. When coupled with a collaborative-filtering component that identifies similar users, this enables recommending items with high potential for serendipity. To facilitate the evaluation of topic-level models for surprise and serendipity, we introduce a dataset of book reading histories extracted from Goodreads, containing over 26 thousand users and close to 1.3 million books, where we manually annotate 450 books read by 4 users in terms of their time-dependent, topic-level surprise. Experimental evaluations show that models that use Bayesian surprise correlate much better with the manual annotations of topic-level surprise than distance-based heuristics, and also obtain better serendipitous item recommendation performance.</p>
    <p><strong>Categories:</strong> Recommender Systems, Serendipity in Recommendations, Bayesian Methods, Collaborative Filtering, Filter Bubbles, Evaluation Metrics (e.g., Surprise), Books/Book Recommendations, Content-Based Filtering, Topic-Level Analysis, User-Centric Design (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/930/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Collaborative filtering algorithms are prone to mainstream-taste bias (2023)</h3>
    <p><strong>Authors:</strong> Philipp Hager, Pantelis Analytis</p>
    <p>Collaborative filtering has been the main steam engine of the recommender systems community since the early 1990s. Collaborative filtering (and other) algorithms, however, have been predominantly evaluated by aggregating results across users or user groups. These performance averages hide large disparities: an algorithm may perform very well for some users (or groups) and very poorly for others. We show that performance variation is large and systematic. In experiments on three large scale datasets and using an array of collaborative filtering algorithms, we demonstrate the large performance disparities for different users across algorithms and datasets. We then show that performance variation is systematic and that two key features that characterize users, their mean taste similarity with other users and the dispersion in taste similarity, can explain performance variation better than previously identified features. We use these two features to visualize algorithm performance for different users, and point out that this mapping can be used to capture different categories of users that have been proposed before. Our results demonstrate an extensive mainstream-taste bias in all collaborative filtering algorithms, and they imply a fundamental fairness limitation that needs to be mitigated.</p>
    <p><strong>Categories:</strong> Collaborative Filtering, Fairness, Mainstream Bias, Algorithm Limitations, User Performance Variation, Taste Similarity, Evaluation Methods, Recommender Systems, Large Scale Datasets, Diversity of Recommendations, Visualization Techniques, User Centric Design (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/906/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Adversarial Collaborative Filtering for Free (2023)</h3>
    <p><strong>Authors:</strong> Chin-Chia Michael Yeh, Vivian Lai, Yan Zheng, Hao Yang, Mahashweta Das, Yujie Fan, Xiaoting Li, Huiyuan Chen</p>
    <p>Collaborative Filtering (CF) has been successfully applied to help users discover the items of interest. Nevertheless, existing CF methods suffer from noisy data issue, which negatively impacts the quality of personalized recommendation. To tackle this problem, many  prior studies leverage the adversarial learning principle to regularize the representations of users and items, which  has shown great ability in improving both generalizability and robustness. Generally, those methods  learn adversarial perturbations and model parameters using min-max optimization framework. However, there still have two major limitations: 1) Existing methods lack theoretical guarantees of why adding perturbations improve the model generalizability and robustness since noisy data is naturally different from adversarial attacks; 2)  Solving min-max optimization is time-consuming.  In addition to updating the model parameters, each iteration requires additional computations to update the perturbations, making them not scalable for industry-scale datasets. In this paper, we present Sharpness-aware Matrix Factorization (SharpMF), a simple yet effective method that conducts adversarial training without extra computational cost over the base optimizer. To achieve this goal, we first revisit the existing adversarial collaborative filtering and discuss its connection with recent Sharpness-aware Minimization. This analysis shows that adversarial training actually seeks model parameters that lie in neighborhoods having uniformly low loss values, resulting in better generalizability. To reduce the computational overhead, SharpMF introduces a novel trajectory loss to measure sharpness between current weights and past weights. Experimental results on real-world datasets demonstrate that our SharpMF achieves superior performance with almost zero additional computational cost comparing to adversarial training.</p>
    <p><strong>Categories:</strong> Adversarial Training, Collaborative Filtering, Matrix Factorization, Recommendation Systems, Noise Handling, Generalizability, Robustness, Computational Efficiency, Optimization Techniques, Real-World Applications, User-Centric Design, Scalability, Robustness in Machine Learning (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/848/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Enhanced Privacy Preservation for Recommender Systems (2023)</h3>
    <p><strong>Authors:</strong> Ziqing Wu</p>
    <p>My research focuses on privacy preservation for recommender systems specifically in the following aspects: first, how to better address users’ realistic privacy concerns and offer enhanced privacy control by considering what and with whom to share sensitive information for decentralized recommender systems; second, how to enhance the privacy preservation capability of LLM-based recommender systems; last, how to formulate uniform metrics to compare the privacy-preservation efficacy of the recommender system.</p>
    <p><strong>Categories:</strong> Privacy Preservation, Recommender Systems, User-Centric Design, Decentralized Systems, Large Language Models, Machine Learning, Deep Learning, Evaluation Metrics, Performance Measurement, Algorithm Evaluation, Data Security, Trust (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/976/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>HELPeR: An Interactive Recommender System for Ovarian Cancer Patients and Their Caregivers (2022)</h3>
    <p><strong>Authors:</strong> Khushboo Maulikmihir Thaker, Young Ji Lee, Peter Brusilovsky, Daqing He, Behnam Rahdari, Zhimeng Luo</p>
    <p>Recommending online resources to patients with ovarian cancer and their caregivers is a challenging task. On one hand, the recommended items must be relevant, recent, and reliable. On the other hand, they need to match the user’s levels of disease-specific health literacy. In this demonstration, we describe the overall architecture and key components of HELPeR, a knowledge-adaptive interactive recommender system for ovarian cancer patients and their caregivers.</p>
    <p><strong>Categories:</strong> Healthcare, Ovarian Cancer, Content Recommendation, Interactive Systems, Personalized Recommendations, System Architecture, User-Centric Design, Health Literacy, Information Retrieval, User Experience, Trustworthy Recommendations (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/806/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Building public service recommenders: Logbook of a journey (2021)</h3>
    <p><strong>Authors:</strong> Alessandro Piscopo</p>
    <p>Almost a hundred years old, the BBC is the world’s largest public service broadcasters, providing a broad range of digital, radio, and television services. At the core of what it does, sits the BBC’s mission to “act in the public interest, serving all audiences through the provision of impartial, high-quality and distinctive output and services which inform, educate, and entertain” [ 3]. The BBC’s remit is to be a provider of high quality, accurate, and unbiased content that should aim to engage its audiences, reflecting their diversity, e.g. with respect to their age, culture, and socio-economic background, and following the values of trust and the commitment to being independent, impartial, and honest. Editorial staff follows a large collection of editorial guidelines [ 5] to ensure that all BBC’s output is consistent with these principles. Every week, the BBC reaches more than 80% of UK’s adult population [4 ] and 279 million people worldwide [ 2], giving access to a vast and diverse amount of content, including video, audio, and text, spanning topics such as news, sport, education, and entertainment. BBC content is prized the world over for its quality. However, the media landscape has changed substantially since the BBC was founded, introducing users to unlimited content. Personalised experiences are the norm in online media platforms, e.g. Spotify or Netflix, and are valued positively; for example, personalisation in news has been found found to increase depth and diversity of content [1]. How can the BBC stay relevant for all its audiences this environment? Delivering a personalised experience which is relevant, timely, and contextually useful to every user is key to fulfil the public service remit of the BBC. We need to reach out and appeal to the under-served groups, such as young audiences or ethnic minorities, to be a BBC for each and everyone and not just for the majority. Although this is an excellent way to surface quality content and produce engaging content discovery paths, it is inherently unable to be tailored to the user and is hard to scale—the more the amount of content, the harder it is for curators to find relevant items. Automated recommendations are able to attain these goals. Nevertheless, the shift from manual curation to data-driven personalisation is a change of perspective that entails a number of challenges: Cultural challenges: Editorial have so far decided which pieces of content to surface and how these are connected. How do we switch to a data-driven approach which connects every single audience member to the most relevant and engaging content, while retaining the quality ensured by the domain experts’ experience? Operational challenges: Editorial have the last say on all BBC output and guarantee its compliance with the editorial guidelines. Data-driven personalised content makes no exception to that. How does editorial oversight apply to large-scale recommendations? How do we translate editorial guidelines into data science approaches, tools, and metrics? Infrastructural challenges: The BBC has a large number of products and services. How do we enable data scientists and data engineers to create and deploy highly performant recommenders with the minimal possible effort? This document outlines the technical solutions and work processes our team, Datalab, have devised and the lessons we learnt along this journey.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Broadcasting/Media, Personalization, Public Service Broadcasting, Diversity of Recommendations, User-Centric Design, Editorial Oversight, Cultural Challenges, Operational Challenges, Infrastructural Challenges, Scalability, Data Science Approaches (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/723/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Estimating and Penalizing Preference Shifts in Recommender Systems (2021)</h3>
    <p><strong>Authors:</strong> Micah Carroll, Stuart Russell, Anca Dragan, Dylan Hadfield-Menell</p>
    <p>Recommender systems trained via long-horizon optimization (e.g., reinforcement learning) will have incentives to actively manipulate user preferences through the recommended content. While some work has argued for making systems myopic to avoid this issue, even such systems can induce systematic undesirable preference shifts. Thus, rather than artificially stifling the capabilities of the system, in this work we explore how we can make capable systems that explicitly avoid undesirable shifts. We advocate for (1) estimating the preference shifts that would be induced by recommender system policies, and (2) explicitly characterizing what unwanted shifts are and assessing before deployment whether such policies will produce them – ideally even actively optimizing to avoid them. These steps involve two challenging ingredients: (1) requires the ability to anticipate how hypothetical policies would influence user preferences if deployed; instead, (2) requires metrics to assess whether such influences are manipulative or otherwise unwanted. We study how to do (1) from historical user interaction data by building a user predictive model that implicitly contains their preference dynamics; to address (2), we introduce the notion of a “safe policy”, which defines a trust region within which behavior is believed to be safe. We show that recommender systems that optimize for staying in the trust region avoid manipulative behaviors (e.g., changing preferences in ways that make users more predictable), while still generating engagement.</p>
    <p><strong>Categories:</strong> Recommender Systems, Reinforcement Learning, Preference Dynamics, Ethical Considerations, User Modeling, Trustworthy AI, Safe Policy, Fairness, Optimization, User-Centric Design, Real-World Applications, Improvement of Recommender Systems, Predictive Modeling (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/690/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>NU:BRIEF – A Privacy-aware Newsletter Personalization Engine for Publishers (2021)</h3>
    <p><strong>Authors:</strong> Reshma Narayanan Kutty, Igor Brigadir, Ernesto Diaz-Aviles, Claudia Orellana-Rodriguez</p>
    <p>Newsletters have (re-) emerged as a powerful tool for publishers to engage with their readers directly and more effectively. Despite the diversity in their audiences, publishers’ newsletters remain largely a one-size-fits-all offering, which is suboptimal. In this paper, we present NU:BRIEF, a web application for publishers that enables them to personalize their newsletters without harvesting personal data. Personalized newsletters build a habit and become a great conversion tool for publishers, providing an alternative readers-generated revenue model to a declining ad/clickbait-centered business model.<br>Demo: https://demo.nubrief.com/md03PaAJSwXMegL5BbKpQlArK3elb3hDUglcHodx4gE=/<br>Explainer video: https://www.youtube.com/watch?v=AUZGuyPJYH4</p>
    <p><strong>Categories:</strong> Newsletter Personalization, Privacy-Preserving Techniques, User-Centric Design, Web Applications, Personalized Content, Publisher Tools, User Engagement, Media and Publishing, Recommendation Systems, Real-World Applications, Privacy-Aware Algorithms, Business Models, Habit Formation (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/692/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Measuring and Mitigating Bias and Harm in Personalized Advertising (2021)</h3>
    <p><strong>Authors:</strong> Muhammad Ali</p>
    <p>Online personalized advertising is often very effective in identifying relevant audiences for each piece of content, which has led to its widespread adoption. In today’s internet, however, these advertising systems are used not only to market products, but also consequential life opportunities such as employment or housing, as well as socially important political messaging. This has led to increasing concerns about the presence of algorithmic bias and possible discrimination in these important domains — with results showing problematic biases along gender, race, and political affiliation, even when the advertiser might have targeted broadly.<br>A growing body of work focuses on measuring and characterizing these biases, as well as finding ways to mitigate these effects and building responsible systems. However, these results often emerge from different scientific communities and are often disconnected in the literature. In this paper, I attempt at bridging the gap between isolated efforts to either measure these biases, or to mitigate them. I discuss how the need to measure bias in advertising, and the efforts to mitigate it, despite being distant in the literature, are complementary problems that need to center their methodolgy around user studies.<br>This paper presents a research agenda that focuses on the need for user-centric measurements of bias, by collecting real ads from users, and using surveys to understand user perceptions for these ads. My approach also calls for incorporating user sentiments into the mitigation efforts, by constraining optimization on user values that emerge from surveys. Finally, I also emphasize the need for involving users in the evaluation of responsible advertising systems; efforts to mitigate bias eventually need to be contextualized in terms of benefits to users instead of simple performance tradeoffs. My focus on the users is motivated by the fact that they are stakeholders in personalized advertising, vulnerable at the hand of algorithmic bias and harm, and therefore crucial in both efforts to measure and mitigate these effects.</p>
    <p><strong>Categories:</strong> Bias in Algorithms, Discrimination, Personalized Advertising, User-Centric Design, Stakeholder Analysis, Measurement Tools, Mitigation Strategies, Responsible AI, Ethics, Harm Reduction, Real-World Applications, Fairness in Algorithms, Cross-Disciplinary Approaches, Algorithmic Justice, User Studies (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/719/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>A Joint Dynamic Ranking System with DNN and Vector-based Clustering Bandit (2020)</h3>
    <p><strong>Authors:</strong> Changping Peng, Jincheng Wang, Xiaoxiao Xu, Weipeng Yan, Yong Li, Yongjun Bao, Yu Liu</p>
    <p>The ad-ranking module is the core of the advertising recommender system. Existing ad-ranking modules are mainly based on the deep neural network click-through rate prediction model. Recently an innovative ad-ranking paradigm called DNN-MAB has been introduced to address DNN-only paradigms’ weakness in perceiving highly dynamic user intent over time. We introduce the DNN-MAB paradigm into our ad-ranking system to alleviate the Matthew effect that harms the user experience. Due to data sparsity, however, the actual performance of DNN-MAB is lower than expected. In this paper, we propose an innovative ad-ranking paradigm called DNN-VMAB to solve these problems. Based on vectorization and clustering, it utilizes latent collaborative information in user behavior data to find a set of ads with higher relativity and diversity. As an integration of the essences of classical collaborative filtering, deep click-through rate prediction model, and contextual multi-armed bandit, it can improve platform revenue and user experience. Both offline and online experiments show the advantage of our new algorithm over DNN-MAB and some other existing algorithms.</p>
    <p><strong>Categories:</strong> Advertising, Recommender Systems, Deep Neural Networks (DNN), Multi-Armed Bandits, Clustering, Collaborative Filtering, Ad-Ranking, Vectorization, Cold Start, Handling Sparse Data, Offline Experiments, Online Experiments, Monetization Strategies, User-Centric Design (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/603/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Do Channels Matter?: Illuminating Interpersonal Influence on Music Recommendations (2020)</h3>
    <p><strong>Authors:</strong> Minju Park, So Yeon Park, Kyogu Lee, Hyun Jeong Kim</p>
    <p>Researchers and service providers have focused on leveraging social information acquired from interactions between users to improve the accuracy of system recommendations. However, few have explained the characteristics of music recommendations through interpersonal relationships. To investigate how interpersonal relationships affect users’ evaluation of music recommendation, we conducted a survey-based study that compared two types of recommendation channels—interpersonal (i.e., from friends) and non-interpersonal (i.e., from systems). We found that relevance was evaluated higher in music recommended from non-interpersonal channels on average, while diversity, novelty, and serendipity were higher in interpersonal channels. Non-interpersonal channels surpassed interpersonal channels in terms of convenience, frequency, and adoption rate. These results illustrate that interpersonal and non-interpersonal channels have different strengths and that digital streaming platforms, which have mainly provided system recommendations thus far, need to better support interpersonal channels for richer user experience.</p>
    <p><strong>Categories:</strong> Recommendation Channels, Social Recommendations, Interpersonal Relationships, User Experience, Evaluation of Recommendations, Diversity, Serendipity, Relevance, Digital Platforms, User-Centric Design. (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/607/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>