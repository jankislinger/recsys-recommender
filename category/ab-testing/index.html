<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reinforcement-learning/">Reinforcement Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/user-behavior-analysis/">User Behavior Analysis</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/explainability/">Explainability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/e-commerce/">E-commerce</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/evaluation-methods/">Evaluation Methods</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Prompt Tuning for Item Cold-start Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Jingchi Wang, Gaode Chen, Qi Zhang, Jingjian Lin, Yuezihan Jiang, Kaigui Bian, Wenhan Zhang, Peng Jiang, Yinjie Jiang</p>
    <p>The item cold-start problem is crucial for online recommender systems, as the success of the cold-start phase determines whether items can transition into popular ones. Prompt learning, a powerful technique used in natural language processing (NLP) to address zero- or few-shot problems, has been adapted for recommender systems to tackle similar challenges. However, existing methods typically rely on content-based properties or text descriptions for prompting, which we argue may be suboptimal for cold-start recommendations due to 1) semantic gaps with recommender tasks, 2) model bias caused by warm-up items contribute most of positive feedback to the model, which is the core of the cold-start problem that hinder the recommender quality on cold-start items. We propose to leverage high-value positive feedback, termed pinnacle feedback as prompt information, to simultaneously resolve the above two problems. We experimentally prove that comparing to content description proposed in existing works, the positive feedback is more suitable to serve as prompt information by bridging the semantic gaps. Besides, we propose item-wise personalized prompt networks to encode pinnaclce feedback to relieve the model bias by the positive feedback dominance problem. Extensive experiments on four real-world datasets demonstrate the superiority of our model over state-of-the-art methods. Moreover, PROMO has been successfully deployed on a popular short-video sharing platform, a billion-user scale commercial short-video application, achieving remarkable performance gains across various commercial metrics within cold-start scenarios.</p>
    <p><strong>Categories:</strong> Cold Start, Prompt Learning, Natural Language Processing (NLP), Recommender Systems, Semantic Gaps, Model Bias, High-Value Feedback, Personalized Prompts, Real-World Applications, A/B Testing, Evaluation Metrics, Feedback-Based Methods (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1055/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models (2024)</h3>
    <p><strong>Authors:</strong> Weiwen Liu, Bo Chen, Yong Yu, Hong Zhu, Yunjia Xi, Weinan Zhang, Jianghao Lin, Xiaoling Cai, Jieming Zhu, Ruiming Tang</p>
    <p>Recommender system plays a vital role in various online services. However, its insulated nature of training and deploying separately within a specific closed domain limits its access to open-world knowledge. Recently, the emergence of large language models (LLMs) has shown promise in bridging this gap by encoding extensive world knowledge and demonstrating reasoning capabilities. Nevertheless, previous attempts to directly use LLMs as recommenders cannot meet the inference latency demand of industrial recommender systems. In this work, we propose an Open-World Knowledge Augmented Recommendation Framework with Large Language Models, dubbed KAR, to acquire two types of external knowledge from LLMs — the reasoning knowledge on user preferences and the factual knowledge on items. We introduce factorization prompting to elicit accurate reasoning on user preferences. The generated reasoning and factual knowledge are effectively transformed and condensed into augmented vectors by a hybrid-expert adaptor in order to be compatible with the recommendation task. The obtained vectors can then be directly used to enhance the performance of any recommendation model. We also ensure efficient inference by preprocessing and prestoring the knowledge from the LLM. Extensive experiments show that KAR significantly outperforms the state-of-the-art baselines and is compatible with a wide range of recommendation algorithms. We deploy KAR to Huawei’s news and music recommendation platforms and gain a 7% and 1.7% improvement in the online A/B test, respectively.</p>
    <p><strong>Categories:</strong> Large Language Models (LLMs), Knowledge Augmentation, Recommender Systems, Open-World Recommendation, Knowledge Graphs, User Preference Modeling, Factorization Prompting, Feature Extraction, Representation Learning, Hybrid Models, Efficiency Optimization, Scalability, State-of-the-Art Methods, Real-World Applications, A/B Testing, News, Music, Beyond Accuracy, Compatibility (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1071/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Towards Understanding The Gaps of Offline And Online Evaluation Metrics: Impact of Series vs. Movie Recommendations (2024)</h3>
    <p><strong>Authors:</strong> Ashok Chandrashekar, Puja Das, Bora Edizel, Tim Sweetser, Kamilia Ahmadi</p>
    <p>In the realm of recommender systems research, offline evaluation metrics like NDCG, Recall, or Precision are often used to measure the impact. On the other hand, common industry practices suggest evaluating new ideas/models through A/B tests where decisions are made based on business metrics like the overall engagement of users. A new model may show improvement in offline metrics but performance loss in online metrics. One reason that leads to this phenomenon is the counterfactual nature of the recommendation problem which can be addressed by off-policy evaluation methods. Another reason is the degree of causal connection between offline evaluation metrics and observed online metrics. In this work, we will share our learnings from two sets of A/B tests that we conducted at Max1 where we observed a mismatch between online and offline metrics due to a weak causal connection between online and offline metrics. Thanks to learnings from A/B tests, we discovered and quantified the impact of series to movie ratio at recommendations. Our experiments show that there is an optimal amount of series to movies ratio that provides the best possible results for user engagement</p>
    <p><strong>Categories:</strong> Evaluation Metrics, Offline Evaluation, Online Evaluation, Recommender Systems, A/B Testing, User Engagement, Content Types, Causal Inference, Business Metrics, Movie Recommendations, Series Recommendations (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1182/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Touch the Core: Exploring Task Dependence Among Hybrid Targets for Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Xiuqiang He, Xing Tang, Dugang Liu, Yang Qiao, Fuyuan Lyu</p>
    <p>As user behaviors become complicated on business platforms, online recommendations focus more on how to touch the core conversions, which are highly related to the interests of platforms. These core conversions are usually continuous targets, such as watch time, revenue, and so on, whose predictions can be enhanced by previous discrete conversion actions. Therefore, multi-task learning (MTL) can be adopted as the paradigm to learn these hybrid targets. However, existing works mainly emphasize investigating the sequential dependence among discrete conversion actions, which neglects the complexity of dependence between discrete conversions and the final continuous conversion. Moreover, simultaneously optimizing hybrid tasks with stronger task dependence will suffer from volatile issues where the core regression task might have a larger influence on other tasks. In this paper, we study the MTL problem with hybrid targets for the first time and propose the model named Hybrid Targets Learning Network (HTLNet) to explore task dependence and enhance optimization. Specifically, we introduce label embedding for each task to explicitly transfer the label information among these tasks, which can effectively explore logical task dependence. We also further design the gradient adjustment regime between the final regression task and other classification tasks to enhance the optimization. Extensive experiments on two offline public datasets and one real-world industrial dataset are conducted to validate the effectiveness of HTLNet. Moreover, online A/B tests on the financial recommender system also show our model has superior improvement.</p>
    <p><strong>Categories:</strong> Multi-task Learning (MTL), Recommendation Systems, Task Dependence, Hybrid Targets, User Behavior, Real-World Applications, A/B Testing, Financial Recommender Systems, Optimization Techniques, Online Recommendations, Gradient Adjustment, Label Embedding. (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1068/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Δ-OPE: Off-Policy Estimation with Pairs of Policies (2024)</h3>
    <p><strong>Authors:</strong> Aleksei Ustimenko, Olivier Jeunen</p>
    <p>The off-policy paradigm casts recommendation as a counterfactual decision-making task, allowing practitioners to unbiasedly estimate online metrics using offline data. This leads to effective evaluation metrics, as well as learning procedures that directly optimise online success. Nevertheless, the high variance that comes with unbiased- ness is typically the crux that complicates practical applications. An important insight is that the difference between policy values can often be estimated with significantly reduced variance, if said policies have positive covariance. This allows us to formulate a pairwise off-policy estimation task: Δ-OPE. Δ-OPE subsumes the common use-case of estimating improve- ments of a learnt policy over a production policy, using data col- lected by a stochastic logging policy. We introduce Δ-OPE methods based on the widely used Inverse Propensity Scoring estimator and its extensions. Moreover, we characterise a variance-optimal additive control variate that further enhances efficiency. Simulated, offline, and online experiments show that our methods significantly improve performance for both evaluation and learning tasks.</p>
    <p><strong>Categories:</strong> Off-Policy Estimation, Inverse Propensity Scoring, A/B Testing, Recommendation Evaluation, Variance Reduction, Experimental Methods, Multi-Policy Evaluation, Domain-Specific Applications, Optimization Methods. (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1117/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>ReLand: Integrating Large Language Models’ Insights into Industrial Recommenders via a Controllable Reasoning Pool (2024)</h3>
    <p><strong>Authors:</strong> Haoyu Chen, Changxin Tian, Jiawei Chen, Jun Zhou, Ziqi Liu, Li Yu, Chunjing Gan, Zhuo Zhang, Binbin Hu, Zhiqiang Zhang</p>
    <p>Recently, Large Language Models (LLMs) have shown significant potential in addressing the isolation issues faced by recommender systems. However, despite performance comparable to traditional recommenders, the current methods are cost-prohibitive for industrial applications. Consequently, existing LLM-based methods still need to catch up regarding effectiveness and efficiency. To tackle the above challenges, we present an LLM-enhanced recommendation framework named ReLand, which leverages Retrieval to effortlessly integrate Large language models’ insights into industrial recommenders. Specifically, ReLand employs LLMs to perform generative recommendations on sampled users (a.k.a., seed users), thereby constructing an LLM Reasoning Pool. Subsequently, we leverage retrieval to attach reliable recommendation rationales for the entire user base, ultimately effectively improving recommendation performance. Extensive offline and online experiments validate the effectiveness of ReLand. Since January 2024, ReLand has been deployed in the recommender system of Alipay, achieving statistically significant improvements of 3.19% in CTR and 1.08% in CVR.</p>
    <p><strong>Categories:</strong> Large Language Models, Recommender Systems, Industrial Applications, Retrieval-Based Methods, Generative Recommendations, Controllable Reasoning Pool, Cold Start, Interpretability of Recommendations, Diversity of Recommendations, Evaluation Metrics (CTR, CVR), Real-World Applications, A/B Testing (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1054/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Explore versus repeat: insights from an online supermarket (2024)</h3>
    <p><strong>Authors:</strong> Mariagiorgia Agnese Tandoi, Daniela Solis Morales</p>
    <p>At online supermarket Picnic, we implemented both traditional collaborative filtering and a hybrid method to provide recipe recommendations at scale. This case study presents findings from the online evaluation of these algorithms, focusing on the repeat-explore trade-off. Our findings allow other online retailers to gain insights into the importance of thoughtful model design in navigating this important balance. We argue that even when exploiting known preferences proves highly beneficial in the short term, prioritizing exploratory content is essential for long-term customer satisfaction and sustained growth. Our research lays the groundwork for a compelling discussion on defining success in balancing the familiar and the novel in online grocery shopping.</p>
    <p><strong>Categories:</strong> Explore vs Exploit, Collaborative Filtering, Online Retail, A/B Testing, Recipe Recommendations, Scalability, Recommendation Systems, User Satisfaction, Hybrid Methods, Success Metrics (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1162/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Learned Ranking Function: From Short-term Behavior Predictions to Long-term User Satisfaction (2024)</h3>
    <p><strong>Authors:</strong> Yi Wu, Jennifer She, Daryl Chang, Lukasz Heldt, Zhe Zhao, Li Wei</p>
    <p>We present the Learned Ranking Function (LRF), a system that takes short-term user-item behavior predictions as input and outputs a slate of recommendations directly optimizing for long-term user satisfaction. Most previous work is based on optimizing hyper-parameters of a heuristic function. We propose to model the problem directly as a slate optimization problem with the objective of maximizing long-term user satisfaction. We also develop a new constraint optimization method that is able to improve the stability for multi-objective optimization. We evaluate our approach with live experiments and describe its deployment on YouTube.</p>
    <p><strong>Categories:</strong> Learned Ranking Function, Recommendation Systems, Slate Optimization, Long-term User Satisfaction, Multi-objective Optimization, Stability, Real-world Applications, A/B Testing, YouTube, Media/Video, Direct Ranking Methods, Cold Start (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1090/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>MLoRA: Multi-Domain Low-Rank Adaptive Network for CTR Prediction (2024)</h3>
    <p><strong>Authors:</strong> Libin Yang, Haining Gao, Wei Ning, Luwei Yang, Zhiming Yang, Dehong Gao, Guannan Zhang, Xiaoyan Cai</p>
    <p>Click-through rate (CTR) prediction is one of the fundamental tasks in the industry, especially in e-commerce, social media, and streaming media. It directly impacts website revenues, user satisfaction, and user retention. However, real-world production platforms often encompass various domains to cater for diverse customer needs. Traditional CTR prediction models struggle in multi-domain recommendation scenarios, facing challenges of data sparsity and disparate data distributions across domains. Existing multi-domain recommendation approaches introduce specific-domain modules for each domain, which partially address these issues but often significantly increase model parameters and lead to insufficient training. In this paper, we propose a Multi-domain Low-Rank Adaptive network (MLoRA) for CTR prediction, where we introduce a specialized LoRA module for each domain. This approach enhances the model’s performance in multi-domain CTR prediction tasks and is able to be applied to various deep-learning models. We evaluate the proposed method on several multi-domain datasets. Experimental results demonstrate our MLoRA approach achieves a significant improvement compared with state-of-the-art baselines. Furthermore, we deploy it in a real-world e-commerce production website. The online A/B testing results indicate the superiority and flexibility in real-world production environments.</p>
    <p><strong>Categories:</strong> Multi-Domain, Click-Through Rate Prediction, Recommendation Systems, Deep Learning Models, Low-Rank Adaptive Methods, Real-World Applications, A/B Testing, Data Sparsity, Cross-Domain, Scalability (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1041/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Enhancing Recommendation Quality of the SASRec Model by Mitigating Popularity Bias (2024)</h3>
    <p><strong>Authors:</strong> Xenija Neufeld, Sebastian Loth, Andreas Grün, Venkata Harshit Koneru</p>
    <p>ZDF is a Public Service Media (PSM) broadcaster in Germany that uses recommender systems on its streaming service platform ZDFmediathek. One of the main use cases within the ZDFmediathek is Next Video, which is currently based on a Self-Attention based Sequential Recommendation model (SASRec). For this use case, we modified the loss function, the sampling method of negative items, and introduced the top-k negative sampling strategy and compared this to the vanilla SASRec model. We show that this not only reduces popularity bias, but also increases clicks and the viewing volume compared to that of the vanilla version.</p>
    <p><strong>Categories:</strong> SASRec, Popularity Bias, Recommendation Quality, Streaming Services, Public Service Media, Clicks/Engagement Metrics, Loss Function Modification, Negative Sampling Strategies, Real World Applications, A/B Testing, User Experience Optimization, Cold Start Problem (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1164/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Multi-Objective Recommendation via Multivariate Policy Learning (2024)</h3>
    <p><strong>Authors:</strong> Ivan Potapov, Sourabh Vaid, Wenzhe Shi, Nakul Agarwal, Olivier Jeunen, Jatin Mandav, Aleksei Ustimenko</p>
    <p>Real-world recommender systems often need to balance multiple objectives when deciding which recommendations to present to users. These include behavioural signals (e.g. clicks, shares, dwell time), as well as broader objectives (e.g. diversity, fairness). Scalarisation methods are commonly used to handle this balancing task, where a weighted average of per-objective reward signals determines the final score used for ranking. Naturally, <i>how</i> these weights are computed exactly, is key to success for any online platform. We frame this as a decision-making task, where the scalarisation weights are <i>actions</i> taken to maximise an overall North Star reward (e.g. long-term user retention or growth). We extend existing policy learning methods to the continuous multivariate action domain, proposing to maximise a pessimistic lower bound on the North Star reward that the learnt policy will yield. Typical lower bounds based on normal approximations suffer from insufficient coverage, and we propose an efficient and effective policy-dependent correction for this. We provide guidance to design stochastic data collection policies, as well as highly sensitive reward signals. Empirical observations from simulations, offline and online experiments highlight the efficacy of our deployed approach.</p>
    <p><strong>Categories:</strong> Multi-Objective Optimization, Fairness in Recommendations, Diversity of Recommendations, Policy Learning, Reinforcement Learning in Recommender Systems, Implicit Feedback, Real-World Applications, A/B Testing, Scalarization Methods, Evaluation Methods, Scalability (<i>deepseek-r1:70b</i>)</p>
    <p><a href="/recsys-recommender/response/1051/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Country-diverted experiments for mitigation of network effects (2024)</h3>
    <p><strong>Authors:</strong> Ningren Han, Lina Lin, Jean Pouget-Abadie, Jennifer Brennan, Yajun Peng, Changping Meng, Shuchao Bi</p>
    <p>We describe the process of conducting a country-diverted experiment on a major content platform to mitigate the interference often observed in user-diverted A/B experiments. In particular, we propose a heuristic measure of leakage based on cosine similarity between treatment and control groups, which is used to select suitable country diversions, paired with a synthetic control approach to estimate the total treatment effect. We demonstrate the success of our approach through a live experiment on a key user engagement metric, as compared to a previous user-diverted experiment.</p>
    <p><strong>Categories:</strong> A/B Testing, Experimentation Methods, Network Effects Mitigation, Country-Diverted Experiments, Heuristic Measure of Leakage, Synthetic Control Approach, User Engagement Metrics, Recommendation Systems, Causal Inference (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1155/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Powerful A/B-Testing Metrics and Where to Find Them (2024)</h3>
    <p><strong>Authors:</strong> Shubham Baweja, Aleksei Ustimenko, Neeti Pokharna, Olivier Jeunen</p>
    <p>Online controlled experiments, colloquially known as A/B-tests, are the bread and butter of real-world recommender system evaluation. Typically, end-users are randomly assigned some system variant, and a plethora of metrics are then tracked, collected, and aggregated throughout the experiment. A North Star metric (e.g. long-term growth or revenue) is used to assess which system variant should be deemed superior. As a result, most collected metrics are <i>supporting</i> in nature, and serve to either (i) provide an understanding of how the experiment impacts user experience, or (ii) allow for confident decision-making when the North Star metric moves insignificantly (i.e. a false negative or type-II error). The latter is not straightforward: suppose a treatment variant leads to fewer but longer sessions, with more views but fewer engagements; should this be considered a positive or negative outcome? The question then becomes: how do we assess a supporting metric’s utility when it comes to decision-making using A/B-testing?Online platforms typically run dozens of experiments at any given time. This provides a wealth of information about interventions and treatment effects that can be used to evaluate metrics’ utility for online evaluation. We propose to collect this information and leverage it to quantify type-I, type-II, and type-III errors for the metrics of interest, alongside a distribution of measurements of their statistical power (e.g. $z$-scores and $p$-values). We present results and insights from building this pipeline at scale for two large-scale short-video platforms: ShareChat and Moj; leveraging hundreds of past experiments to find online metrics with high statistical power.</p>
    <p><strong>Categories:</strong> A/B Testing, Evaluation Metrics, Statistical Analysis, Recommender Systems, Real-World Applications, Experimentation, Decision-Making Under Uncertainty, Metrics Analysis and Evaluation, Statistical Power, Scalability, Historical Data Analysis (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1173/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Multi-Preview Recommendation via Reinforcement Learning (2024)</h3>
    <p><strong>Authors:</strong> Kuan-Ting Lai, Pengcheng Xiong, Zhong Wu, Yang Xu</p>
    <p>Preview recommendations serve as a crucial shortcut for attracting users’ attention on various systems, platforms, and webpages, significantly boosting user engagement. However, the variability of preview types and the flexibility of preview duration make it challenging to use an integrated framework for multi-preview recommendations under resource constraints. In this paper, we present an approach that incorporates constrained Q-learning into a notification recommendation system, effectively handling both multi-preview ranking and duration orchestration by targeting long-term user retention. Our method bridges the gap between combinatorial reinforcement learning, which often remains too theoretical for practical use, and segmented modules in production, where model performance is typically compromised due to over-simplification. We demonstrate the superiority of our approach through off-policy evaluation and online A/B testing using Microsoft data.</p>
    <p><strong>Categories:</strong> Reinforcement Learning, Recommendation Systems, Multi-Preview, Notifications, User Engagement, Resource Constraints, Q-Learning, Combinatorial Optimization, Online Evaluation, A/B Testing, Microsoft Data, Production Systems (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1207/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>MODEM: Decoupling User Behavior for Shared-Account Video Recommendations on Large Screen Devices (2024)</h3>
    <p><strong>Authors:</strong> Jiang Li, Lantao Hu, Zhen Zhang, Xiang Feng, Yongqi Liu, Muyang Li</p>
    <p>In scenarios involving sequence recommendations on large screen devices, such as tablets or TVs, the equipment is often shared among multiple users. This sharing leads to a mixture of behaviors from different users, posing significant challenges to recommendation systems, especially when clear supervisory signals for distinguishing among users are absent. Current solutions tend to either operate in an unsupervised manner or rely on constructed supervisory signals that are not entirely reliable. Moreover, the peculiarities of short video recommendations in this context have not been thoroughly explored in existing research. In response to these challenges, this paper introduces Multi-User Contrastive Decoupling Model (MODEM), a novel short video recommendation model specifically designed for large screen devices. MODEM leverages an attention mechanism, grounded in session segmentation, to disentangle the intertwined user behavior histories. It also discriminates between the impacts of long and short viewing behaviors on short video recommendations by cross-analyzing sequences of both. Furthermore, we have developed a contrastive learning method to oversee the decoupling of user behaviors effectively. Our evaluations demonstrate noticeable improvements through both offline assessments within public datasets and online A/B testing within Kuaishou’s short video recommendation environment on large screen devices. Specifically, our online A/B tests resulted in a 0.55% increase in watch time. These results underscore MODEM’s efficacy in enhancing recommendation quality in shared account contexts.</p>
    <p><strong>Categories:</strong> Video Recommendations, Large Screen Devices, Shared Accounts, User Behavior Analysis, Attention Mechanism, Contrastive Learning, Session Segmentation, A/B Testing, Real-World Application, Beyond Accuracy, Cold Start (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1098/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Why the Shooting in the Dark Method Dominates Recommender Systems Practice (2024)</h3>
    <p><strong>Authors:</strong> David Rohde</p>
    <p>The introduction of A/B Testing represented a great leap forward in recommender systems research. Like the randomized control trial for evaluating drug efficacy; A/B Testing has equipped recommender systems practitioners with a protocol for measuring performance as defined by actual business metrics and with minimal assumptions. While A/B testing provided a way to measure the performance of two or more candidate systems, it provides no guide for determining what policy we should test. The focus of this industry talk is to better understand, why the development of A/B testing was the last great leap forward in the development of reward optimizing recommender systems despite more than a decade of efforts in both industry and academia. The talk will survey: industry best practice, standard theories and tools including: collaborative filtering (MovieLens RecSys), contextual bandits, attribution, off-policy estimation, causal inference, click through rate models and will explain why we have converged on a fundamentally heuristic solution or guess and check type method. The talk will offer opinions about which of these theories are useful, and which are not and make a concrete proposal to make progress based on a non-standard use of deep learning tools.</p>
    <p><strong>Categories:</strong> A/B Testing, Evaluation Methods, Industry Practices, Challenges, Collaborative Filtering, Contextual Bandits, Attribution, Off-Policy Estimation, Causal Inference, Click-Through Rate Models, Methodologies, Theory Application, Future Directions (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1183/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>