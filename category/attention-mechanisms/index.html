<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">Attention Mechanisms</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/explainability/">Explainability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reinforcement-learning/">Reinforcement Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/real-world-applications/">Real-world Applications</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Enhancing Performance and Scalability of Large-Scale Recommendation Systems with Jagged Flash Attention (2024)</h3>
    <p><strong>Authors:</strong> Hong Li, Mingwei Tang, Meng Liu, Junjie Yang, Dai Li, Xing Liu, Tunhou Zhang, Arnold Overwijk, Haoci Zhang, Rengan Xu, Sijia Chen, Sri Reddy, Devashish Shankar, Jiaqi Zhai, Bill Zhu, Boyang Li, Zehua Zhang, Yifan Xu, Yuxi Hu</p>
    <p>The integration of hardware accelerators has significantly advanced the capabilities of modern recommendation systems, enabling the exploration of complex ranking paradigms previously deemed impractical. However, the GPU-based computational costs present substantial challenges. In this paper, we demonstrate our development of an efficiency-driven approach to explore these paradigms, moving beyond traditional reliance on native PyTorch modules. We address the specific challenges posed by ranking models’ dependence on categorical features, which vary in length and complicate GPU utilization. We introduce Jagged Feature Interaction Kernels, a novel method designed to extract fine-grained insights from long categorical features through efficient handling of dynamically sized tensors. We further enhance the performance of attention mechanisms by integrating Jagged tensors with Flash Attention. As the feature length grows, the Jagged Flash Attention is able to scale memory linearly rather than quadratically. Our experimental results demonstrate that Jagged Flash Attention achieves speedups of 2.4× to 5.6× over dense attention and reduces memory usage by up to 21.8×. This allows to scale the recommendation systems with longer features and more complex model architecture.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Scalability, GPU Usage, Categorical Features, Efficient Algorithms, Flash Attention, Memory Efficiency, Hardware Acceleration, Performance Optimization, Large-Scale Systems, Attention Mechanisms, Novel Methods, Efficiency-Driven Approaches, Complex Model Architectures. (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1163/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Pay Attention to Attention for Sequential Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Xiaojing Liu, Yuli Liu, Min Liu</p>
    <p>Transformer-based approaches have demonstrated remarkable success in various sequence-based tasks. However, traditional self-attention models may not sufficiently capture the intricate dependencies within items in sequential recommendation scenarios. This is due to the lack of explicit emphasis on attention weights, which play a critical role in allocating attention and understanding item-to-item correlations. To better exploit the potential of attention weights and improve the capability of sequential recommendation in learning high-order dependencies, we propose a novel sequential recommendation (SR) approach called attention weight refinement (AWRSR). AWRSR enhances the effectiveness of self-attention by additionally paying attention to attention weights, allowing for more refined attention distributions of correlations among items. We conduct comprehensive experiments on multiple real-world datasets, demonstrating that our approach consistently outperforms state-of-the-art SR models. Moreover, we provide a thorough analysis of AWRSR’s effectiveness in capturing higher-level dependencies. These findings suggest that AWRSR offers a promising new direction for enhancing the performance of self-attention architecture in SR tasks, with potential applications in other sequence-based problems as well.</p>
    <p><strong>Categories:</strong> Sequential Recommendations, Attention Mechanisms, Transformer-Based Models, Recommendation Systems, Higher-Order Dependencies, Model Performance, Experimental Analysis, Item Correlations, Real-World Applications, Novel Methods, Attention Weight Refinement, Self-Attention Architecture, Machine Learning for Recommendations, Potential Applications in Other Domains (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1104/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>STRec: Sparse Transformer for Sequential Recommendations (2023)</h3>
    <p><strong>Authors:</strong> Lixin Zou, Yiqi Wang, Qidong Liu, Wanyu Wang, Wenqi Fan, Yejing Wang, Chengxi Li, Qing Li, Xiangyu Zhao</p>
    <p>With the rapid evolution of transformer architectures, an increasing number of researchers are exploring their application in sequential recommender systems (SRSs). Compared with the former SRS models, the transformer-based models get promising performance on SRS tasks. Existing transformer-based SRS frameworks, however, retain the vanilla attention mechanism, which calculates the attention scores between all item-item pairs in each layer, i.e., item interactions. Consequently, redundant item interactions may downgrade the inference speed and cause high memory costs for the model. In this paper, we first identify the sparse information phenomenon in transformer-based SRS scenarios and propose an efficient model, i.e., Sparse Transformer sequential Recommendation model (STRec). First, we devise a cross-attention-based sparse transformer for efficient sequential recommendation. Then, a novel sampling strategy is derived to  preserve the necessary interactions. Extensive experimental results validate the effectiveness of our framework, which could outperform the state-of-the-art accuracy while reducing 54% inference time and 70% memory cost. Besides, we provide massive extended experiments to further investigate the property of our framework. Our code is available to ease reproducibility.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Sequential Recommendations, Transformer Models, Sparse Attention Mechanism, Efficiency Optimization, Inference Speed, Memory Management, Experimental Results, Model Design, Attention Mechanisms, Evaluation Metrics, Real-World Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/879/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Dual Attentional Higher Order Factorization Machines (2022)</h3>
    <p><strong>Authors:</strong> Prakash Mandayam Comar, Dipankar Das, Vivek Sembium, Arindam Sarkar</p>
    <p>Numerous problems of practical significance such as clickthrough rate (CTR) prediction, forecasting, tagging and so on, involve complex interaction of various user, item and context features. Manual feature engineering has been used in the past to model these combinatorial features but it requires domain expertise and becomes prohibitively expensive as the number of features increases. Feedforward neural networks alleviate the need for manual feature engineering to a large extent and have shown impressive performance across multiple domains due to their ability to learn arbitrary functions. Despite multiple layers of non-linear projections, neural networks are limited in their ability to efficiently model functions with higher order interaction terms. In recent years, Factorization Machines and its variants have been proposed to explicitly capture higher order combinatorial interactions. However not all feature interactions are equally important, and in sparse data settings, without a suitable suppression mechanism, this might result into noisy terms during inference and hurt model generalization. In this work we present Dual Attentional Higher Order Factorization Machine (DA-HoFM), a unified attentional higher order factorization machine which leverages a compositional architecture to compute higher order terms with complexity linear in terms of maximum interaction degree. Equipped with sparse dual attention mechanism, DA-HoFM summarizes interaction terms at each layer, and is able to efficiently select important higher order terms. We empirically demonstrate effectiveness of our proposed models on the task of CTR prediction, where our model exhibits superior performance compared to the recent state-of-the-art models, outperforming them by up to 6.7% on the logloss metric.</p>
    <p><strong>Categories:</strong> Matrix Factorization, Attention Mechanisms, Higher Order Interactions, Feature Interaction, User Behavior Modeling, Recommendation Systems, Predictive Analytics, CTR Prediction, Sparse Data Handling, Machine Learning Models, Real-World Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/759/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Reducing Cross-Topic Political Homogenization in Content-Based News Recommendation (2022)</h3>
    <p><strong>Authors:</strong> Aron Culotta, Karthik Shivaram, Matthew Shapiro, Mustafa Bilgic, Ping Liu</p>
    <p>Content-based news recommenders learn words that correlate with user engagement and recommend articles accordingly. This can be problematic for users with diverse political preferences by topic — e.g., users that prefer conservative articles on one topic but liberal articles on another. In such instances, recommenders can have a homogenizing effect by recommending articles with the same political lean on both topics, particularly if both topics share salient, politically polarized terms like “far right” or “radical left.” In this paper, we propose attention-based neural network models to reduce this homogenization effect by increasing attention on words that are topic specific while decreasing attention on polarized, topic-general terms. We find that the proposed approach results in more accurate recommendations for simulated users with such diverse preferences.</p>
    <p><strong>Categories:</strong> Content-Based Recommendation, News Domain, Neural Networks, Attention Mechanisms, Political Polarization, Fairness and Bias Mitigation, Personalization, Recommendation Quality, Evaluation Metrics Beyond Accuracy, Real World Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/781/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Denoising Self-Attentive Sequential Recommendation (2022)</h3>
    <p><strong>Authors:</strong> Yan Zheng, Fei Wang, Chin-Chia Michael Yeh, Huiyuan Chen, Menghai Pan, Yusan Lin, Xiaoting Li, Hao Yang, Lan Wang</p>
    <p>Transformer-based sequential recommenders are very powerful for capturing both short-term and long-term sequential item dependencies. This is mainly attributed to their unique self-attention networks to exploit pairwise item-item interactions within the sequence. However, real-world item sequences are often noisy, which is particularly true for implicit feedback. For example, a large portion of clicks do not align well with user preferences, and many products end up with negative reviews or being returned. As such, the current user action only depends on a subset of items, not on the entire sequences. Many existing Transformer-based models use full attention distributions, which inevitably assign certain credits to irrelevant items. This may lead to sub-optimal performance if Transformers are not regularized properly.<br>Here we propose the Rec-denoiser model for better training of self-attentive recommender systems. In Rec-denoiser, we aim to adaptively prune noisy items that are unrelated to the next item prediction. To achieve this, we simply attach each self-attention layer with a trainable binary mask to prune noisy attentions, resulting in sparse and clean attention distributions. This largely purifies item-item dependencies and provides better model interpretability. In addition, the self-attention network is typically not Lipschitz continuous and is vulnerable to small perturbations. Jacobian regularization is further applied to the Transformer blocks to improve the robustness of Transformers for noisy sequences. Our Rec-denoiser is a general plugin that is compatible to many Transformers. Quantitative results on real-world datasets show that our Rec-denoiser outperforms the state-of-the-art baselines.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Implicit Feedback, Attention Mechanisms, Transformer-Based Models, Sequential Recommendations, Model Optimization, Noisy Data Handling, Real-World Applications, Algorithm Adaptation, Denoising Techniques, Robustness Enhancement. (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/758/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Multi-Modal Dialog State Tracking for Interactive Fashion Recommendation (2022)</h3>
    <p><strong>Authors:</strong> Iadh Ounis, Craig Macdonald, Yaxiong Wu</p>
    <p>Multi-modal interactive recommendation is a type of task that allows users to receive visual recommendations and express natural-language feedback about the recommended items across multiple iterations of interactions. However, such multi-modal dialog sequences (i.e. turns consisting of the system’s visual recommendations and the user’s natural-language feedback) make it challenging to correctly incorporate the users’ preferences across multiple turns. Indeed, the existing formulations of interactive recommender systems suffer from their inability to capture the multi-modal sequential dependencies of textual feedback and visual recommendations because of their use of recurrent neural network-based (i.e., RNN-based) or transformer-based models. To alleviate the multi-modal sequential dependency issue, we propose a novel multi-modal recurrent attention network (MMRAN) model to effectively incorporate the users’ preferences over the long visual dialog sequences of the users’ natural-language feedback and the system’s visual recommendations. Specifically, we leverage a gated recurrent network (GRN) with a feedback gate to separately process the textual and visual representations of natural-language feedback and visual recommendations into hidden states (i.e. representations of the past interactions) for multi-modal sequence combination. In addition, we apply a multi-head attention network (MAN) to refine the hidden states generated by the GRN and to further enhance the model’s ability in dynamic state tracking. Following previous work, we conduct extensive experiments on the Fashion IQ Dresses, Shirts, and Tops & Tees datasets to assess the effectiveness of our proposed model by using a vision-language transformer-based user simulator as a surrogate for real human users. Our results show that our proposed MMRAN model can significantly outperform several existing state-of-the-art baseline models.</p>
    <p><strong>Categories:</strong> Multi-Modal RNNs, Attention Mechanisms, Interactive Recommendation, Fashion, Dialog State Tracking, Multi-Turn Dialog, Vision-Language Models, Real-World Applications, Gated Recurrent Networks, Performance Evaluation (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/774/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Tops, Bottoms, and Shoes: Building Capsule Wardrobes via Cross-Attention Tensor Network (2021)</h3>
    <p><strong>Authors:</strong> Hao Yang, Fei Wang, Yusan Lin, Huiyuan Chen</p>
    <p>Fashion is more than Paris runways. Fashion is about how people express their interests, identity, mood, and cultural influences. Given an inventory of candidate garments from different categories, how to assemble them together would most improve their fashionability? This question presents an intriguing visual recommendation challenge to automatically create capsule wardrobes. Capsule wardrobe generation is a complex combinatorial problem that requires the understanding of how multiple visual items interact. The generative process often needs fashion experts to manually tease the combinations out, making it hard to scale.<br>We introduce TensorNet, an approach that captures the key ingredients of visual compatibility among tops, bottoms, and shoes. TensorNet aims to provide actionable advice for full-body clothing outfits that mix and match well. Our TensorNet consists of two core modules: a Cross-Attention Message Passing module and a Wide&Deep Tensor Interaction module. As such, TensorNet is able to characterize the local region-based patterns as well as the global compatibility of the entire outfits. Our experimental results on the real-word datasets indicate that the proposed method is capable of learning visual compatibility and outperforms all the baselines. TensorNet opens up opportunities for fashion designers to narrow down the search space for multi-clothes combinations.</p>
    <p><strong>Categories:</strong> Neural Networks, Attention Mechanisms, Multi-Item Recommendation, Fashion, Cross-Attention, Retail, Computer Vision, Capsule Wardrobes, Visual Compatibility, Empirical Validation, Wide&amp;Deep Models, Personal Style Assistant (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/671/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Shared Neural Item Representations for Completely Cold Start Problem (2021)</h3>
    <p><strong>Authors:</strong> Guannan Liang, Young-joo Chung, Ramin Raziperchikolaei</p>
    <p>Neural networks have become popular recently in recommendation systems to extract user and item representations. Most previous works follow a two-branch setting, where user and item networks learn user and item representations in the first and second branches, respectively. In the item cold-start problem, where the usage patterns of the items do not exist, the user network uses ID/interaction vector as the input and the item network uses the item side information (content) as the input. In this paper, we will show that by using this structure, two representations are learned for each item in the training set; one is the output of the item network and the other one is hidden inside the user network and is used for learning user representations. Learning two representations makes training slower and optimization more difficult. We propose to unify the two representations and only use the one generated by the item network. Also, we will show how attention mechanisms fit in our setting and how they can improve the quality of the representations. Our results on public and real-world datasets show that our approach converges faster, achieves higher recall in fewer iterations, and is more robust to the changes in the number of training samples compared to the previous works.</p>
    <p><strong>Categories:</strong> Cold Start, Item Cold Start, Representation Learning, Neural Networks, Recommendation Systems, Optimization, Attention Mechanisms, Evaluation Metrics, Deep Learning, Real-World Applications, Performance Improvements (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/662/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Hierarchical Latent Relation Modeling for Collaborative Metric Learning (2021)</h3>
    <p><strong>Authors:</strong> Guillaume Salha-Galvan, Romain Hennequin, Viet-Anh Tran, Manuel Moussallam</p>
    <p>Collaborative Metric Learning (CML) recently emerged as a powerful paradigm for recommendation based on implicit feedback collaborative filtering. However, standard CML methods learn fixed user and item representations, which fails to capture the complex interests of users. Existing extensions of CML also either ignore the heterogeneity of user-item relations, i.e. that a user can simultaneously like very different items, or the latent item-item relations, i.e. that a user’s preference for an item depends, not only on its intrinsic characteristics, but also on items they previously interacted with. In this paper, we present a hierarchical CML model that jointly captures latent user-item and item-item relations from implicit data. Our approach is inspired by translation mechanisms from knowledge graph embedding and leverages memory-based attention networks. We empirically show the relevance of this joint relational modeling, by outperforming existing CML models on recommendation tasks on several real-world datasets. Our experiments also emphasize the limits of current CML relational models on very sparse datasets.</p>
    <p><strong>Categories:</strong> Collaborative Filtering, Recommendation Systems, Latent Relations, Implicit Feedback, User-Item Interactions, Hierarchical Models, Knowledge Graph Embeddings, Attention Mechanisms (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/637/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>SSE-PT: Sequential Recommendation Via Personalized Transformer (2020)</h3>
    <p><strong>Authors:</strong> Shuqing Li, James Sharpnack, Liwei Wu, Cho-Jui Hsieh</p>
    <p>Temporal information is crucial for recommendation problems because user preferences are naturally dynamic in the real world. Recent advances in deep learning, especially the discovery of various attention mechanisms and newer architectures in addition to widely used RNN and CNN in natural language processing, have allowed for better use of the temporal ordering of items that each user has engaged with. In particular, the SASRec model, inspired by the popular Transformer model in natural languages processing, has achieved state-of-the-art results. However, SASRec, just like the original Transformer model, is inherently an un-personalized model and does not include personalized user embeddings. To overcome this limitation, we propose a Personalized Transformer (SSE-PT) model, outperforming SASRec by almost 5% in terms of NDCG@10 on 5 real-world datasets. Furthermore, after examining some random users’ engagement history, we find our model not only more interpretable but also able to focus on recent engagement patterns for each user. Moreover, our SSE-PT model with a slight modification, which we call SSE-PT++, can handle extremely long sequences and outperform SASRec in ranking results with comparable training speed, striking a balance between performance and speed requirements. Our novel application of the Stochastic Shared Embeddings (SSE) regularization is essential to the success of personalization. Code and data are open-sourced at https://github.com/wuliwei9278/SSE-PT.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Sequential Recommendations, Personalization, Transformer Models, Attention Mechanisms, Deep Learning, User Engagement History, Scalability, Model Interpretability, Evaluation Metrics, Real-World Applications, Regularization Techniques (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/554/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Interpretable Contextual Team-aware Item Recommendation: Application in Multiplayer Online Battle Arena Games (2020)</h3>
    <p><strong>Authors:</strong> Vladimir Araujo, Denis Parra, Andrés Villa, Francisca Cattan</p>
    <p>The video game industry has adopted recommendation systems to boost users interest with a focus on game sales. Other exciting applications within video games are those that help the player make decisions that would maximize their playing experience, which is a desirable feature in real-time strategy video games such as Multiplayer Online Battle Arena (MOBA) like as DotA and LoL. Among these tasks, the recommendation of items is challenging, given both the contextual nature of the game and how it exposes the dependence on the formation of each team. Existing works on this topic do not take advantage of all the available contextual match data and dismiss potentially valuable information. To address this problem we develop TTIR, a contextual recommender model derived from the Transformer neural architecture that suggests a set of items to every team member, based on the contexts of teams and roles that describe the match. TTIR outperforms several approaches and provides interpretable recommendations through visualization of attention weights. Our evaluation indicates that both the Transformer architecture and the contextual information are essential to get the best results for this item recommendation task. Furthermore, a preliminary user survey indicates the usefulness of attention weights for explaining recommendations as well as ideas for future work. The code and dataset are available at https://github.com/ojedaf/IC-TIR-Lol .</p>
    <p><strong>Categories:</strong> Video Games, Multiplayer Online Battle Arenas (MOBA), Team-Aware Recommendations, Contextual Recommendations, Transformer Architecture, Interpretable AI, User Survey, Real-World Applications, Deep Learning Models, Attention Mechanisms (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/578/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>MEANTIME: Mixture of Attention Mechanisms with Multi-temporal Embeddings for Sequential Recommendation (2020)</h3>
    <p><strong>Authors:</strong> Sung Min Cho, Eunhyeok Park, Sungjoo Yoo</p>
    <p>Recently, self-attention based models have achieved state-of-the-art performance in sequential recommendation task. Following the custom from language processing, most of these models rely on a simple positional embedding to exploit the sequential nature of the user’s history. However, there are some limitations regarding the current approaches. First, sequential recommendation is different from language processing in that timestamp information is available. Previous models have not made good use of it to extract additional contextual information. Second, using a simple embedding scheme can lead to information bottleneck since the same embedding has to represent all possible contextual biases. Third, since previous models use the same positional embedding in each attention head, they can wastefully learn overlapping patterns. To address these limitations, we propose MEANTIME (MixturE of AtteNTIon mechanisms with Multi-temporal Embeddings) which employs multiple types of temporal embeddings designed to capture various patterns from the user’s behavior sequence, and an attention structure that fully leverages such diversity. Experiments on real-world data show that our proposed method outperforms current state-of-the-art sequential recommendation methods, and we provide an extensive ablation study to analyze how the model gains from the diverse positional information.</p>
    <p><strong>Categories:</strong> Sequential Recommendation, Attention Mechanisms, Temporal Embeddings, User Behavior Analysis, Model Architecture, Cold Start Problem, Evaluation Metrics, Real-World Applications, Diversity of Recommendations, Novel Methods, Hybrid Models (<i>deepseek-r1:70b</i>)</p>
    <p><a href="/recsys-recommender/response/582/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>TAFA: Two-headed Attention Fused Autoencoder for Context-Aware Recommendations (2020)</h3>
    <p><strong>Authors:</strong> Jin Peng Zhou, Felipe Perez, Maksims Volkovs, Zhaoyue Cheng</p>
    <p>Collaborative filtering with implicit feedback is a ubiquitous class of recommendation problems where only positive interactions such as purchases or clicks are observed. Autoencoder-based recommendation models have shown strong performance on many implicit feedback benchmarks. However, these models tend to suffer from popularity bias making recommendations less personalized. User-generated reviews contain a rich source of preference information, often with specific details that are important to each user, and can help mitigate the popularity bias. Since not all reviews are equally useful, existing work has been exploring various forms of attention to distill relevant information. In the majority of proposed approaches, representations from implicit feedback and review branches are simply concatenated at the end to generate predictions. This can prevent the model from learning deeper correlations between the two modalities and affect prediction accuracy. To address these problems, we propose a novel Two-headed Attention Fused Autoencoder (TAFA) model that jointly learns representations from user reviews and implicit feedback to make recommendations. We apply early and late modality fusion which allows the model to fully correlate and extract relevant information from both input sources. To further combat popularity bias, we leverage the Noise Contrastive Estimation (NCE) objective to “de-popularize” the fused user representation via a two-headed decoder architecture. Empirically, we show that TAFA outperforms leading baselines on multiple real-world benchmarks. Moreover, by tracing attention weights back to reviews we can provide explanations for the generated recommendations and gain further insights into user preferences. Full code for this work is available here: https://github.com/layer6ai-labs/TAFA.</p>
    <p><strong>Categories:</strong> Collaborative Filtering, Implicit Feedback, Autoencoder, Popularity Bias, User Reviews, Attention Mechanisms, Early and Late Fusion, Noise Contrastive Estimation (NCE), Real-World Applications, Interpretability/Explainability, Recommendation Systems, Context-Aware Recommendations (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/555/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Explainable Recommendations via Attentive Multi-Persona Collaborative Filtering (2020)</h3>
    <p><strong>Authors:</strong> Avi Caciularu, Oren Barkan, Noam Koenigstein, Yonatan Fuchs</p>
    <p>Two main challenges in recommender systems are modeling users with heterogeneous taste, and providing explainable recommendations. In this paper, we propose the neural Attentive Multi-Persona Collaborative Filtering (AMP-CF) model as a unified solution for both problems. AMP-CF breaks down the user to several latent ‘personas’ (profiles) that identify and discern the different tastes and inclinations of the user. Then, the revealed personas are used to generate and explain the final recommendation list for the user. AMP-CF models users as an attentive mixture of personas, enabling a dynamic user representation that changes based on the item under consideration. We demonstrate AMP-CF on five collaborative filtering datasets from the domains of movies, music, video games and social networks. As an additional contribution, we propose a novel evaluation scheme for comparing the different items in a recommendation list based on the distance from the underlying distribution of “tastes” in the user’s historical items. Experimental results show that AMP-CF is competitive with other state-of-the-art models. Finally, we provide qualitative results to showcase the ability of AMP-CF to explain its recommendations.</p>
    <p><strong>Categories:</strong> Explainable Recommendations, Collaborative Filtering, User Modeling, Multi-Persona Models, Domain-Specific Recommendations (Movies, Music, Video Games, Social Networks), Evaluation Metrics, Attention Mechanisms, Deep Learning, Performance Comparison, Beyond Accuracy, User-Centric Design (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/576/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Tripartite Heterogeneous Graph Propagation for Large-scale Social Recommendation (2019)</h3>
    <p><strong>Authors:</strong> Donghyun Kwak, Minkyu Kim, Hanock Kwak, Jae-Han Cho, Nako Sung, Jung-Woo Ha, Kyung-Min Kim, Sangkwon Sim, Young-Jin Park, Jihun Kwon</p>
    <p>Graph Neural Networks (GNNs) have been emerging as a promising method for relational representation including recommender systems. However, various challenging issues of social graphs hinder the practical usage of GNNs for social recommendation, such as their complex noisy connections and high heterogeneity. The oversmoothing of GNNs is an obstacle of GNN-based social recommendation as well. Here we propose a new graph embedding method Heterogeneous Graph Propagation (HGP) to tackle these issues. HGP uses a group-user-item tripartite graph as input to reduce the number of edges and the complexity of paths in a social graph. To solve the oversmoothing issue, HGP embeds nodes under a personalized PageRank based propagation scheme, separately for group-user graph and user-item graph. Node embeddings from each graph are integrated using an attention mechanism. We evaluate our HGP on a large-scale real-world dataset consisting of 1,645,279 nodes and 4,711,208 edges. The experimental results show that HGP outperforms several baselines in terms of AUC and F1-score metrics.</p>
    <p><strong>Categories:</strong> Graph Neural Networks, Social Networks, Recommendation Systems, Graph-based Methods, Heterogeneous Graphs, Propagation Techniques, Large-scale Data, Attention Mechanisms, Evaluation Metrics, Real-world Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/523/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>