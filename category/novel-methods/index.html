<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">Novel Methods</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/bias-mitigation/">Bias Mitigation</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/scalability/">Scalability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Pay Attention to Attention for Sequential Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Xiaojing Liu, Yuli Liu, Min Liu</p>
    <p>Transformer-based approaches have demonstrated remarkable success in various sequence-based tasks. However, traditional self-attention models may not sufficiently capture the intricate dependencies within items in sequential recommendation scenarios. This is due to the lack of explicit emphasis on attention weights, which play a critical role in allocating attention and understanding item-to-item correlations. To better exploit the potential of attention weights and improve the capability of sequential recommendation in learning high-order dependencies, we propose a novel sequential recommendation (SR) approach called attention weight refinement (AWRSR). AWRSR enhances the effectiveness of self-attention by additionally paying attention to attention weights, allowing for more refined attention distributions of correlations among items. We conduct comprehensive experiments on multiple real-world datasets, demonstrating that our approach consistently outperforms state-of-the-art SR models. Moreover, we provide a thorough analysis of AWRSR’s effectiveness in capturing higher-level dependencies. These findings suggest that AWRSR offers a promising new direction for enhancing the performance of self-attention architecture in SR tasks, with potential applications in other sequence-based problems as well.</p>
    <p><strong>Categories:</strong> Sequential Recommendations, Attention Mechanisms, Transformer-Based Models, Recommendation Systems, Higher-Order Dependencies, Model Performance, Experimental Analysis, Item Correlations, Real-World Applications, Novel Methods, Attention Weight Refinement, Self-Attention Architecture, Machine Learning for Recommendations, Potential Applications in Other Domains (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1104/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Enhancing Performance and Scalability of Large-Scale Recommendation Systems with Jagged Flash Attention (2024)</h3>
    <p><strong>Authors:</strong> Hong Li, Mingwei Tang, Meng Liu, Junjie Yang, Dai Li, Xing Liu, Tunhou Zhang, Arnold Overwijk, Haoci Zhang, Rengan Xu, Sijia Chen, Sri Reddy, Devashish Shankar, Jiaqi Zhai, Bill Zhu, Boyang Li, Zehua Zhang, Yifan Xu, Yuxi Hu</p>
    <p>The integration of hardware accelerators has significantly advanced the capabilities of modern recommendation systems, enabling the exploration of complex ranking paradigms previously deemed impractical. However, the GPU-based computational costs present substantial challenges. In this paper, we demonstrate our development of an efficiency-driven approach to explore these paradigms, moving beyond traditional reliance on native PyTorch modules. We address the specific challenges posed by ranking models’ dependence on categorical features, which vary in length and complicate GPU utilization. We introduce Jagged Feature Interaction Kernels, a novel method designed to extract fine-grained insights from long categorical features through efficient handling of dynamically sized tensors. We further enhance the performance of attention mechanisms by integrating Jagged tensors with Flash Attention. As the feature length grows, the Jagged Flash Attention is able to scale memory linearly rather than quadratically. Our experimental results demonstrate that Jagged Flash Attention achieves speedups of 2.4× to 5.6× over dense attention and reduces memory usage by up to 21.8×. This allows to scale the recommendation systems with longer features and more complex model architecture.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Scalability, GPU Usage, Categorical Features, Efficient Algorithms, Flash Attention, Memory Efficiency, Hardware Acceleration, Performance Optimization, Large-Scale Systems, Attention Mechanisms, Novel Methods, Efficiency-Driven Approaches, Complex Model Architectures. (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1163/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>BRUCE – Bundle Recommendation Using Contextualized item Embeddings (2022)</h3>
    <p><strong>Authors:</strong> Amit Livne, Bracha Shapira, Oren Sar Shalom, Mark Last, Tzoof Avny Brosh</p>
    <p>A bundle is a pre-defined set of items that are collected together. In many domains, bundling is one of the most important marketing strategies for item promotion, commonly used in e-commerce. Bundle recommendation resembles the item recommendation task, where bundles are the recommended unit, but it poses additional challenges; while item recommendation requires only user and item understanding, bundle recommendation also requires modeling the connections between the various items in a bundle. Transformers have driven the state-of-the-art methods for set and sequence modeling in various natural language processing and computer vision tasks, emphasizing the understanding that the neighbors of an element are of crucial importance. Under some required adjustments, we believe the same applies for items in bundles, and better capturing the relations of an item with other items in the bundle may lead to improved recommendations. To address that, we introduce BRUCE - a novel model for bundle recommendation, in which we adapt Transformers to represent data on users, items, and bundles. This allows exploiting the self-attention mechanism to model the following: latent relations between the items in a bundle; and users’ preferences toward each of the items in the bundle and toward the whole bundle. Moreover, we examine various architectures to integrate the items’ and the users’ information and provide insights on architecture selection based on data characteristics. Experiments conducted on three benchmark datasets show that the proposed approach contributes to the accuracy of the recommendation and substantially outperforms state-of-the-art methods</p>
    <p><strong>Categories:</strong> Bundle Recommendation, E-commerce, Recommendation Systems, Transformers, Attention Mechanism, Item Relationships, User Preferences, Model Architecture, Embeddings, Neighbor Modeling, Recommendation Accuracy, Novel Methods, Real-world Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/750/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>MEANTIME: Mixture of Attention Mechanisms with Multi-temporal Embeddings for Sequential Recommendation (2020)</h3>
    <p><strong>Authors:</strong> Sung Min Cho, Eunhyeok Park, Sungjoo Yoo</p>
    <p>Recently, self-attention based models have achieved state-of-the-art performance in sequential recommendation task. Following the custom from language processing, most of these models rely on a simple positional embedding to exploit the sequential nature of the user’s history. However, there are some limitations regarding the current approaches. First, sequential recommendation is different from language processing in that timestamp information is available. Previous models have not made good use of it to extract additional contextual information. Second, using a simple embedding scheme can lead to information bottleneck since the same embedding has to represent all possible contextual biases. Third, since previous models use the same positional embedding in each attention head, they can wastefully learn overlapping patterns. To address these limitations, we propose MEANTIME (MixturE of AtteNTIon mechanisms with Multi-temporal Embeddings) which employs multiple types of temporal embeddings designed to capture various patterns from the user’s behavior sequence, and an attention structure that fully leverages such diversity. Experiments on real-world data show that our proposed method outperforms current state-of-the-art sequential recommendation methods, and we provide an extensive ablation study to analyze how the model gains from the diverse positional information.</p>
    <p><strong>Categories:</strong> Sequential Recommendation, Attention Mechanisms, Temporal Embeddings, User Behavior Analysis, Model Architecture, Cold Start Problem, Evaluation Metrics, Real-World Applications, Diversity of Recommendations, Novel Methods, Hybrid Models (<i>deepseek-r1:70b</i>)</p>
    <p><a href="/recsys-recommender/response/582/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Music Playlist Recommendation via Preference Embedding (2016)</h3>
    <p><strong>Authors:</strong> Chih-Chun Hsia, Ming-Feng Tsai, Yian Chen, Chih-Ming Chen, Chun-Yao Yang</p>
    <p>Music playlists usually contain some particular musical styles or atmospheres in which users would like to be involved. Music streaming services, such as Spotify, Apple Music, and KKBOX, even allow users to edit and listen to playlists online. While it has been some well-known methods that can nicely model the preference between users and songs, little has been done in the literature to recommend music playlists, each of which can be considered as a set of many individual songs, to users. In the light of this, this paper proposes a preference embedding based on a user-song-playlist graph to learn the preference representations of these three entities. After the embedding process, we then use the learned representations to perform the task of playlist recommendation. Experiments conducted on a real-world dataset show that the proposed embedding method outperforms the baseline of popularity; in addition, we also make a comparison with DeepWalk and LINE for the recommendation task, and the results show that the proposed method can stand comparison with the two state-of-the-art graph embedding techniques.</p>
    <p><strong>Categories:</strong> Graph-Based Methods, Preference Embedding, Music Recommendation, Playlist Recommendation, Set Recommendations, User-Song-Playlist Graph, DeepWalk, LINE, Evaluation, Performance Comparison, Real-World Applications, Scalability, Music Streaming Services, Recommendation Accuracy, Novel Methods. (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/250/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>