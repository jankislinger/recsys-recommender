<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">Efficiency</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/offline-evaluation/">Offline Evaluation</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/scalability/">Scalability</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Neighborhood-Based Collaborative Filtering for Conversational Recommendation (2024)</h3>
    <p><strong>Authors:</strong> Julian Mcauley, Zhouhang Xie, Nathan Kallus, Dawen Liang, Rahul Jha, Hyunsik Jeon, Zhankui He, Harald Steck, Junda Wu</p>
    <p>Conversational recommender systems (CRS) should understand users’ expressed interests that are frequently semantically rich and knowledge intensive. Prior works attempt to address this challenge by making use of external knowledge bases or parametric knowledge in large language models (LLMs). In this paper, we study a complementary solution, exploiting item knowledge in the training data. We hypothesise that many inference-time user requests can be answered via reusing popular crowd-written answers associated with similar training queries. Following this intuition, we define a class of neighborhood-based CRS that make recommendations by identifying popular items associated with similar training dialogue contexts. Experiments on Inspired, Redial, and Reddit benchmarks show that despite its simplicity, our method achieves comparable to better performance than state-of-the-art LLM-based methods with over 200 times more parameters. We also show neighborhood and model-based predictions can be combined to achieve further performance improvements over both components.</p>
    <p><strong>Categories:</strong> Collaborative Filtering, Conversational Recommender Systems, Neighborhood-Based Methods, Similarity-Based Recommendations, Hybrid Recommendations, Performance Evaluation, Recommendation Quality, Crowd-Written Answers, Item Knowledge Utilization, Beyond Accuracy, Efficiency, Recommendation Systems (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1100/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>One-class recommendation systems  with  the hinge pairwise distance loss and orthogonal representations (2024)</h3>
    <p><strong>Authors:</strong> Ramin Raziperchikolaei, Young-joo Chung</p>
    <p>In one-class recommendation systems, the goal is to learn a model from a small set of interacted users and items and then identify the positively-related (i.e., similar) user-item pairs among a large number of pairs with unknown interactions. Most loss functions in the literature rely on dissimilar pairs of users and items, which are selected from the ones with unknown interactions, to obtain better prediction performance. The main issue of this strategy is that it needs a large number of dissimilar pairs, which increases the training time significantly. In this paper, the goal is to only use the similar set to train the models and discard the dissimilar set.  We highlight three trivial solutions that the models converge to when they are trained only on similar pairs: collapsed, dimensional collapsed, and shrinking solutions. We propose a hinge pairwise loss and an orthogonality term that can be added to the objective functions in the literature to avoid these trivial solutions. We conduct experiments on various tasks on public and real-world datasets, which show that our approach using only similar pairs can be trained several times faster than the state-of-the-art methods while achieving competitive results.</p>
    <p><strong>Categories:</strong> Recommendation Systems, One-class Learning, Algorithm Design, Loss Function Design, Optimization Techniques, User-Item Interaction, Similarity-based Recommendations, Cold Start Problem, Efficiency, Real-world Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1102/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Joint Modeling of Search and Recommendations Via an Unified Contextual Recommender (UniCoRn) (2024)</h3>
    <p><strong>Authors:</strong> Sudarshan Lamkhede, Vito Ostuni, Moumita Bhattacharya</p>
    <p>Search and recommendation systems are essential in many services,and they are often developed separately, leading to complex maintenance and technical debt. In this paper, we present a unified deep learning model that efficiently handles key aspects of both tasks</p>
    <p><strong>Categories:</strong> Search, Recommendation Systems, Unified Models, Deep Learning, Efficiency, Contextual Recommendations, System Integration, Technical Debt Reduction (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1168/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Gradient Matching for Categorical Data Distillation in CTR Prediction (2023)</h3>
    <p><strong>Authors:</strong> Ruixuan Li, Zhenhua Dong, Jiacheng Sun, Cheng Wang, Rui Zhang</p>
    <p>The cost of hardware and energy consumption on training a click-through rate (CTR) model is highly prohibitive. A recent promising direction for reducing such costs is data distillation with gradient matching, which aims to synthesize a small distilled dataset to guide the model to a similar parameter space as those trained on real data. However, there are two main challenges to implementing such a method in the recommendation field: (1) The categorical recommended data are high dimensional and sparse one- or multi-hot data which will block the gradient flow, causing backpropagation-based data distillation invalid. (2) The data distillation process with gradient matching is computationally expensive due to the bi-level optimization. To this end, we investigate efficient data distillation tailored for recommendation data with plenty of side information where we formulate the discrete data to the dense and continuous data format. Then, we further introduce a one-step gradient matching scheme, which performs gradient matching for only a single step to overcome the inefficient training process. The overall proposed method is called Categorical data distillation with Gradient Matching (CGM), which is capable of distilling a large dataset into a small of informative synthetic data for training CTR models from scratch. Experimental results show that our proposed method not only outperforms the state-of-the-art coreset selection and data distillation methods but also has remarkable cross-architecture performance. Moreover, we explore the application of CGM on continual updating and mitigate the effect of different random seeds on the training results.</p>
    <p><strong>Categories:</strong> Categorical Data, Gradient Matching, Click-Through Rate (CTR), Recommendation Systems, Data Distillation, High Dimensional Data, Sparse Data, Dense Representation, Bi-Level Optimization, Continual Learning, Practical Applications, Efficiency, Scalability, Data Transformation. (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/870/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Nonlinear Bandits Exploration for Recommendations (2023)</h3>
    <p><strong>Authors:</strong> Minmin Chen, Yi Su</p>
    <p>The paradigm of framing recommendations as (sequential) decision-making processes has gained significant interest. To achieve long-term user satisfaction, these interactive systems need to strikes a balance between exploitation (recommending high-reward items) and exploration (exploring uncertain regions for potentially better items). Classical bandit algorithms like Upper-Confidence-Bound and Thompson Sampling, and their contextual extensions with linear payoffs have exhibited strong theoretical guarantees and empirical success in managing the exploration-exploitation trade-off. Building efficient exploration-based systems for deep neural network powered real-world, large-scale industrial recommender systems remains under studied. In addition, these systems are often multi-stage, multi-objective and response time sensitive.  In this talk, we share our experience in addressing these challenges in building exploration based industrial recommender systems. Specifically, we adopt the Neural Linear Bandit algorithm, which effectively combines the representation power of deep neural networks, with the simplicity of linear bandits to incorporate exploration in DNN based recommender systems. We introduce  exploration capability to both the nomination and ranking stage of the industrial recommender system.  In the context of the ranking stage, we delve into the extension of this algorithm to accommodate the multi-task setup, enabling exploration in systems with multiple objectives. Moving on to the nomination stage, we will address the development of efficient bandit algorithms tailored to factorized bi-linear models. These algorithms play a crucial role in facilitating maximum inner product search, which is commonly employed in large-scale retrieval systems. We validate our algorithms and present findings from real-world live experiments.</p>
    <p><strong>Categories:</strong> Multi-Armed Bandits, Recommendation Systems, Deep Learning, Exploration, Exploitation, Neural Linear Bandit, Industrial Recommender Systems, Nomination Stage, Ranking Stage, Multi-Task Learning, Large Scale Systems, Real World Applications, Efficiency, Beyond Accuracy, Evaluation Metrics, Performance Evaluation, Algorithm Performance (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1012/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Integrating Offline Reinforcement Learning with Transformers for Sequential Recommendation (2023)</h3>
    <p><strong>Authors:</strong> Xumei Xi, Yang Wu, Liwen Ouyang, Yuke Zhao, Quan Liu</p>
    <p>We consider the problem of sequential recommendation, where the current recommendation is made based on past interactions. This recommendation task requires efficient processing of the sequential data and aims to provide recommendations that maximize the long-term reward. To this end, we train a farsighted recommender by using an offline RL algorithm with the policy network in our model architecture that has been initialized from a pre-trained transformer model. The pre-trained model leverages the superb ability of the transformer to process sequential information. Compared to prior works that rely on online interaction via simulation, we focus on implementing a fully offline RL framework that is able to converge in a fast and stable way. Through extensive experiments on public datasets, we show that our method is robust across various recommendation regimes, including e-commerce and movie suggestions. Compared to state-of-the-art supervised learning algorithms, our algorithm yields recommendations of higher quality, demonstrating the clear advantage of combining RL and transformers.</p>
    <p><strong>Categories:</strong> Reinforcement Learning, Transformers, Sequential Recommendation, Offline Reinforcement Learning, Sequential Data Processing, Transfer Learning, E-commerce, Movies, Recommendation System, Long-term Reward Maximization, Algorithm Comparison, Efficiency (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/955/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Exploring Unlearning Methods to Ensure the Privacy, Security, and Usability of Recommender Systems (2023)</h3>
    <p><strong>Authors:</strong> Jens Leysen</p>
    <p>Machine learning algorithms have proven highly effective in analyzing large amounts of data and identifying complex patterns and relationships. One application of machine learning that has received significant attention in recent years is recommender systems, which are algorithms that analyze user behavior and other data to suggest items or content that a user may be interested in. However useful, these systems may unintentionally retain sensitive, outdated, or faulty information. Posing a risk to user privacy, system security, and limiting a system’s usability. In this research proposal, we aim to address these challenges by investigating methods for machine “unlearning”, which would allow information to be efficiently “forgotten” or “unlearned” from machine learning models. The main objective of this proposal is to develop the foundation for future machine unlearning methods. We first evaluate current unlearning methods and explore novel adversarial attacks on these methods’ verifiability, efficiency, and accuracy to gain new insights and further develop the theory of machine unlearning. Using our gathered insights, we seek to create novel unlearning methods that are verifiable, efficient, and limit unnecessary accuracy degradation. Through this research, we seek to make significant contributions to the theoretical foundations of machine unlearning while also developing unlearning methods that can be applied to real-world problems.</p>
    <p><strong>Categories:</strong> Unlearning Methods, Privacy, Security, Recommender Systems, Machine Learning, Data Privacy, User Behavior, Adversarial Attacks, Theoretical Foundations, Applications, Efficiency (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/981/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Effective and Efficient Training for Sequential Recommendation using Recency Sampling (2022)</h3>
    <p><strong>Authors:</strong> Aleksandr Petrov, Craig Macdonald</p>
    <p>Many modern sequential recommender systems use deep neural networks, which can effectively estimate the relevance of items but require a lot of time to train. Slow training increases expenses, hinders product development timescales and prevents the model from being regularly updated to adapt to changing user preferences. Training such sequential models involves appropriately sampling past user interactions to create a realistic training objective. The existing training objectives have limitations. For instance, next item prediction never uses the beginning of the sequence as a learning target, thereby potentially discarding valuable data. On the other hand, the item masking used by BERT4Rec is only weakly related to the goal of the sequential recommendation; therefore, it requires much more time to obtain an effective model. Hence, we propose a novel Recency-based Sampling of Sequences training objective that addresses both limitations. We apply our method to various recent and state-of-the-art model architectures – such as GRU4Rec, Caser, and SASRec. We show that the models enhanced with our method can achieve performances exceeding or very close to state-of-the-art BERT4Rec, but with much less training time.</p>
    <p><strong>Categories:</strong> Sequential Recommendations, Training Techniques, Recency Sampling, Model Architecture, Efficiency, Scalability, Algorithm Improvements, Optimization, Deep Learning, Real-World Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/764/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Fast Multi-Step Critiquing for VAE-based Recommender Systems (2021)</h3>
    <p><strong>Authors:</strong> Boi Faltings, Diego Antognini</p>
    <p>Recent studies have shown that providing personalized explanations alongside recommendations increases trust and perceived quality. Furthermore, it gives users an opportunity to refine the recommendations by critiquing parts of the explanations. On one hand, current recommender systems model the recommendation, explanation, and critiquing objectives jointly, but this creates an inherent trade-off between their respective performance. On the other hand, although recent latent linear critiquing approaches are built upon an existing recommender system, they suffer from computational inefficiency at inference due to the objective optimized at each conversation’s turn. We address these deficiencies with M&Ms-VAE, a novel variational autoencoder for recommendation and explanation that is based on multimodal modeling assumptions. We train the model under a weak supervision scheme to simulate both fully and partially observed variables. Then, we leverage the generalization ability of a trained M&Ms-VAE model to embed the user preference and the critique separately. Our work’s most important innovation is our critiquing module, which is built upon and trained in a self-supervised manner with a simple ranking objective. Experiments on four real-world datasets demonstrate that among state-of-the-art models, our system is the first to dominate or match the performance in terms of recommendation, explanation, and multi-step critiquing. Moreover, M&Ms-VAE processes the critiques up to 25.6x faster than the best baselines. Finally, we show that our model infers coherent joint and cross generation, even under weak supervision, thanks to our multimodal-based modeling and training scheme.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Variational Autoencoder (VAE), Explainability, User Interaction, Multi-Modal Modeling, Weak Supervision, Self-Supervised Learning, Scalability, Efficiency, Inference Methods, AI/ML Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/638/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Neural Collaborative Filtering vs. Matrix Factorization Revisited (2020)</h3>
    <p><strong>Authors:</strong> John Anderson, Steffen Rendle, Walid Krichene, Li Zhang</p>
    <p>Embedding based models have been the state of the art in collaborative filtering for over a decade. Traditionally, the dot product or higher order equivalents have been used to combine two or more embeddings, e.g., most notably in matrix factorization. In recent years, it was suggested to replace the dot product with a learned similarity e.g. using a multilayer perceptron (MLP). This approach is often referred to as neural collaborative filtering (NCF). In this work, we revisit the experiments of the NCF paper that popularized learned similarities using MLPs. First, we show that with a proper hyperparameter selection, a simple dot product substantially outperforms the proposed learned similarities. Second, while a MLP can in theory approximate any function, we show that it is non-trivial to learn a dot product with an MLP. Finally, we discuss practical issues that arise when applying MLP based similarities and show that MLPs are too costly to use for item recommendation in production environments while dot products allow to apply very efficient retrieval algorithms. We conclude that MLPs should be used with care as embedding combiner and that dot products might be a better default choice.</p>
    <p><strong>Categories:</strong> Neural Collaborative Filtering, Matrix Factorization, Algorithm Comparison, Hyperparameters, Recommendation Systems, Evaluation Metrics, Production Systems, Efficiency, Cost Analysis, Traditional Methods vs. Neural Methods, Practical Recommendations (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/590/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>A Generative Model for Review-Based Recommendations (2019)</h3>
    <p><strong>Authors:</strong> Guy Uziel, Amir Kantor, Oren Sar Shalom</p>
    <p>User generated reviews is a highly informative source of information, that has recently gained lots of attention in the recommender systems community. In this work we propose a generative latent variable model that explains both observed ratings and textual reviews. This latent variable model allows to combine any traditional collaborative filtering method, together with any deep learning architecture for text processing. Experimental results on four benchmark datasets demonstrate its superiority comparing to all baseline recommender systems. Furthermore, a running time analysis shows that this approach is in order of magnitude faster that relevant baselines. i>Presentation: Tuesday Poster Lunch</i</p>
    <p><strong>Categories:</strong> Generative Models, Recommender Systems, Collaborative Filtering, Text Processing, Reviews, Evaluation Metrics, Latent Variable Models, Multimodal Data, Recommendation Accuracy, Efficiency (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/459/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Pick &amp; Merge: An Efficient Item Filtering Scheme for Windows Store Recommendations (2019)</h3>
    <p><strong>Authors:</strong> Nir Nice, Adi Makmal, Liron Allerhand, Jonathan Ephrath, Hilik Berezin, Noam Koenigstein</p>
    <p>Microsoft Windows is the most popular operating system (OS) for personal computers (PCs). With hundreds of millions of users, its app marketplace, Windows Store, is one of the largest in the world. As such, special considerations are required in order to improve online computational efficiency and response times. This paper presents the results of an extensive research of effective filtering method for semi-personalized recommendations. The filtering problem, defined here for the first time, addresses an aspect that was so far largely overlooked by the recommender systems literature, namely effective and efficient method for removing items from semi-personalized recommendation lists. Semi-personalized recommendation lists serve a common list to a group of people based on their shared interest or background. Unlike fully personalized lists, these lists are cacheable and constitute the majority of recommendation lists in many online stores. This motivates the following question: can we remove (most of) the users’ undesired items without collapsing onto fully personalized recommendations?Our solution is based on dividing the users into few subgroups, such that each subgroup receives a different variant of the original recommendation list. This approach adheres to the principles of semi-personalization and hence preserves simplicity and cacheability. We formalize the problem of finding optimal subgroups that minimize the total number of filtering errors, and show that it is combinatorially formidable. Consequently, a greedy algorithm is proposed that filters out most of the undesired items, while bounding the maximal number of errors for each user. Finally, a detailed evaluation of the proposed algorithm is presented using both proprietary and public datasets. i>Presentation: Wednesday Poster Lunch</i</p>
    <p><strong>Categories:</strong> Recommendation Systems, App Recommendations, Marketplace Recommendations, Semi-Personalized Recommendations, Filtering Methods, Greedy Algorithm, Scalability, Efficiency, User Grouping, Real-World Application, Evaluation Analysis, Error Minimization (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/486/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>AnnoMathTeX – a Formula Identifier Annotation Recommender System for STEM Documents (2019)</h3>
    <p><strong>Authors:</strong> Bela Gipp, Corinna Breitinger, Joeran Beel, Ian Mackerracher, Philipp Scharpf, Moritz Schubotz</p>
    <p>Documents from science, technology, engineering and mathematics (STEM) often contain a large number of mathematical formulae alongside text. Semantic search, recommender, and question answering systems require the occurring formula constants and variables (identifiers) to be disambiguated. We present a first implementation of a recommender system that enables and accelerates formula annotation by displaying the most likely candidates for formula and identifier names from four different sources (arXiv, Wikipedia, Wikidata, or the surrounding text). A first evaluation shows that in total, 78% of the formula identifier name recommendations were accepted by the user as a suitable annotation. Furthermore, document-wide annotation saved the user the annotation of ten times more other identifier occurrences. Our long-term vision is to integrate the annotation recommender into the edit-view of Wikipedia and the online LaTeX editor Overleaf.</p>
    <p><strong>Categories:</strong> Recommendation System, Formula Annotation, Collaborative Filtering, STEM Documents, User Acceptance, Efficiency, Semantic Search, Real-World Applications, Multi-source Recommendation, Automated Annotation, Question Answering, Wikipedia Integration (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/500/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Greedy Optimized Multileaving for Personalization (2019)</h3>
    <p><strong>Authors:</strong> Kojiro Iizuka, Yoshifumi Seki, Takeshi Yoneda</p>
    <p>Personalization plays an important role in many services. To evaluate personalized rankings, online evaluation, such as A/B testing, is widely used today. Recently, multileaving has been found to be an efficient method for evaluating rankings in information retrieval fields. This paper describes the first attempt to optimize the multileaving method for personalization settings. We clarify the challenges of applying this method to personalized rankings. Then, to solve these challenges, we propose greedy optimized multileaving (GOM) with a new credit feedback function. The empirical results showed that GOM was stable for increasing ranking lengths and the number of rankers. We implemented GOM on our actual news recommender systems, and compared its online performance. The results showed that GOM evaluated the personalized rankings precisely, with significantly smaller sample sizes (< 1/10) than A/B testing. i>Presentation: Tuesday Poster Lunch</i</p>
    <p><strong>Categories:</strong> Greedy Algorithm, Multileaving, News Recommender Systems, Personalization, Evaluation Metrics, Efficiency, Precision, A/B Testing, Credit Feedback Function, Stability, Scalability, Ranking, New Algorithm, Real-World Application, Evaluation Challenges, Recommender Systems (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/476/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Preference Elicitation as an Optimization Problem (2018)</h3>
    <p><strong>Authors:</strong> Filip Radlinski, Maarten de Rijke, Julia Kiseleva, Anna Sepliarskaia</p>
    <p>The new user cold-start problem arises when a recommender system does not yet have any information about a user. A common solution to this problem is to generate a user profile as part of the sign-up process, by asking the user to rate several items. We propose a new elicitation method to generate a static preference questionnaire (SPQ) that asks a new user to make pairwise comparisons between items by posing relative preference questions. Using a latent factor model, SPQ improves personalized recommendations by choosing a minimal and diverse set of static preference questions to ask any new user. We are the first to rigorously prove which optimization task should be solved in order to select the next preference question for static questionnaires. Our theoretical results are confirmed by extensive experimentation. We test the performance of SPQ on two real-world datasets, under two experimental conditions: simulated, when users behave according to LFM, and real, in which there is no user rating model. SPQ reduces the questionnaire length that is necessary to make accurate recommendations for new users by up to a factor of three compared to state-of-the-art preference elicitation methods. Moreover, solving the right optimization task, SPQ shows better performance than baselines with dynamically generated questions.</p>
    <p><strong>Categories:</strong> Cold Start, Recommendation Systems, User Profiling, Preference Elicitation, Optimization, Latent Factor Models, Real-World Applications, Experiments, Datasets, Evaluation Metrics, Performance Comparison, Efficiency, Pairwise Comparisons (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/357/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Interactive Recommendation via Deep Neural Memory Augmented Contextual Bandits (2018)</h3>
    <p><strong>Authors:</strong> Yue Deng, Avik Ray, Yilin Shen, Hongxia Jin</p>
    <p>Personalized recommendation with user interactions has become increasingly popular nowadays in many applications with dynamic change of contents (news, media, etc.). Existing approaches model user interactive recommendation as a contextual bandit problem to balance the trade-off between exploration and exploitation. However, these solutions require a large number of interactions with each user to provide high quality personalized recommendations. To mitigate this limitation, we design a novel deep neural memory augmented mechanism to model and track the history state for each user based on his previous interactions. As such, the user’s preferences on new items can be quickly learned within a small number of interactions. Moreover, we develop new algorithms to leverage large amount of all users’ history data for offline model training and online model fine tuning for each user with the focus of policy evaluation. Extensive experiments on different synthetic and real-world datasets validate that our proposed approach consistently outperforms a variety of state-of-the-art approaches.</p>
    <p><strong>Categories:</strong> Contextual Bandits, Deep Neural Networks, Interactive Recommendation, News, Media, Scalability, Efficiency, Offline Training, Online Fine-Tuning, Real-World Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/341/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>