<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/offline-evaluation/">Offline Evaluation</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/real-world-applications/">Real-world Applications</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/explainability/">Explainability</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Improving Data Efficiency for Recommenders and LLMs (2024)</h3>
    <p><strong>Authors:</strong> James Caverlee, Noveen Sachdeva, Jianmo Ni, Benjamin Coleman, Ed Chi, Lichan Hong, Wang-Cheng Kang, Derek Cheng</p>
    <p>In recent years, massive transformer-based architectures have driven breakthrough performance in practical applications like autoregressive text-generation (LLMs) and click-prediction (recommenders). A common recipe for success is to train large models on massive web-scale datasets, e.g., modern recommenders are trained on billions of user-item click events, and LLMs are trained on trillions of tokens extracted from the public internet. We are close to hitting the computational and economical limits of scaling up the size of these models, and we expect the next frontier of gains to come from improving the: (i) data quality of the training dataset, and (ii) data efficiency of the extremely expensive training procedure. Inspired by this shift, we present a set of “data-centric” techniques for recommendation and language models that summarizes a dataset into a terse data summary, which is both (i) high-quality, i.e., trains better quality models, and (ii) improves the data-efficiency of the overall training procedure. We propose techniques from two disparate data frameworks: (i) data selection (a.k.a., coreset construction) methods that sample portions of the dataset using grounded heuristics, and(ii) data distillation techniques that generate synthetic examples which are optimized to retain the signals needed for training high-quality models. Overall, this work sheds light on the challenges and opportunities offered by data optimization in web-scale systems, a particularly relevant focus as the recommendation community grapples with the grand challenge of leveraging LLMs.</p>
    <p><strong>Categories:</strong> Recommender Systems, Large Language Models (LLMs), Web-Scale Systems, Data Efficiency, Data Quality, Data Selection, Coreset Construction, Data Distillation, Synthetic Data, High-Quality Datasets, Cross-Domain Applications, Web-Scale Optimization, Recommender-LLM Integration (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1161/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Demystifying Recommender Systems: A Multi-faceted Examination of Explanation Generation, Impact, and Perception (2023)</h3>
    <p><strong>Authors:</strong> Giacomo Balloccu</p>
    <p>Recommender systems have become an integral component of the digital landscape, impacting a multitude of services and industries ranging from e-commerce to entertainment and beyond. By offering personalised suggestions, these systems challenge a fundamental problem in our modern information society named information overload. As users face a deluge of choices, recommender systems help sift through this immense sea of possibilities, delivering a personalised subset of options that align with user preferences and historical behaviour. However, despite their considerable utility, recommender systems often operate as “black boxes,” obscuring the rationale behind recommendations. This opacity can engender mistrust and undermine user engagement, thus attenuating the overall effectiveness of the system. Researchers have emphasized the importance of explanations in recommender systems, highlighting how explanations can enhance system transparency, foster user trust, and improve decision-making processes, thereby enriching user experiences and yielding potential business benefits. Yet, a significant gap persists in the current state of human-understandable explanations research. While recommender systems have grown increasingly complex, our capacity to generate clear, concise, and relevant explanations that reflect this complexity remains limited. Crafting explanations that are both understandable and reflective of sophisticated algorithmic decision-making processes poses a significant challenge, especially in a manner that meets the user’s cognitive and contextual needs.</p>
    <p><strong>Categories:</strong> Explainability, User Trust, Information Overload, Transparency, Artificial Intelligence, Human-Computer Interaction, Personalization, Relevance, Algorithmic Transparency, Cross-Domain Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/975/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Content-based book recommendations (2021)</h3>
    <p><strong>Authors:</strong> Niels Bogaards</p>
    <p>The content-based book recommendation approach developed by Bookarang yields many improvements over existing systems, not only in recommendation quality and consistency, but also in flexibility, depth and transparency. Content-based book recommenders can be implemented in a large array of products, from personalised newsletters to conversational AI systems.</p>
    <p><strong>Categories:</strong> Content-Based Filtering, Book Recommendations, Personalization, Recommendation Techniques, Scalability, Real-World Applications, Industry Use Cases, Beyond Accuracy, Books, NLP, Cross-Domain Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/731/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Deep Generative Ranking for Personalized Recommendation (2019)</h3>
    <p><strong>Authors:</strong> Huafeng Liu, Jingxuan Wen, Jian Yu, Liping Jing</p>
    <p>Recommender systems offer critical services in the age of mass information. Personalized ranking have been attractive both for content providers and customers due to its ability of creating a user-specific ranking on the item set. Although the powerful factor-analysis methods including latent factor model and deep neural network models have achieved promising results, they still suffer from the challenging issues, such as sparsity of recommendation data, uncertainty of optimization, and etc. To enhance the accuracy and generalization of recommender system, in this paper, we propose a deep generative ranking (DGR) model under the Wasserstein auto-encoder framework. Specifically, DGR simultaneously generates the pointwise implicit feedback data (via a Beta-Bernoulli distribution) and creates the pairwise ranking list by sufficient exploiting both interacted and non-interacted items for each user. DGR can be efficiently inferred by minimizing its penalized evidence lower bound. Meanwhile, we theoretically analyze the generalization error bounds of DGR model to guarantee its performance in extremely sparse feedback data. A series of experiments on four large-scale datasets (Movielens (20M), Netflix, Epinions and Yelp in movie, product and business domains) have been conducted. By comparing with the state-of-the-art methods, the experimental results demonstrate that DGR consistently benefit the recommendation system in ranking estimation task, especially for the near-cold-start-users (with less than five interacted items).</p>
    <p><strong>Categories:</strong> Recommendation Systems, Deep Learning, Generative Models, Personalization, Implicit Feedback, Cold Start Problem, Matrix Factorization, Large-Scale Datasets, Theoretical Analysis, Cross-Domain Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/434/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Word2vec applied to Recommendation: Hyperparameters Matter (2018)</h3>
    <p><strong>Authors:</strong> Jimena Royo-Letelier, Hugo Caselles-Dupré, Florian Lesaint</p>
    <p>Skip-gram with negative sampling, a popular variant of Word2vec originally designed and tuned to create word embeddings for Natural Language Processing, has been used to create item embeddings with successful applications in recommendation. While these fields do not share the same type of data, neither evaluate on the same tasks, recommendation applications tend to use the same already tuned hyperparameters values, even if optimal hyperparameters values are often known to be data and task dependent. We thus investigate the marginal importance of each hyperparameter in a recommendation setting through large hyperparameter grid searches on various datasets. Results reveal that optimizing neglected hyperparameters, namely negative sampling distribution, number of epochs, subsampling parameter and window-size, significantly improves performance on a recommendation task, and can increase it by an order of magnitude. Importantly, we find that optimal hyperparameters configurations for Natural Language Processing tasks and Recommendation tasks are noticeably different.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Word Embeddings, Collaborative Filtering, Algorithm Performance, Hyperparameter Optimization, Item Embeddings, Cross-Domain Applications, Beyond Accuracy, Explicit Feedback, Implicit Feedback, Domain Adaptation (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/407/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>