<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/explainability/">Explainability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reinforcement-learning/">Reinforcement Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/evaluation-methods/">Evaluation Methods</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Overcoming Recommendation Limitations with Neuro-Symbolic Integration (2023)</h3>
    <p><strong>Authors:</strong> Tommaso Carraro</p>
    <p>Despite being studied for over twenty years, Recommender Systems (RSs) still suffer from important issues that limit their applicability in real-world scenarios. Data sparsity, cold start, and explainability are some of the most impacting problems. Intuitively, these historical limitations can be mitigated by injecting prior knowledge into recommendation models. Neuro-Symbolic (NeSy) approaches are suitable candidates for achieving this goal. Specifically, they aim to integrate learning (e.g., neural networks) with symbolic reasoning (e.g., logical reasoning). Generally, the integration lets a neural model interact with a logical knowledge base, enabling reasoning capabilities. In particular, NeSy approaches have been shown to deal well with poor training data, and their symbolic component could enhance model transparency. This gives insights that NeSy systems could potentially mitigate the aforementioned RSs limitations. However, the application of such systems to RSs is still in its early stages, and most of the proposed architectures do not really exploit the advantages of a NeSy approach. To this end, we conducted preliminary experiments with a Logic Tensor Network (LTN), a novel NeSy framework. We used the LTN to train a vanilla Matrix Factorization model using a First-Order Logic knowledge base as an objective. In particular, we encoded facts to enable the regularization of the latent factors using content information, obtaining promising results. In this paper, we review existing NeSy recommenders, argue about their limitations, show our preliminary results with the LTN, and propose interesting future works in this novel research area. In particular, we show how the LTN can be intuitively used to regularize models, perform cross-domain recommendation, ensemble learning, and explainable recommendation, reduce popularity bias, and easily define the loss function of a model.</p>
    <p><strong>Categories:</strong> Neuro-Symbolic Integration, Recommender Systems, Data Sparsity, Cold Start Problem, Explainability, Neural Networks, Symbolic Reasoning, Logic Tensor Networks (LTN), Matrix Factorization, Content-Based Recommendations, Model Transparency, Regularization, Cross-Domain Recommendation, Ensemble Learning, Popularity Bias (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/986/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Translating the Public Service Media Remit into Metrics and Algorithms (2022)</h3>
    <p><strong>Authors:</strong> Andreas Grün, Xenija Neufeld</p>
    <p>After multiple years of providing automated video recommendations in the ZDFmediathek, ZDF has established a solid ground for the usage of recommender systems. Being a Public Service Media (PSM) provider, our most important driver on this journey is our Public Service Media Remit (PSMR). We are committed to cultivate PSM values such as diversity, fairness, and transparency while providing fresh and relevant content. Therefore, it is important for us to not only measure the success of our recommender systems in terms of basic business Key Performance Indicators (KPIs) such as clicks and viewing minutes but also to ensure and to measure the achievement of PSM values. While speaking about PSM values, however, it is important to keep in mind that there is no easy way to directly measure values as such. In order to be able to measure their extent in a recommender system, we need to translate these values into public value metrics. However, not only the final results are essential for the PSMR. Additionally, it is highly important to establish transparency while working towards these results, that is, while defining the data, the algorithms, and the pipelines used in recommender systems. In our talk we will provide a deeper insight into how we approach this task with Model Cards and give an overview of some models, their Model Cards, and metrics that we are currently using for ZDFmediathek.</p>
    <p><strong>Categories:</strong> Public Service Media, Metrics for Public Value, Fairness in Recommendations, Model Transparency, Recommendation Algorithms, Algorithm Design, Diversity of Recommendations, Beyond Accuracy Evaluation, Public Service Values, Metrics Design, Recommendation System Design, Transparency in Machine Learning (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/830/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Elsevier, Fairness in Recommendations (2021)</h3>
    <p><strong>Authors:</strong> Daniel Kershaw</p>
    <p>At Elsevier we aim to help scientists further their research. On the one hand, by offering a platform for publishing ground-breaking research, and on the other by helping researchers discover relevant content to assist their work. In our team, Editorial Data Science, we support the former goal by providing tools to help editors in the decisions that they make, from finding reviewers to recommending transfers for manuscripts to more appropriate journals. To improve the workflows for publishing research and help editors in finding relevant reviewers we developed a reviewer recommender. The reviewer recommender will, based on a submitted manuscript, recommend reviewers that optimise the fit between the manuscript, the journal, and the reviewer themselves. To achieve this, the reviewer recommender is built on top of the three principles of quality data, expert knowledge, and experimentation. We leverage data from across Elsevier, ranging from Scopus profiles which includes publication histories and academic impact data, to data from our publishing platform which includes historic invites and subsequent actions around submitted manuscripts. All this data is used to develop and evaluate appropriate models for recommending reviewers. There are many factors to consider when deciding if someone is relevant to review a submitted manuscript. To name a few, the topic of interest of the reviewer, level of expertise of the reviewer, fit of the reviewer to the journal. But we are ultimately looking for the fit between the three stake holders: the manuscript, the journal, and the reviewer. However, it is not as simple as looking for a fit purely on topic or relevance. We can find evidence for that in the growth of research into recommender systems for multi-sided market [ 7] places where all sides within the systems must be considered. In our case, in addition to topical fit, we must consider fairness in recommendations, where minority groups get equal and fair treatment. For this reason, we consider two ’additional’ constraints when developing recommender systems at Elsevier, fairness in the recommendations and respecting the reviewers time and privacy. Fairness in Recommendations is important in recommending people particular within academic fields where there are known biases against minority groups. This bias is present in data which we use. [REF] showed that women and minorities over their academic and research careers are disadvantaged compared to members of the majority groups, either through less research output due to more teaching [ 6] or taking time out to raise families which resulted in lower number of research output [8]. Though bias is not only limited to quantities of research output [ 4 ] showed that women receive fewer citations even when controlling to quantity of research output and gender. This effect of receiving proportionally fewer citation is also amplified [ 5] identified that women are disproportionally ranked lower on the author list of a paper. These observations are not limited to gender but also race and age. Using these features derived from bibliometric analysis without due care within a model could lead to adverse impact against minority groups, resulting in the reviewers from these groups being recommended less and thus less invites to review manuscripts, whilst making sure any interventions does not create additional harms. Respecting user privacy and time. We know through user research that there is also the need to respect the researcher whom we are asking to review these manuscripts. Either because they are too busy to accept another review, or that they feel the manuscript is out of their scope and they are invited to review irrelevant papers. In this talk we show how at Elsevier we build fairness and understanding of stake holders into our recommenders, particularly the reviewer recommender. We experimented with a variety of technologies, ranging from simple search to sophisticated ML models. They all follow the common IR approach of candidate selection followed by re-ranking. All approaches use roughly the same features representing the reviewer (topics they have published in), the journal (topics the journal publishes and metrics about the reviewers they have selected before), the manuscript (key concepts within the manuscript). Through iterating over different models and techniques, we understand the importance of different features, along with their impact on model performance. But more specifically which features have inherent bias built into them, which feature are most predictive , and which fairness aware methods have been used to mitigate these biases [1, 2]. We will then discuss the methods and challenges of including bias metrics in the evaluation and development of the reviewer recommender. In this we focus on how we evaluate fairness from historical data along with group-wise fairness metrics such as demographic parity and equal opportunity. We then discuss the challenges and successes of monitoring these metrics in production [3]. This combination of feature understanding and fairness evaluation at every stage in the model development means we have a deep understanding of how the model works, who it impacts, and how. Resulting in a fair model which keeps the reviewers at the heart of the review process. However, we must not forget, this work sits within the wider academic review system, where bias is introduced on many levels through hiring, promotions and research engagement. Even though a reviewer recommender can not correct the underlying bias, it can contribute to a fairer and more equal outcome.</p>
    <p><strong>Categories:</strong> Fairness in Recommendations, Academic Publishing, Reviewer Recommenders, Machine Learning Models, Information Retrieval, Bias Mitigation, Gender Bias, Minority Groups, Group-wise Fairness Metrics, Demographic Parity, Equal Opportunity, Systemic Bias, Feature Analysis, Model Transparency, Reviewer Privacy, Time Management (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/720/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>