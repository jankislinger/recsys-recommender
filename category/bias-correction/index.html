<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">Bias Correction</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/cold-start/">Cold Start</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/scalability/">Scalability</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Correcting for Interference in Experiments: A Case Study at Douyin (2023)</h3>
    <p><strong>Authors:</strong> Xinyuyang Ren, Andrew Zheng, Vivek Farias, Hao Li, Tianyi Peng, Huawei Zhang</p>
    <p>Interference is a ubiquitous problem in experiments conducted on two-sided content marketplaces, such as Douyin (China’s analog of TikTok). In many cases, creators are the natural unit of experimentation, but creators interfere with each other through competition for viewers’ limited time and attention. “Naive” estimators currently used in practice simply ignore the interference, but in doing so incur bias on the order of the treatment effect. We formalize the problem of inference in such experiments as one of policy evaluation. Off-policy estimators, while unbiased, are impractically high variance. We introduce a novel Monte-Carlo estimator, based on “Differences-in-Qs” (DQ) techniques, which achieves bias which is second-order in the treatment effect, while remaining sample-efficient to estimate. On the theoretical side, our contribution is to develop a generalized theory of Taylor expansions for policy evaluation, which extends DQ theory to all major MDP formulations. On the practical side, we implement our estimator on Douyin’s experimentation platform, and in the process develop DQ into a truly “plug-and-play” estimator for interference in real-world settings: one which provides robust, low-bias, low-variance treatment effect estimates; admits computationally cheap, asymptotically exact uncertainty quantification; and reduces MSE by 99\% compared to the best existing alternatives in our applications.</p>
    <p><strong>Categories:</strong> Interference in Experiments, Experimentation Platforms, Two-Sided Marketplaces, Policy Evaluation, Monte Carlo Estimators, Differences-in-Qs (DQ) Technique, Bias Correction, Variance Reduction, Scalability, Real-World Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/850/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Aspect Re-distribution for Learning Better Item Embeddings in Sequential Recommendation (2022)</h3>
    <p><strong>Authors:</strong> Wei Cai, Jingwen Mao, Weike Pan, Zhechao Yu, Congfu Xu</p>
    <p>Sequential recommendation has attracted a lot of attention from both academia and industry. Since item embeddings directly affect the recommendation results, their learning process is very important. However, most existing sequential models may introduce bias when updating the item embeddings. For example, in a sequence where all items are endorsed by a same celebrity, the co-occurrence of two items only indicates their similarity in terms of endorser, and is independent of the other aspects such as category and color. The existing models often update the entire item as a whole or update different aspects of the item without distinction, which fails to capture the contributions of different aspects to the co-occurrence pattern. To overcome the above limitations, we propose aspect re-distribution (ARD) to focus on updating the aspects that are important for co-occurrence. Specifically, we represent an item using several aspect embeddings with the same initial importance. We then re-calculate the importance of each aspect according to the other items in the sequence. Finally, we aggregate these aspect embeddings into a single aspect-aware embedding according to their importance. The aspect-aware embedding can be provided as input to a successor sequential model. Updates of the aspect-aware embedding are passed back to the aspect embeddings based on their importance. Therefore, different from the existing models, our method pays more attention to updating the important aspects. In our experiments, we choose self-attention networks as the successor model. The experimental results on four real-world datasets indicate that our method achieves very promising performance in comparison with seven state-of-the-art models.</p>
    <p><strong>Categories:</strong> Aspect Re-distribution, Item Embeddings, Sequential Recommendation, Algorithm Design, Model Optimization, Recommendation Quality, Attribute-Based Recommendations, Self-Attention Networks, Bias Correction, Dynamic Embedding Updates, Domain-Agnostic (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/745/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Unbiased Implicit Recommendation and Propensity Estimation via Combinational Joint Learning (2020)</h3>
    <p><strong>Authors:</strong> Yun He, James Caverlee, Yin Zhang, Ziwei Zhu</p>
    <p>This paper focuses on how to generate unbiased recommendations based on biased implicit user-item interactions. We propose a combinational joint learning framework to simultaneously learn unbiased user-item relevance and unbiased propensity. More specifically, we first present a new unbiased objective function for estimating propensity. We then show how a naïve joint learning approach faces an estimation-training overlap problem. Hence, we propose to jointly train multiple sub-models from different parts of the training dataset to avoid this problem. Finally, we show how to incorporate residual components trained by the complete training data to complement the relevance and propensity sub-models. Extensive experiments on two public datasets demonstrate the effectiveness of the proposed model with an improvement of 4% on average over the best alternatives.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Implicit Feedback, Bias Correction, Collaborative Filtering, Propensity Estimation, Joint Learning, Unbiased Recommendations, Machine Learning, Evaluation Metrics, Model Training, Bias in Machine Learning, Model Combination (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/587/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Keeping Dataset Biases out of the Simulation: A Debiased Simulator for Reinforcement Learning based Recommender Systems (2020)</h3>
    <p><strong>Authors:</strong> Herke van Hoof, Jin Huang, Maarten de Rijke, Harrie Oosterhuis</p>
    <p>Reinforcement learning for recommendation (RL4Rec) methods are increasingly receiving attention as an effective way to improve long-term user engagement. However, applying RL4Rec online comes with risks: exploration may lead to periods of detrimental user experience. Moreover, few researchers have access to real-world recommender systems. Simulations have been put forward as a solution where user feedback is simulated based on logged historical user data, thus enabling optimization and evaluation without being run online. While simulators do not risk the user experience and are widely accessible, we identify an important limitation of existing simulation methods. They ignore the interaction biases present in logged user data, and consequently, these biases affect the resulting simulation. As a solution to this issue, we introduce a debiasing step in the simulation pipeline, which corrects for the biases present in the logged data before it is used to simulate user behavior. To evaluate the effects of bias on RL4Rec simulations, we propose a novel evaluation approach for simulators that considers the performance of policies optimized with the simulator. Our results reveal that the biases from logged data negatively impact the resulting policies, unless corrected for with our debiasing method. While our debiasing methods can be applied to any simulator, we make our complete pipeline publicly available as the Simulator for OFfline leArning and evaluation (SOFA): the first simulator that accounts for interaction biases prior to optimization and evaluation.</p>
    <p><strong>Categories:</strong> Reinforcement Learning, Recommendation Systems, Bias Correction, Simulation Methods, Evaluation Metrics, Offline Learning, User Experience, Recommender Evaluation, Algorithmic Bias, Policy Optimization (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/541/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Doubly Robust Estimator for Ranking Metrics with Post-Click Conversions (2020)</h3>
    <p><strong>Authors:</strong> Yuta Saito</p>
    <p>Post-click conversion, a pre-defined action on a web service after a click, is an essential form of feedback, as it directly contributes to the final revenue and accurately captures user preferences for items, compared with the ambiguous click. However, naively using post-click conversions can lead to severe bias when learning or evaluating recommenders because of the selection bias between clicked and unclicked data. In this study, we address the offline evaluation problem of algorithmic recommendations with biased post-click conversions. A possible solution to address this bias is to use the inverse propensity score estimator, as it can provide an unbiased evaluation even with the selection bias. However, this estimator is known to be subject to variance and instability problems, which can be severe in the recommendation setting, as feedback is often highly sparse. To address these limitations with the previous unbiased estimator, we propose a doubly robust estimator for the ground-truth ranking performance of a given recommender. The proposed estimator is unbiased against the ground-truth ranking metric and improves the variance and estimation error tail bound of the existing unbiased estimator. Finally, to evaluate the empirical efficacy of the proposed estimator, we conduct empirical evaluations using semi-synthetic and two public real-world datasets. The results show that the proposed metric reveals a better model evaluation performance compared with existing baseline metrics, particularly in a situation with severe selection bias.</p>
    <p><strong>Categories:</strong> Bias Correction, Conversion Rate, Recommendation Systems, Evaluation Metrics, Click Behavior, Selection Bias, Propensity Score Methods, Metric Robustness, Algorithmic Recommendations, Empirical Evaluation (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/532/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Deconfounding User Satisfaction Estimation from Response Rate Bias (2020)</h3>
    <p><strong>Authors:</strong> Madeleine Traverse, Ed Chi, Trevor Potter, Konstantina Christakopoulou, Daniel Li, Chris Haulk, Minmin Chen, Emma Marriott</p>
    <p>Improving user satisfaction is at the forefront of industrial recommender systems. While significant progress has been made by utilizing logged implicit data of user-item interactions (i.e., clicks, dwell/watch time, and other user engagement signals), there has been a recent surge of interest in measuring and modeling user satisfaction, as provided by orthogonal data sources. Such data sources typically originate from responses to user satisfaction surveys, which explicitly ask users to rate their experience with the system and/or specific items they have consumed in the recent past. This data can be valuable for measuring and modeling the degree to which a user has had a satisfactory experience on the recommendation platform, since what users do (engagement) does not always align with what users say they want (satisfaction as measured by surveys).<br>We focus on a large-scale industrial system trained on user survey responses to predict user satisfaction. The predictions of the satisfaction model for each user-item pair, combined with the predictions of the other models (e.g., engagement-focused ones), are fed into the ranking component of a real-world recommender system in deciding items to present to the user. It is therefore imperative that the satisfaction model does an equally good job on imputing user satisfaction across slices of users and items, as it would directly impact which items a user is exposed to. However, the data used for training satisfaction models is biased in that users are more likely to respond to a survey when they will respond that they are more satisfied. When the satisfaction survey responses in slices of data with high response rate follow a different distribution than those with low response rate, response rate becomes a confounding factor for user satisfaction estimation.<br>We find positive correlation between response rate and ratings in a large-scale survey dataset collected in our case study. To address this inherent response rate bias in the satisfaction data, we propose an inverse propensity weighting approach within a multi-task learning framework. We extend a simple feed-forward neural network architecture predicting user satisfaction to a shared-bottom multi-task learning architecture with two tasks: the user satisfaction estimation task, and the response rate estimation task. We concurrently train these two tasks, and use the inverse of the predictions of the response rate task as loss weights for the satisfaction task to address the response rate bias. We showcase that by doing this, (i) we can accurately model whether a user will respond to a survey, (ii) we improve the user satisfaction estimation error for the data slices with lower response rate while not hurting slices with higher response rate, and (iii) we demonstrate in live A/B experiments that applying the resulting satisfaction predictions to rank recommendations translates to higher user satisfaction.</p>
    <p><strong>Categories:</strong> User Satisfaction, Bias Correction, Multi-Task Learning, Inverse Propensity Weighting, Recommendation Systems, Survey Data, A/B Testing, Neural Networks, Response Rate Analysis, Industrial Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/570/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Correcting Popularity Bias by Enhancing Recommendation Neutrality (2014)</h3>
    <p><strong>Authors:</strong> Shotaro Akaho, Hideki Asoh, Toshihiro Kamishima, Jun Sakuma</p>
    <p>In this paper, we attempt to correct a popularity bias, which is the tendency for popular items to be recommended more frequently, by enhancing recommendation neutrality. Recommendation neutrality involves excluding specified information from the prediction process of recommendation. This neutrality was formalized as the statistical independence between a recommendation result and the specified information, and we developed a recommendation algorithm that satisfies this independence constraint. We correct the popularity bias by enhancing neutrality with respect to information regarding whether candidate items are popular or not. We empirically show that a popularity bias in the predicted preference scores can be corrected.</p>
    <p><strong>Categories:</strong> Recommendation Algorithm, Popularity Bias, Recommendation Neutrality, Bias Correction, Beyond Accuracy, Statistical Independence, Algorithm Evaluation, Fairness in AI/ML, Machine Learning (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/73/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>