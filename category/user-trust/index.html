<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/explainability/">Explainability</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/real-world-applications/">Real-world Applications</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/evaluation-metrics/">Evaluation Metrics</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Explainability in Music Recommender System (2024)</h3>
    <p><strong>Authors:</strong> Shahrzad Shashaani</p>
    <p>Recommendation systems play a crucial role in our daily lives, influencing many of our significant and minor decisions. These systems also have become integral to the music industry, guiding users to discover new content based on their tastes. However, the lack of transparency in these systems often leaves users questioning the rationale behind recommendations. To address this issue, adding transparency and explainability to recommender systems is a promising solution. Enhancing the explainability of these systems can significantly improve user trust and satisfaction. This research focuses on exploring transparency and explainability in the context of recommendation systems, focusing on the music domain. This research can help to understand the gaps in explainability in music recommender systems to create more engaging and trustworthy music recommendations.</p>
    <p><strong>Categories:</strong> Music Recommendations, Explainability, Transparency, User Trust, Recommendation Systems, Human-Computer Interaction, Algorithmic Transparency, Ethical Considerations, Case Study, Music Industry Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1132/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Less is More: Towards Sustainability-Aware Persuasive Explanations in Recommender Systems (2024)</h3>
    <p><strong>Authors:</strong> Seda Polat Erdeniz, Viet-Man Le, Sebastian Lubos, Alexander Felfernig, Merfat El Mansi, Thi Ngoc Trang Tran</p>
    <p>Recommender systems play an important role in supporting the achievement of the United Nations sustainable development goals (SDGs). In recommender systems, explanations can support different goals, such as increasing a user’s trust in a recommendation, persuading a user to purchase specific items, or increasing the understanding of the reasons behind a recommendation. In this paper, we discuss the concept of “sustainability-aware persuasive explanations” which we regard as a major concept to support the achievement of the mentioned SDGs. Such explanations are orthogonal to most existing explanation approaches since they focus on a “less is more” principle, which per se is not included in existing e-commerce platforms. Based on a user study in three item domains, we analyze the potential impacts of sustainability-aware persuasive explanations. The study results are promising regarding user acceptance and the potential impacts of such explanations.</p>
    <p><strong>Categories:</strong> Sustainability, Recommender Systems, User Trust, Persuasion, Explanation Methods, User Studies, Ethical Considerations, Sustainable Development Goals (SDGs), Human-Computer Interaction, Impact Analysis (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1202/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Demystifying Recommender Systems: A Multi-faceted Examination of Explanation Generation, Impact, and Perception (2023)</h3>
    <p><strong>Authors:</strong> Giacomo Balloccu</p>
    <p>Recommender systems have become an integral component of the digital landscape, impacting a multitude of services and industries ranging from e-commerce to entertainment and beyond. By offering personalised suggestions, these systems challenge a fundamental problem in our modern information society named information overload. As users face a deluge of choices, recommender systems help sift through this immense sea of possibilities, delivering a personalised subset of options that align with user preferences and historical behaviour. However, despite their considerable utility, recommender systems often operate as “black boxes,” obscuring the rationale behind recommendations. This opacity can engender mistrust and undermine user engagement, thus attenuating the overall effectiveness of the system. Researchers have emphasized the importance of explanations in recommender systems, highlighting how explanations can enhance system transparency, foster user trust, and improve decision-making processes, thereby enriching user experiences and yielding potential business benefits. Yet, a significant gap persists in the current state of human-understandable explanations research. While recommender systems have grown increasingly complex, our capacity to generate clear, concise, and relevant explanations that reflect this complexity remains limited. Crafting explanations that are both understandable and reflective of sophisticated algorithmic decision-making processes poses a significant challenge, especially in a manner that meets the user’s cognitive and contextual needs.</p>
    <p><strong>Categories:</strong> Explainability, User Trust, Information Overload, Transparency, Artificial Intelligence, Human-Computer Interaction, Personalization, Relevance, Algorithmic Transparency, Cross-Domain Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/975/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Recommender Systems and Algorithmic Hate (2022)</h3>
    <p><strong>Authors:</strong> Robin Burke, Lucia Jayne, Jessie Smith</p>
    <p>Despite increasing reliance on personalization in digital platforms, many algorithms that curate content or information for users have been met with resistance. When users feel dissatisfied or harmed by recommendations, this can lead users to hate, or feel negatively towards these personalized systems. Algorithmic hate detrimentally impacts both users and the system, and can result in various forms of algorithmic harm, or in extreme cases can lead to public protests against “the algorithm” in question. In this work, we summarize some of the most common causes of algorithmic hate and their negative consequences through various case studies of personalized recommender systems. We explore promising future directions for the RecSys research community that could help alleviate algorithmic hate and improve the relationship between recommender systems and their users.</p>
    <p><strong>Categories:</strong> Algorithm Design, User Experience (UX), Ethics in AI/ML, Trust Issues, Algorithmic Harm, Case Studies, Public Perception, User Trust, Ethical Considerations, Personalization Issues (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/798/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Designing and evaluating explainable AI for non-AI experts: challenges and opportunities (2022)</h3>
    <p><strong>Authors:</strong> Katrien Verbert, Vero Vanden Abeele, Maxwell Szymanski</p>
    <p>Artificial intelligence (AI) has seen a steady increase in use in the health and medical field, where it is used by lay users and health experts alike. However, these AI systems often lack transparency regarding the inputs and decision making process (often called black boxes), which in turn can be detrimental to the user’s satisfaction and trust towards these systems. Explainable AI (XAI) aims to overcome this problem by opening up certain aspects of the black box, and has proven to be a successful means of increasing trust, transparency and even system effectiveness. However, for certain groups (i.e. lay users in health), explanation methods and evaluation metrics still remain underexplored. In this paper, we will outline our research regarding designing and evaluating explanations for health recommendations for lay users and domain experts, as well as list a few takeaways we were already able to find in our initial studies.</p>
    <p><strong>Categories:</strong> Explainable AI, Transparency, Trust in AI, Healthcare, Medicine, User-Centered Design, Evaluation Metrics, Health Recommendations, Explanation Methods, User Trust, Challenges and Opportunities, User Satisfaction (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/811/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Developing a Human-Centered Framework for Transparency in Fairness-Aware Recommender Systems (2022)</h3>
    <p><strong>Authors:</strong> Jessie J. Smith</p>
    <p>Though recommender systems fundamentally rely on human input and feedback, human-centered research in the RecSys discipline is lacking. When recommender systems aim to treat users more fairly, misinterpreting user objectives could lead to unintentional harm, whether or not fairness is part of the aim. When users seek to understand recommender systems better, a lack of transparency could act as an obstacle for their trust and adoption of the platform. Human-centered machine learning seeks to design systems that understand their users, while simultaneously designing systems that the users can understand. In this work, I propose to explore the intersection of transparency and user-system understanding through three phases of research that will result in a Human-Centered Framework for Transparency in Fairness-Aware Recommender Systems.</p>
    <p><strong>Categories:</strong> Recommender Systems, Fairness-Aware Recommendation, Transparency in Recommendations, Human-Centered Design, User-System Interaction, Trust in Recommendations, Ethical AI, Explainable AI (XAI), User Trust (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/815/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Argument-based generation and explanation of recommendations (2021)</h3>
    <p><strong>Authors:</strong> Andrés Segura-Tinoco</p>
    <p>In the recommender systems literature, it has been shown that, in addition to improving system effectiveness, explaining recommendations may increase user satisfaction, trust, persuasion and loyalty. In general, explanations focus on the filtering algorithms or the users and items involved in the generation of recommendations. However, on certain domains that are rich on user-generated textual content, it would be valuable to provide justifications of recommendations according to arguments that are explicit, underlying or related with the data used by the systems, e.g., the reasons for customers’ opinions in reviews of e-commerce sites, and the requests and claims in citizens’ proposals and debates of e-participation platforms. In this context, there is a need and challenging task to automatically extract and exploit the arguments given for and against evaluated items. We thus advocate to focus not only on user preferences and item features, but also on associated arguments. In other words, we propose to not only consider what is said about items, but also why it is said. Hence, arguments would not only be part of the recommendation explanations, but could also be used by the recommendation algorithms themselves. To this end, in this thesis, we propose to use argument mining techniques and tools that allow retrieving and relating argumentative information from textual content, and investigate recommendation methods that exploit that information before, during and after their filtering processes.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Explanation Mechanisms, Argument Mining, User Trust, Natural Language Processing (NLP), E-commerce, Citizen Engagement Platforms, Beyond Accuracy, User Satisfaction, Textual Data Analysis, Justification of Recommendations, Persuasion (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/713/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Debiased Explainable Pairwise Ranking from Implicit Feedback (2021)</h3>
    <p><strong>Authors:</strong> Olfa Nasraoui, Khalil Damak, Sami Khenissi</p>
    <p>Recent work in recommender systems has emphasized the importance of fairness, with a particular interest in bias and transparency, in addition to predictive accuracy. In this paper, we focus on the state of the art pairwise ranking model, Bayesian Personalized Ranking (BPR), which has previously been found to outperform pointwise models in predictive accuracy, while also being able to handle implicit feedback. Specifically, we address two limitations of BPR: (1) BPR is a black box model that does not explain its outputs, thus limiting the user’s trust in the recommendations, and the analyst’s ability to scrutinize a model’s outputs; and (2) BPR is vulnerable to exposure bias due to the data being Missing Not At Random (MNAR). This exposure bias usually translates into an unfairness against the least popular items because they risk being under-exposed by the recommender system. In this work, we first propose a novel explainable loss function and a corresponding Matrix Factorization-based model called Explainable Bayesian Personalized Ranking (EBPR) that generates recommendations along with item-based explanations. Then, we theoretically quantify additional exposure bias resulting from the explainability, and use it as a basis to propose an unbiased estimator for the ideal EBPR loss. The result is a ranking model that aptly captures both debiased and explainable user preferences. Finally, we perform an empirical study on three real-world datasets that demonstrate the advantages of our proposed models.</p>
    <p><strong>Categories:</strong> Matrix Factorization, Pairwise Ranking Models, Bayesian Personalized Ranking (BPR), Recommender Systems, Explainability, Fairness, Cold Start, Diversity of Recommendations, Beyond Accuracy, Real-World Datasets, User Trust, Debiasing (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/635/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Using Explainability for Constrained Matrix Factorization (2017)</h3>
    <p><strong>Authors:</strong> Olfa Nasraoui, Behnoush Abdollahi</p>
    <p>Accurate model-based Collaborative Filtering (CF) approaches tend to be black-box machine learning models, such as Matrix Factorization (MF), that lack interpretability and do not provide a straightforward explanation for their outputs. Yet explanations can improve the transparency of a recommender system by justifying recommendations, and this in turn can enhance the user’s trust in the recommendations. Hence, one main challenge in designing a recommender system is mitigating the trade-off between an explainable technique with moderate prediction accuracy and a more accurate technique with no explainable recommendations. In this paper, we focus on MF and further assume the absence of any additional data source, such as item content or user attributes. We propose an explainability constrained MF technique that computes the top-n recommendation list from items that are explainable. Experimental results show that our method is effective in generating accurate and explainable recommendations.</p>
    <p><strong>Categories:</strong> Matrix Factorization, Collaborative Filtering, Explainable AI, Transparency, Trust, User Trust, Recommendation Systems, Algorithmic Transparency, Interpretability, Evaluation Metrics, Performance Analysis (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/309/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Folding: Why Good Models Sometimes Make Spurious Recommendations (2017)</h3>
    <p><strong>Authors:</strong> Nicolas Mayoraz, Karthik Lakshmanan, John R. Anderson, Hubert Pham, Doris Xin</p>
    <p>In recommender systems based on low rank factorization of a partially observed user-item matrix, a common phenomenon that plagues many otherwise effective models is the interleaving of good and spurious recommendations in the top-K results. A single incongruous recommendation can dramatically impact the perceived quality of a recommender system. In this work, we investigate folding, a major contributing factor to spurious recommendations. Folding refers to the unintentional overlap of disparate groups of users and items in the low-rank embedding vector space, induced by improper handling of missing data. We formally define a metric that quantifies the severity of folding in a trained system, to assist in diagnosing its potential to shock users with inappropriate recommendations. The folding metric complements existing information retrieval metrics that focus on the number of good recommendations and their ranks but ignore the impact of undesired recommendations. We motivate our definition of the folding metric on synthetic data and evaluate its effectiveness on both synthetic and real world datasets. We study the relationship between the folding metric and other characteristics of recommender systems and observe that optimizing for goodness metrics can lead to high folding and thus more spurious recommendations.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Matrix Factorization, Spurious Recommendations, Missing Data, Low-Rank Embedding, Folding, User-Item Matrix, Evaluation Metrics, Optimization, Metrics Development, Data Handling, User Trust, System Quality, Perceived Quality (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/261/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Dynamics of Human Trust in Recommender Systems (2014)</h3>
    <p><strong>Authors:</strong> Tarek Abdelzahe, Cleotilde Gonzalez, Jason Harman, John O’Donovan</p>
    <p>The trust that humans place on recommendations is key to the success of recommender systems. The formation and decay of trust in recommendations is a dynamic process influenced by context, human preferences, accuracy of recommendations, and the interactions of these factors. This paper describes two psychological experiments (N=400) that evaluate the evolution of trust in recommendations over time, under personalized and non-personalized recommendations by matching or not matching a participant’s profile. Main findings include: Humans trust inaccurate recommendations more than they should; when recommendations are personalized, they lose trust in inaccurate recommendations faster than when recommendations are not personalized; and participants learn to select the options that provide best outcomes increasingly over time when they use personalized recommendations, while they are unable to learn if the recommendations are not personalized. We make connections to the possible implications that these psychological findings to the design of recommender systems.</p>
    <p><strong>Categories:</strong> Trust Dynamics, User Behavior, Psychology of Recommendations, Recommendation Accuracy, Personalization in Recommendations, Human Factors, Recommender Systems Design, User Trust, Learning from Recommendations, Evaluation Methods, Psychological Experiments (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/48/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>