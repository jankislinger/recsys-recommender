<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reinforcement-learning/">Reinforcement Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/bias-mitigation/">Bias Mitigation</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/e-commerce/">E-commerce</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reproducibility/">Reproducibility</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Scale-Invariant Learning-to-Rank (2024)</h3>
    <p><strong>Authors:</strong> Ye-Sheen Lim, Alessio Petrozziello, Xiaoke Liu, Christian Sommeregger</p>
    <p>At Expedia, learning-to-rank (LTR) models plays a key role on our website in sorting and presenting information more relevant to users, such as search filters, property rooms, amenities, and images. A major challenge in deploying these models is ensuring consistent feature scaling between training and production data, as discrepancies can lead to unreliable rankings when deployed. Normalization techniques like feature standardization and batch normalization could address these issues but are impractical in production due to latency impacts and the difficulty of distributed real-time inference. To address consistent feature scaling issue, we introduce a scale-invariant LTR framework which combines a deep and a wide neural network to mathematically guarantee scale-invariance in the model at both training and prediction time.</p>
    <p><strong>Categories:</strong> Learning-to-Rank, E-commerce, Travel, Model Scaling, Feature Engineering, Production Systems, Scale-Invariant Methods, Ranking Algorithms, Web Systems, Real-World Applications, Model Robustness, Algorithm Design (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1174/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Multi-Preview Recommendation via Reinforcement Learning (2024)</h3>
    <p><strong>Authors:</strong> Kuan-Ting Lai, Pengcheng Xiong, Zhong Wu, Yang Xu</p>
    <p>Preview recommendations serve as a crucial shortcut for attracting users’ attention on various systems, platforms, and webpages, significantly boosting user engagement. However, the variability of preview types and the flexibility of preview duration make it challenging to use an integrated framework for multi-preview recommendations under resource constraints. In this paper, we present an approach that incorporates constrained Q-learning into a notification recommendation system, effectively handling both multi-preview ranking and duration orchestration by targeting long-term user retention. Our method bridges the gap between combinatorial reinforcement learning, which often remains too theoretical for practical use, and segmented modules in production, where model performance is typically compromised due to over-simplification. We demonstrate the superiority of our approach through off-policy evaluation and online A/B testing using Microsoft data.</p>
    <p><strong>Categories:</strong> Reinforcement Learning, Recommendation Systems, Multi-Preview, Notifications, User Engagement, Resource Constraints, Q-Learning, Combinatorial Optimization, Online Evaluation, A/B Testing, Microsoft Data, Production Systems (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1207/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Toward 100TB Recommendation Models with Embedding Offloading (2024)</h3>
    <p><strong>Authors:</strong> Sarunya Pumma, Levy Zhao, Joshua Deng, Dennis Van Der Staay, Jian He, Intaik Park, Ehsan Ardestani, Damian Reeves, Yu Guo, Paul Zhang, Henry Tsang</p>
    <p>Training recommendation models become memory-bound with large embedding tables, and fast GPU memory is scarce. In this paper, we explore embedding caches and prefetch pipelines to effectively leverage large but slow host memory for embedding tables. We introduce Locality-Aware Sharding and iterative planning that automatically size caches optimally and produce effective sharding plans. Embedding Offloading, a system that combines all of these components and techniques, is implemented on top of Meta’s open-source libraries, FBGEMM GPU and TorchRec, and it is used to improve scalability and efficiency of industry-scale production models. Embedding Offloading achieved 37x model scale to 100TB model size with only 26% training speed regression.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Embedding, Scalability, Memory Optimization, System Design, Cache Management, Sharding, Production Systems, Industry Applications, System Performance, GPU Memory, Resource Management (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1178/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>From Research to Production: Towards Scalable and Sustainable Neural Recommendation Models on Commodity CPU Hardware (2023)</h3>
    <p><strong>Authors:</strong> Anshumali Shrivastava, Nicholas Meisburger, Tharun Medini, Siddharth Jain, David Torres Ramos, Vihan Lakshman, Joshua Engels, Pratik Pranav, Yashwanth Adunukota, Shubh Gupta, Benito Geordie</p>
    <p>In the last decade, large-scale deep learning has fundamentally transformed industrial recommendation systems. However, this revolutionary technology remains prohibitively expensive due to the need for costly and scarce specialized hardware, such as GPUs, to train and serve models. In this talk, we share our multi-year journey at ThirdAI in developing efficient neural recommendation models that can be trained and deployed on commodity CPU machines without the need for costly accelerators like GPUs. In particular, we discuss the limitations of the current GPU-based ecosystem in machine learning, why recommendation systems are amenable to the strengths of CPU devices, and present results from our efforts to translate years of academic research into a deployable system that fundamentally shifts the economics of training and operating large-scale machine learning models.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Neural Networks, Production Systems, Scalability, Hardware Optimization, Cost-Effectiveness, Efficiency Optimization, Research to Practice (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/999/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Contextual Multi-Armed Bandit for Email Layout Recommendation (2023)</h3>
    <p><strong>Authors:</strong> Benjamin Schroeder, Emilian Vankov, Akash Mehta, Preston Donovan, Yan Chen, Linas Baltrunas</p>
    <p>We present the use of a contextual multi-armed bandit approach to improve the personalization of marketing emails sent to Wayfair’s customers. Emails are a critical outreach tool as they economically unlock a significant amount of revenue. We describe how we formulated our problem of selecting the optimal personalized email layout to use as a contextual multi-armed bandit problem. We also explain how we approximated a solution with an Epsilon-greedy strategy. We detail the thorough evaluations we ran, including offline experiments, an off-policy evaluation, and an online A/B test. Our results demonstrate that our approach is able to select personalized email layouts that lead to significant gains in topline business metrics including engagement and conversion rates.</p>
    <p><strong>Categories:</strong> Contextual Multi-Armed Bandits, Marketing, Email, Recommendation Systems, Personalization, Evaluation Methods, Beyond Accuracy, A/B Test, User Behavior, Production Systems (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/990/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Augmenting Netflix Search with In-Session Adapted Recommendations (2022)</h3>
    <p><strong>Authors:</strong> Moumita Bhattacharya, Sudarshan Dnyaneshwar Lamkhede</p>
    <p>We motivate the need for recommendation systems that can cater to the members’ in-the-moment intent by leveraging their interactions from the current session. We provide an overview of an end-to-end in-session adaptive recommendations system in the context of Netflix Search. We discuss the challenges and potential solutions when developing such a system at production scale.</p>
    <p><strong>Categories:</strong> In-Session Recommendations, Production Systems, Personalized Recommendations, Media Recommendations, Scalability, Real-Time Adaptation, Recommendation Systems Algorithms, Dynamic Adaptation, Netflix Recommendations, Search Optimization (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/817/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Zillow: Volume Governing for Email and Push Messages (2022)</h3>
    <p><strong>Authors:</strong> Ruomeng Xu, Shruti Kamath, Balasubramanian Thiagarajan, Eric Paul Nichols</p>
    <p>This talk describes the system used at Zillow to govern the quantity of email and push messages sent to users. Emphasis is given to practical issues and lessons learned in running the system in production.</p>
    <p><strong>Categories:</strong> Communication Channels, Email Management, Push Notifications, Volume Control, User Engagement, System Design, Production Systems, Best Practices, Monitoring, Real-World Applications, Zillow (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/841/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Building and Deploying a Multi-Stage Recommender System with Merlin (2022)</h3>
    <p><strong>Authors:</strong> Even Oldridge, Ronay Ak, Gabriel de Souza Pereira Moreira, Sara Rabhi, Karl Higley</p>
    <p>Newcomers to recommender systems often face challenges related to their lack of understanding of how these systems operate in real life. In most online content related to this topic, the focus is on models and algorithms that score items based on the user’s preferences. However, the recommender model alone does not comprise everything needed for serving optimized recommender systems that meet the company’s business objectives. An industry-standard recommender system involves a number of steps, including data preprocessing, defining and training recommender models, as well as filtering and business logic for serving. In this work, we propose the four-stage recommender system, an industry-wide design pattern we have identified for production recommender systems. The four-stage pipeline includes an item retrieval step that prepares a small subset of relevant items for scoring. The filtering stage then cleans up the subset of items based on business logic such as removing out-of-stock or previously seen items. As for the ranking component, it uses a recommender model to score each item in the presented list based on the preferences of the user. In the final step, the scores are re-ordered to provide a final recommendation list aligned with other business needs or constraints such as diversity. In particular, the presented demo demonstrates how easy it is to build and deploy a four-stage recommender system pipeline using the NVIDIA Merlin open-source framework.</p>
    <p><strong>Categories:</strong> Multi-stage Recommender Systems, Item Retrieval, Filtering and Ranking, Business Logic Integration, Data Preprocessing, Recommender System Design Patterns, Real-world Applications, Production Systems (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/800/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Off-Policy Actor Critic for Recommender Systems (2022)</h3>
    <p><strong>Authors:</strong> Minmin Chen, Can Xu, Aviral Kumar, Ed Chi, Vince Gatto, Devanshu Jain</p>
    <p>Industrial recommendation platforms are increasingly concerned with how to make recommendations that cause users to enjoy their long term experience on the platform. Reinforcement learning emerged naturally as an appealing approach for its promise in 1) combating feedback loop effect resulted from myopic system behaviors; and 2) sequential planning to optimize long term outcome. Scaling RL algorithms to production recommender systems serving billions of users and contents, however remain challenging. Sample inefficiency and instability of online RL hinder its widespread adoption in production. Offline RL enables usage of off-policy data and batch learning. It on the other hand faces significant challenges in learning due to the distribution shift.<br>A REINFORCE agent [3] was successfully tested for YouTube recommendation, significantly outperforming a sophisticated supervised learning production system. Off-policy correction was employed to learn from logged data. The algorithm partially mitigates the distribution shift by employing a one-step importance weighting. We resort to the off-policy actor critic algorithms to addresses the distribution shift to a better extent. Here we share the key designs in setting up an off-policy actor-critic agent for production recommender systems. It extends  [3] with a critic network that estimates the value of any state-action pairs under the target learned policy through temporal difference learning. We demonstrate in offline and live experiments that the new framework out-performs baseline and improves long term user experience.<br>An interesting discovery along our investigation is that recommendation agents that employ a softmax policy parameterization, can end up being too pessimistic about out-of-distribution (OOD) actions. Finding the right balance between pessimism and optimism on OOD actions is critical to the success of offline RL for recommender systems.</p>
    <p><strong>Categories:</strong> Reinforcement Learning, Recommendation Systems, Offline Reinforcement Learning, Actor-Critic Methods, Off-Policy Learning, Production Systems, Distribution Shift, State-Action Value Estimation, Softmax Policy, Evaluation of Recommendations. (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/772/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Online Learning for Recommendations at Grubhub (2021)</h3>
    <p><strong>Authors:</strong> Alex Egg</p>
    <p>We propose a method to easily modify existing offline Recommender Systems to run online using Transfer Learning. Online Learning for Recommender Systems has two main advantages: quality and scale. Like many Machine Learning algorithms in production if not regularly retrained will suffer from Concept Drift. A policy that is updated frequently online can adapt to drift faster than a batch system. This is especially true for user-interaction systems like recommenders where the underlying distribution can shift drastically to follow user behaviour. As a platform grows rapidly like Grubhub, the cost of running batch training jobs becomes material. A shift from stateless batch learning offline to stateful incremental learning online can recover, for example, at Grubhub, up to a 45x cost savings and a +20% metrics increase. There are a few challenges to overcome with the transition to online stateful learning, namely convergence, non-stationary embeddings and off-policy evaluation, which we explore from our experiences running this system in production.</p>
    <p><strong>Categories:</strong> Online Learning, Recommender Systems, Transfer Learning, Concept Drift, Scalability, Real-World Applications, Cost Efficiency, Evaluation Methods, Incremental Learning, Production Systems, Machine Learning Optimization, User Interaction Systems (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/730/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Jointly Optimize Capacity, Latency and Engagement in Large-scale Recommendation Systems (2021)</h3>
    <p><strong>Authors:</strong> Hitesh Khandelwal</p>
    <p>As the recommendation systems behind commercial services scale up and apply more and more sophisticated machine learning models, it becomes important to optimize computational cost (capacity) and runtime latency, besides the traditional objective of user engagement. Caching recommended results and reusing them later is a common technique used to reduce capacity and latency. However, the standard caching approach negatively impacts user engagement. To overcome the challenge, this paper presents an approach to optimizing capacity, latency and engagement simultaneously. We propose a smart caching system including a lightweight adjuster model to refresh the cached ranking scores, achieving significant capacity savings without impacting ranking quality. To further optimize latency, we introduce a prefetching strategy which leverages the smart cache. Our production deployment on Facebook Marketplace demonstrates that the approach reduces capacity demand by 50% and p75 end-to-end latency by 35%. While Facebook Marketplace is used as a case study, the approach is applicable to other industrial recommendation systems as well.</p>
    <p><strong>Categories:</strong> System Optimization, Technical Challenges, Caching Strategies, Large-scale Systems, Recommendation Algorithms, Machine Learning Models, Real-World Applications, A/B Testing, Production Systems, User Engagement, Scalability, Industrial Applications (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/727/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Neural Collaborative Filtering vs. Matrix Factorization Revisited (2020)</h3>
    <p><strong>Authors:</strong> John Anderson, Steffen Rendle, Walid Krichene, Li Zhang</p>
    <p>Embedding based models have been the state of the art in collaborative filtering for over a decade. Traditionally, the dot product or higher order equivalents have been used to combine two or more embeddings, e.g., most notably in matrix factorization. In recent years, it was suggested to replace the dot product with a learned similarity e.g. using a multilayer perceptron (MLP). This approach is often referred to as neural collaborative filtering (NCF). In this work, we revisit the experiments of the NCF paper that popularized learned similarities using MLPs. First, we show that with a proper hyperparameter selection, a simple dot product substantially outperforms the proposed learned similarities. Second, while a MLP can in theory approximate any function, we show that it is non-trivial to learn a dot product with an MLP. Finally, we discuss practical issues that arise when applying MLP based similarities and show that MLPs are too costly to use for item recommendation in production environments while dot products allow to apply very efficient retrieval algorithms. We conclude that MLPs should be used with care as embedding combiner and that dot products might be a better default choice.</p>
    <p><strong>Categories:</strong> Neural Collaborative Filtering, Matrix Factorization, Algorithm Comparison, Hyperparameters, Recommendation Systems, Evaluation Metrics, Production Systems, Efficiency, Cost Analysis, Traditional Methods vs. Neural Methods, Practical Recommendations (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/590/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Addressing Delayed Feedback for Continuous Training with Neural Networks in CTR prediction (2019)</h3>
    <p><strong>Authors:</strong> Deepak Dilipkumar, Pranay Kumar Myana, Lucas Theis, Wenzhe Shi, Sofia Ira Ktena, Steven Yoo, Ferenc Huszár, Alykhan Tejani</p>
    <p>One of the challenges in display advertising is that the distribution of features and click through rate (CTR) can exhibit large shifts over time due to seasonality, changes to ad campaigns and other factors. The predominant strategy to keep up with these shifts is to train predictive models continuously, on fresh data, in order to prevent them from becoming stale. However, in many ad systems positive labels are only observed after a possibly long and random delay. These delayed labels pose a challenge to data freshness in continuous training: fresh data may not have complete label information at the time they are ingested by the training algorithm. Naive strategies which consider any data point a negative example until a positive label becomes available tend to underestimate CTR, resulting in inferior user experience and suboptimal performance for advertisers. The focus of this paper is to identify the best combination of loss functions and models that enable large-scale learning from a continuous stream of data in the presence of delayed labels. In this work, we compare 5 different loss functions, 3 of them applied to this problem for the first time. We benchmark their performance in offline settings on both public and proprietary datasets in conjunction with shallow and deep model architectures. We also discuss the engineering cost associated with implementing each loss function in a production environment. Finally, we carried out online experiments with the top performing methods, in order to validate their performance in a continuous training scheme. While training on 668 million in-house data points offline, our proposed methods outperform previous state-of-the-art by 3% relative cross entropy (RCE). During online experiments, we observed 55% gain in revenue per thousand requests (RPMq) against naive log loss. ,</p>
    <p><strong>Categories:</strong> Neural Networks, Loss Functions, Display Advertising, CTR Prediction, Advertising Systems, Delayed Feedback, Continuous Training, Offline Evaluation, Online Experiments, A/B Testing, Production Systems, Engineering Costs, Time-Aware Models, Stream Learning (<i>deepseek-r1:70b</i>)</p>
    <p><a href="/recsys-recommender/response/431/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>A Hierarchical Bayesian Model for Size Recommendation in Fashion (2018)</h3>
    <p><strong>Authors:</strong> Yuen King Ho, Abdul-Saboor Sheikh, Evgenyi Koryagin, Reza Shirvany, Urs Bergmann, Romain Guigourès</p>
    <p>We introduce a hierarchical Bayesian approach to tackle the challenging problem of size recommendation in e-commerce fashion. Our approach jointly models a size purchased by a customer, and its possible return event: 1. no return, 2. returned too small 3. returned too big. Those events are drawn following a multinomial distribution parameterized on the joint probability of each event, built following a hierarchy combining priors. Such a model allows us to incorporate extended domain expertise and article characteristics as prior knowledge, which in turn makes it possible for the underlying parameters to emerge thanks to sufficient data. Experiments are presented on real (anonymized) data from millions of customers along with a detailed discussion on the efficiency of such an approach within a large scale production system.</p>
    <p><strong>Categories:</strong> Hierarchical Models, Bayesian Methods, Recommendation Systems, E-commerce, Fashion, Size Recommendation, Real-World Applications, Domain Expertise, Prior Knowledge, User Feedback, Scalability, Production Systems, A/B Testing, Recommendation Accuracy (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/368/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>