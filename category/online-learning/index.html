<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flask App</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css">
  <link rel="stylesheet" href="/recsys-recommender/static/custom.css">
</head>

<body>

<div class="ui menu container">
  <div class="header item">
    RecSys Recommender
  </div>
  <a class="item" href="/recsys-recommender/">
    Recommendations
  </a>
  <a class="item" href="/recsys-recommender/embedding/">
    Embedding
  </a>
</div>

<div class="ui inverted main container">
  <div class="ui masthead vertical segment">
    

  <h1 class="ui header">RecSys Articles</h1>

  
  <div class="ui segment center aligned">
    <div class="ui" style="flex-wrap: wrap; margin: 10px 0;">
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/large-language-models-llms/">Large Language Models (LLMs)</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/transfer-learning/">Transfer Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/bias-mitigation/">Bias Mitigation</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/data-sparsity/">Data Sparsity</a>
        
      
        
          <a class="ui primary button" style="margin: 5px;"
             href="/recsys-recommender/">Online Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/real-world-applications/">Real-world Applications</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/reinforcement-learning/">Reinforcement Learning</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/fairness/">Fairness</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/sequential-recommendations/">Sequential Recommendations</a>
        
      
        
          <a class="ui button" style="margin: 5px;"
             href="/recsys-recommender/category/ab-testing/">A/B Testing</a>
        
      
    </div>
  </div>

  
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Ranking Across Different Content Types: The Robust Beauty of Multinomial Blending (2024)</h3>
    <p><strong>Authors:</strong> Jan Malte Lichtenberg, Giuseppe Di Benedetto, Matteo Ruffini</p>
    <p>An increasing number of media streaming services have expanded their offerings to include entities of multiple content types. For instance, audio streaming services that started by offering music only, now also offer podcasts, merchandise items, and (music) videos. Ranking items across different content types into a single slate poses a significant challenge for traditional learning-to-rank (LTR) algorithms due to differing feature sets and user engagement patterns for different content types. We explore a simple method, called multinomial blending (MB), which can be used in conjunction with most existing LTR algorithms. We compare MB to a range of baselines not only in terms of ranking quality but also from other industry-relevant perspectives such as interpretability, ease-of-use, and stability in dynamic online-learning environments.</p>
    <p><strong>Categories:</strong> Learning-to-Rank, Multinomial Blending, Media Streaming Services, Content Types, Ranking Across Different Content Types, Cross-Content Type Recommendations, Real-World Applications, Beyond Accuracy, Scalability, Online Learning, Heterogeneous Data, User Engagement Patterns, Feature Engineering, Experimental Evaluation (<i>deepseek-r1:70b</i>)</p>
    <p><a href="/recsys-recommender/response/1180/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Do Not Wait: Learning Re-Ranking Model Without User Feedback At Serving Time in E-Commerce (2024)</h3>
    <p><strong>Authors:</strong> Sirui Chen, Changshuo Zhang, Zhiyu Li, Quan Lin, Xiao Zhang, Yuan Wang, Jun Xu</p>
    <p>Recommender systems have been widely used in e-commerce, and re-ranking models are playing an increasingly significant role in the domain, which leverages the inter-item influence and determines the final recommendation lists. Online learning methods keep updating a deployed model with the latest available samples to capture the shifting of the underlying data distribution in e-commerce. However, they depend on the availability of real user feedback, which may be delayed by hours or even days, such as item purchases, leading to a lag in model enhancement.  In this paper, we propose a novel extension of online learning methods for re-ranking modeling, which we term LAST, an acronym for Learning At Serving Time. It circumvents the requirement of user feedback by using a surrogate model to provide the instructional signal needed to steer model improvement. Upon receiving an online request, LAST finds and applies a model modification on the fly before generating a recommendation result for the request. The modification is request-specific and transient. It means the modification is tailored to and only to the current request to capture the specific context of the request. After a request, the modification is discarded, which helps to prevent error propagation and stabilizes the online learning procedure since the predictions of the surrogate model may be inaccurate. Most importantly, as a complement to feedback-based online learning methods, LAST can be seamlessly integrated into existing online learning systems to create a more adaptive and responsive recommendation experience. Comprehensive experiments, both offline and online, affirm that LAST outperforms state-of-the-art re-ranking models.</p>
    <p><strong>Categories:</strong> Recommender Systems, E-Commerce, Re-ranking Models, Online Learning, Without User Feedback at Serving Time, Surrogate Model, Comprehensive Experiments, Offline Evaluation, Online Evaluation, Novel Method (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1080/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Investigating the effects of incremental training on neural ranking models (2023)</h3>
    <p><strong>Authors:</strong> Wenzhe Shi, Gilberto Titericz, Kazuki Onodera, Gabriel de Souza Pereira Moreira, Praveen Dhinwa, Chris Deotte, Even Oldridge, Vishal Agrawal, Chris Green, Benedikt Schifferer</p>
    <p>Recommender systems are an essential component of online systems, providing users with a personalized experience. Some recommendation scenarios such as social networks or news are very dynamic, with new items added continuously and the interest of users changing over time due to breaking news or popular events. Incremental training is a popular technique to keep recommender models up-to-date in those dynamic platforms. In this paper, we provide an empirical analysis of a large industry dataset from the Sharechat app MOJ, a social media platform for short videos, to answer relevant questions like – how often should I retrain the model? – do different models, features and dataset sizes benefit from incremental training? – Do all users and items benefit the same from incremental training?</p>
    <p><strong>Categories:</strong> Neural Networks, Incremental Training, Recommendation Systems, Model Evaluation, Dynamic Data, Social Media, Online Learning, Industry Dataset, User Behavior Analysis, Short Videos (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/1001/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Continual Collaborative Filtering Through Gradient Alignment (2023)</h3>
    <p><strong>Authors:</strong> Hady Lauw, Hieu Do</p>
    <p>A recommender system operates in a dynamic environment where new items emerge and new users join the system, resulting in ever-growing user-item interactions over time. Existing works either assume a model trained offline on a static dataset (requiring periodic re-training with ever larger datasets); or an online learning setup that favors recency over history. As privacy-aware users could hide their histories, the loss of older information means that periodic retraining may not always be feasible, while online learning may lose sight of users’ long-term preferences. In this work, we adopt a continual learning perspective to collaborative filtering, by compartmentalizing users and items over time into a notion of tasks. Of particular concern is to mitigate catastrophic forgetting that occurs when the model would reduce in performance for older users and items in prior tasks even as it tries to fit the newer users and items in the current task. To alleviate this, we propose a method that leverages gradient alignment to deliver a model that is more compatible across tasks and maximizes user agreement for better user representations to improve long-term recommendations.</p>
    <p><strong>Categories:</strong> Collaborative Filtering, Continual Learning, Online Learning, Catastrophic Forgetting, Recommender Systems, Dynamic Environment, Gradient Alignment, Long-term Recommendations (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/959/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Dynamic Global Sensitivity for Differentially Private Contextual Bandits (2022)</h3>
    <p><strong>Authors:</strong> Hongning Wang, David B. Zhao, Huazheng Wang</p>
    <p>Bandit algorithms have become a reference solution for interactive recommendation. However, as such algorithms directly interact with users for improved recommendations, serious privacy concerns have been raised regarding its practical use. In this work, we propose a differentially private linear contextual bandit algorithm, via a tree-based mechanism to add Laplace or Gaussian noise to model parameters. Our key insight is that as the model converges during online update, the global sensitivity of its parameters shrinks over time (thus named dynamic global sensitivity). Compared with existing solutions, our dynamic global sensitivity analysis allows us to inject less noise to obtain (ϵ, δ)-differential privacy with added regret caused by noise injection in . We provide a rigorous theoretical analysis over the amount of noise added via dynamic global sensitivity and the corresponding upper regret bound of our proposed algorithm. Experimental results on both synthetic and real-world datasets confirmed the algorithm’s advantage against existing solutions.</p>
    <p><strong>Categories:</strong> Differentially Private Contextual Bandits, Privacy, Recommendation Systems, Algorithm Mechanisms, Differential Privacy Techniques, Online Learning, Theoretical Analysis, Regret Analysis, Experimental Validation, Empirical Evaluation, Evaluation Metrics, Dynamic Adaptation, Context-Aware Recommendations (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/755/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Online Learning for Recommendations at Grubhub (2021)</h3>
    <p><strong>Authors:</strong> Alex Egg</p>
    <p>We propose a method to easily modify existing offline Recommender Systems to run online using Transfer Learning. Online Learning for Recommender Systems has two main advantages: quality and scale. Like many Machine Learning algorithms in production if not regularly retrained will suffer from Concept Drift. A policy that is updated frequently online can adapt to drift faster than a batch system. This is especially true for user-interaction systems like recommenders where the underlying distribution can shift drastically to follow user behaviour. As a platform grows rapidly like Grubhub, the cost of running batch training jobs becomes material. A shift from stateless batch learning offline to stateful incremental learning online can recover, for example, at Grubhub, up to a 45x cost savings and a +20% metrics increase. There are a few challenges to overcome with the transition to online stateful learning, namely convergence, non-stationary embeddings and off-policy evaluation, which we explore from our experiences running this system in production.</p>
    <p><strong>Categories:</strong> Online Learning, Recommender Systems, Transfer Learning, Concept Drift, Scalability, Real-World Applications, Cost Efficiency, Evaluation Methods, Incremental Learning, Production Systems, Machine Learning Optimization, User Interaction Systems (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/730/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Burst-induced Multi-Armed Bandit for Learning Recommendation (2021)</h3>
    <p><strong>Authors:</strong> Rodrigo Alves, Antoine Ledent, Marius Kloft</p>
    <p>In this paper, we introduce a non-stationary and context-free Multi-Armed Bandit (MAB) problem and a novel algorithm (which we refer to as BMAB) to solve it. The problem is context-free in the sense that no side information about users or items is needed. We work in a continuous-time setting where each timestamp corresponds to a visit by a user and a corresponding decision regarding recommendation. The main novelty is that we model the reward distribution as a consequence of variations in the intensity of the activity, and thereby we assist the exploration/exploitation dilemma by exploring the temporal dynamics of the audience. To achieve this, we assume that the recommendation procedure can be split into two different states: the loyal and the curious state. We identify the current state by modelling the events as a mixture of two Poisson processes, one for each of the possible states. We further assume that the loyal audience is associated with a single stationary reward distribution, but each bursty period comes with its own reward distribution. We test our algorithm and compare it to several baselines in two strands of experiments: synthetic data simulations and real-world datasets. The results demonstrate that BMAB achieves competitive results when compared to state-of-the-art methods.</p>
    <p><strong>Categories:</strong> Multi-Armed Bandits, Recommendation Systems, Non-Stationary Environments, Temporal Dynamics, Audience Behavior Analysis, Evaluation Methodology, Real-World Applications, Context-Free Methods, Algorithmic Innovation, Scalability, Online Learning, Poisson Processes (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/627/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Cascading Hybrid Bandits: Online Learning to Rank for Relevance and Diversity (2020)</h3>
    <p><strong>Authors:</strong> Maarten de Rijke, Chang Li, Haoyun Feng</p>
    <p>Relevance ranking and result diversification are two core areas in modern recommender systems. Relevance ranking aims at building a ranked list sorted in decreasing order of item relevance, while result diversification focuses on generating a ranked list of items that covers a broad range of topics. In this paper, we study an online learning setting that aims to recommend a ranked list with K items that maximizes the ranking utility, i.e., a list whose items are relevant and whose topics are diverse. We formulate it as the cascade hybrid bandits (CHB) problem. CHB assumes the cascading user behavior, where a user browses the displayed list from top to bottom, clicks the first attractive item, and stops browsing the rest. We propose a hybrid contextual bandit approach, called, for solving this problem. models item relevance and topical diversity using two independent functions and simultaneously learns those functions from user click feedback. We conduct experiments to evaluate on two real-world recommendation datasets: MovieLens and Yahoo music datasets. Our experimental results show that outperforms the baselines. In addition, we prove theoretical guarantees on the n-step performance demonstrating the soundness of .</p>
    <p><strong>Categories:</strong> Multi-Armed Bandits, Recommendation Systems, Relevance Ranking, Result Diversification, Online Learning, Real-World Applications, Click Models, Ranked Lists, Beyond Accuracy, User Behavior, Optimization (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/518/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Contextual Meta-Bandit for Recommender Systems Selection (2020)</h3>
    <p><strong>Authors:</strong> Sandor Caetano, Renan M. de Oliveira, Anderson Soares, Luckeciano C. Melo, Marlesson R. O. de Santana, Fernando H. F. Camargo, Bruno Brandão</p>
    <p>Recommendation systems operate in a highly stochastic and non-stationary environment. As the amount of user-specific information varies, the users’ interests themselves also change. This combination creates a dynamic setting where a single solution will rarely be optimal unless it can keep up with these transformations. One system may perform better than others depending on the situation at hand, thus making the choice of which system to deploy, even more difficult. We address these problems by using the Hierarchical Reinforcement Learning framework. Our proposed meta-bandit acts as a policy over options, where each option maps to a pre-trained, independent recommender system. This meta-bandit learns online and selects a recommender accordingly to the context, adjusting to the situation. We conducted experiments on real data and found that our approach manages to address the dynamics within the user’s changing interests. We also show that it outperforms any of the recommenders separately, as well as an ensemble of them.</p>
    <p><strong>Categories:</strong> Contextual Meta-Bandit, Multi-Armed Bandits, Hierarchical Reinforcement Learning, Recommendation Systems, Online Learning, Real-World Applications, Dynamic Environments, User Behavior, Algorithm Selection (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/574/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Online Learning to Rank for Sequential Music Recommendation (2019)</h3>
    <p><strong>Authors:</strong> Alberto Ueda, Nivio Ziviani, Bruno L. Pereira, Gustavo Penha, Rodrygo L. T. Santos</p>
    <p>The prominent success of music streaming services has brought increasingly complex challenges for music recommendation. In particular, in a streaming setting, songs are consumed sequentially within a listening session, which should cater not only for the user’s historical preferences, but also for eventual preference drifts, triggered by a sudden change in the user’s context. In this paper, we propose a novel online learning to rank approach for music recommendation aimed to continuously learn from the user’s listening feedback. In contrast to existing online learning approaches for music recommendation, we leverage implicit feedback as the only signal of the user’s preference. Moreover, to adapt rapidly to preference drifts over millions of songs, we represent each song in a lower dimensional feature space and explore multiple directions in this space as duels of candidate recommendation models. Our thorough evaluation using listening sessions from Last.fm demonstrates the effectiveness of our approach at learning faster and better compared to state-of-the-art online learning approaches.</p>
    <p><strong>Categories:</strong> Music Recommendations, Online Learning, Implicit Feedback, Streaming Music, Sequential Recommendations, Preference Drift, Real-World Applications, Learning to Rank, Feature Engineering, Model Comparison, Recommendation Systems, Dynamic Recommendations, Evaluation Metrics (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/449/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Multi-Armed Recommender System Bandit Ensembles (2019)</h3>
    <p><strong>Authors:</strong> Marcos Redondo, Rocío Cañamares, Pablo Castells</p>
    <p>It has long been found that well-configured recommender system ensembles can achieve better effectiveness than the combined systems separately. Sophisticated approaches have been developed to automatically optimize the ensembles’ configuration to maximize their performance gains. However most work in this area has targeted simplified scenarios where algorithms are tested and compared on a single non-interactive run. In this paper we consider a more realistic perspective bearing in mind the cyclic nature of the recommendation task, where a large part of the system’s input is collected from the reaction of users to the recommendations they are delivered. The cyclic process provides the opportunity for ensembles to observe and learn about the effectiveness of the combined algorithms, and improve the ensemble configuration progressively. In this paper we explore the adaptation of a multi-armed bandit approach to achieve this, by representing the combined systems as arms, and the ensemble as a bandit that at each step selects an arm to produce the next round of recommendations. We report experiments showing the effectiveness of this approach compared to ensembles that lack the iterative perspective. Along the way, we find illustrative pitfall examples that can result from common, single-shot offline evaluation setups. i>Presentation: Tuesday Poster Lunch</i</p>
    <p><strong>Categories:</strong> Multi-Armed Bandits, Recommender Systems, Ensemble Methods, Online Learning, Dynamic Adaptation, Feedback Loop, Evaluation Methods, User Interaction, Real-Time Recommendations (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/482/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Efficient Online Recommendation via Low-Rank Ensemble Sampling (2018)</h3>
    <p><strong>Authors:</strong> Zheng Wen, Branislav Kveton, Xiuyuan Lu</p>
    <p>The low-rank structure is one of the most prominent features in modern recommendation problems. In this paper, we consider an online learning problem with a low-rank expected reward matrix where both row features and column features are unknown a priori, and the agent aims to learn to choose the best row-column pair (i.e. the maximum entry) in the matrix. We develop a novel online recommendation algorithm based on ensemble sampling, a recently developed computationally efficient approximation of Thompson sampling. Our computational results show that our algorithm consistently achieves order-of-magnitude improvements over the baselines in both synthetic and real-world experiments.</p>
    <p><strong>Categories:</strong> Matrix Factorization, Thompson Sampling, Ensemble Methods, Online Learning, Recommendation Systems, Real-World Applications, Scalability, Cold Start, Matrix Structure, Sampling Methods (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/374/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Kernalized Collaborative Contextual Bandits (2017)</h3>
    <p><strong>Authors:</strong> Leonardo Cella, Romaric Gaudel, Paolo Cremonesi</p>
    <p>We tackle the problem of recommending products in the online recommendation scenario, which occurs many times in real applications. The most famous and explored instances are news recommendations and advertisements. In this work we propose an extension to the state of the art Bandit models to not only take care of different users’ interactions, but also to go beyond the linearity assumption of the expected reward. As applicative case we may consider situations in which the number of actions (products) is too big to sample all of them even once, and at the same time we have several changing users to serve content to.</p>
    <p><strong>Categories:</strong> Collaborative Filtering, Contextual Bandits, Recommendation Systems, Scalability, Kernel Methods, Non-Linear Models, Real World Applications, User Adaptation, Implicit Feedback, Beyond Accuracy, Online Learning, Large Action Space (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/328/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Educational Question Routing in Online Student Communities (2017)</h3>
    <p><strong>Authors:</strong> Ivan Srba, Jakub Macina, Joseph Jay Williams, Maria Bielikova</p>
    <p>Students’ performance in Massive Open Online Courses (MOOCs) is enhanced by high quality discussion forums or recently emerging educational Community Question Answering (CQA) systems. Nevertheless, only a small number of students answer questions asked by their peers. This results in instructor overload, and many unanswered questions. To increase students’ participation, we present an approach for recommendation of new questions to students who are likely to provide answers. Existing approaches to such question routing proposed for non-educational CQA systems tend to rely on a few experts, which is not suitable because we want students to be engaged as it positively influences their learning outcomes. In tackling this novel educational question routing problem, our method (1) goes beyond previous question-answering data as it incorporate additional non-QA data from the course (to improve prediction accuracy and to involve a larger part of community) and (2) applies constraints on users’ workload (to prevent user overloading). We use an ensemble classifier for predicting students’ willingness to answer a question, as well as the students’ expertise for answering. We conducted an online evaluation of the proposed method using an A/B test in our CQA system deployed at an edX MOOC. The proposed method outperformed a baseline method (non-educational question routing enhanced with workload restriction) in recommendation accuracy, involving more community members, and average number of contributions.</p>
    <p><strong>Categories:</strong> Education, Online Learning, Educational Technology, Community Building, Question Routing, Recommender Systems, Student Engagement, Collaboration, Feature Engineering, User Experience, Real World Application, A/B Test, Online Evaluation, Method Comparison, Ensemble Methods. (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/262/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Recommending Learning Materials to Students by Identifying their Knowledge Gaps (2014)</h3>
    <p><strong>Authors:</strong> Alexander Tuzhilin, Konstantin Bauman</p>
    <p>We propose a new content-based method of providing recommendations of educational materials to the students by identifying gaps in their knowledge of the subject matter in the courses they take. We experimentally validate our method by conducting an A/B test on the students from an online university.</p>
    <p><strong>Categories:</strong> Content-Based Recommendations, Education, Educational Technology (EdTech), Knowledge Gaps, A/B Testing, Experimental Methods, Real-World Application, Cold Start, Beyond Accuracy, Online Learning, Personalized Learning (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/79/">See reasoning</a></p>
  </div>

    </div>
  
    <div class="ui segment">
      
  <div class="item">
    <h3>Exploiting Temporal Influence in Online Recommendation (2014)</h3>
    <p><strong>Authors:</strong> Robert Palovics, Tamas Kiss, Andras Benczur, Erzsebet Frigo, Levente Kocsis</p>
    <p>In this paper we give methods for time aware music recommendation in a social media service with the potential of exploiting immediate temporal influences between users. We consider events when a user listens to an artist the first time and this event follows some friend listening to the same artist short time before. We train a blend of matrix factorization methods that model the relation of the influencer, the influenced and the artist, both the individual factor decompositions and their weight learned by variants of stochastic gradient descent (SGD). Special care is taken since events of influence form a subset of the positive implicit feedback data and hence we have to cope with two different definitions of the positive and negative implicit training data. In addition, in the time aware setting we have to use online learning and evaluation methods. While SGD can easily be trained online, evaluation is cumbersome by traditional measures since we will have potentially different top recommendations at different times. Our experiments are carried over the two-year scrobble history of 70,000 Last.fm users and show a 4% increase in recommendation quality by predicting temporal influences.</p>
    <p><strong>Categories:</strong> Recommendation Systems, Time-Aware Recommendations, Matrix Factorization, Social Media Influence, User Behavior, Evaluation Metrics, Online Learning, Music Recommendations, Real-World Applications, Social Influence, Temporal Dynamics (<i>deepseek-r1:32b</i>)</p>
    <p><a href="/recsys-recommender/response/15/">See reasoning</a></p>
  </div>

    </div>
  



  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>

</body>

</html>